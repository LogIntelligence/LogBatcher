{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words as the demonstration\n",
    "api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openai==1.7.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6a/54/e0af4b74ebb732bfa9bc83d3e49e577d4e332990742a9ecbe228c532a02d/openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "     ---------------------------------------- 0.0/212.1 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 92.2/212.1 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 212.1/212.1 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from openai==1.7.2) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from openai==1.7.2) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from openai==1.7.2) (0.25.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from openai==1.7.2) (2.5.1)\n",
      "Requirement already satisfied: sniffio in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from openai==1.7.2) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from openai==1.7.2) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from openai==1.7.2) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.7.2) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.7.2) (1.1.3)\n",
      "Requirement already satisfied: certifi in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.7.2) (2024.2.2)\n",
      "Requirement already satisfied: httpcore in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.7.2) (1.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.7.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.7.2) (2.14.3)\n",
      "Requirement already satisfied: colorama in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from tqdm>4->openai==1.7.2) (0.4.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\develop\\anaconda\\envs\\langchain38\\lib\\site-packages (from httpcore->httpx<1,>=0.23.0->openai==1.7.2) (0.14.0)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.3.2\n",
      "    Uninstalling openai-1.3.2:\n",
      "      Successfully uninstalled openai-1.3.2\n",
      "Successfully installed openai-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai==1.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9NugAUKViOEsOUviK9ytsccElLCvX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='`[HID] [MT] AppleMultitouchDevice::willTerminate entered`', role='assistant', function_call=None, tool_calls=None))], created=1715486446, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=108, total_tokens=126))\n",
      "`[HID] [MT] AppleMultitouchDevice::willTerminate entered`\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "import openai\n",
    "import backoff\n",
    "from utils.postprocess import post_process\n",
    "\n",
    "# api_key = \"sk-DioNHT7QiDI1a84n900aFd05E2E84bEe85F88451A5DfBe67\"\n",
    "api_key = \"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\"\n",
    "# test_key = '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'\n",
    "client = OpenAI(\n",
    "    # base_url=\"https://3.5.996444.icu/v1\",  # 中转url\n",
    "    api_key=api_key,                      # api_key\n",
    "    http_client=httpx.Client(\n",
    "        proxies=\"http://127.0.0.1:7890\"  # 代理地址\n",
    "    ),\n",
    ")\n",
    "\n",
    "@backoff.on_exception(backoff.expo, (openai.APIStatusError, openai.InternalServerError), max_tries=20)\n",
    "def get_responce(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        # model = \"gpt-4-turbo-2024-4-19\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    # return response.choices[0].message.content.strip('\\n')\n",
    "    return response\n",
    "\n",
    "\n",
    "# log message:\n",
    "# '''\n",
    "\n",
    "\n",
    "logs = '''Successfully started service 'sparkExecutorActorSystem' on port 55904.'''\n",
    "\n",
    "log_list = logs.split('\\n')\n",
    "\n",
    "instruct1 = '''You will be provided with some log messages. You should check if the giving log messages share the same template. If so, abstract variables with `{{placeholders}}` to extract the corresponding template.\\nPrint the input log's template delimited by backticks.'''\n",
    "\n",
    "instruct2 = \"You will be provided with a log message delimited by backticks. You must abstract variables with `{{placeholders}}` to extract the corresponding template.\\nPrint the input log's template delimited by backticks.\"\n",
    "\n",
    "instruct3 = '''there are some possible log templates from the sampled log. Think carefully and choose me the most possible one. Attention that only varaibles (full timestamp, number, IP Address or URL) in the log statement should be abstracted with `<*>`.\n",
    "Print the template delimited by backticks.'''\n",
    "prompt = '''Templates:\n",
    "connection from <*> (<*>) at <*>\n",
    "connection from <*> (<*>) at Sun Jul <*> <*> <*>\n",
    "log:\n",
    "connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005'''\n",
    "\n",
    "logs = '''[HID] [MT] AppleMultitouchDevice::willTerminate entered\n",
    "[HID] [MT] AppleMultitouchDevice::willTerminate entered\n",
    "[HID] [MT] AppleMultitouchDevice::willTerminate entered'''\n",
    "\n",
    "# selinux_register_security:  Registering {{module_type}} module capability\n",
    "responce = get_responce( [\n",
    "    {'role': 'system', 'content': instruct1},\n",
    "    {'role': 'user', 'content': logs}\n",
    "    ])\n",
    "print(responce)\n",
    "print(responce.choices[0].message.content.strip('\\n'))\n",
    "\n",
    "# 1. gpt-3.5-turbo\n",
    "# 2. gpt-3.5-turbo-0125\n",
    "# 3. gpt-3.5-turbo-16k\n",
    "# 4. gpt-35-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('selinux_register_security:  Registering {{module_type}} module capability', '')\n"
     ]
    }
   ],
   "source": [
    "from utils.postprocess import post_process\n",
    "print(post_process(responce.choices[0].message.content.strip('\\n'), reference_log= logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPP to sample the words from logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an14 an15 an16 an17 an18 an19 an20 an21 an22 an23 an24 an25 an26 an27 an28 an29 an30 an31 an32 an33 an34 an35 an36 an37 an38 an39 an40 an41 an42 an43 an44 an45 an46 an47 an48 an49 an50 an51 an52 an53 an54 an55 an56 an57 an58 an59 an60 an61 an62 an63 an64 an65 an66 an67 an68 an69 an70 an71 an72 an73 an74 an75 an76 an77 an78 an79 an80 an81 an82 an83 an84 an85 an86 an87 an88 an89 an90 an91 an92 an93 an94 an95 an96 an97 an98 an99 an100 an101 an102 an103 an104 an105 an106 an107 an108 an109 an110 an111 an112 an113 an114 an115 an116 an117 an118 an119 an120 an121 an122 an123 an124 an125 an126 an127 an128', '1831', 'root', '0', 'Thunderbird_B7'}\n"
     ]
    }
   ],
   "source": [
    "# logs = ['proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS',\n",
    "#         '183.62.156.108:22 open through proxy socks.cse.cuhk.edu.hk:5070 SOCKS5',\n",
    "#         '223.167.104.147:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001',\n",
    "#         'qa.sockets.stackexchange.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061',\n",
    "#         'mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection',\n",
    "#         'tcpconn4.tencent.com:80 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy closed the connection unexpectedly.',\n",
    "#         'tcpconn6.tencent.com:443 error : A connection request was canceled before the completion.',\n",
    "#         'proxy.cse.cuhk.edu.hk:5070 close, 2933 bytes (2.86 KB) sent, 11721005 bytes (11.1 MB) received, lifetime 02:48',\n",
    "#         'proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec']\n",
    "\n",
    "# prompt1 = '''You will be provided with some varaibles, you must discard the variables that are similar or of the same type.\n",
    "# print the remaining variables in a list delimited by backticks.\n",
    "# ['183.62.156.108:22', 'socks.cse.cuhk.edu.hk:5070', 'proxy.cse.cuhk.edu.hk:5070', '403 bytes', '426 bytes', '<1 sec', 'video-hkg3-2.xx.fbcdn.net:443', '58373 bytes (57.0 KB)', '8896991 bytes (8.48 MB)', '02:25', 'tcpconn6.tencent.com:443', '182.254.114.110:80']'''\n",
    "\n",
    "# prompt = '''You will be provided with some varaibles and a log message delimited by backticks. You must abstract variables with `{{placeholders}}` to extract the corresponding template.\n",
    "# Print the input log's template delimited by backticks.\n",
    "# varaibles:\n",
    "# `['183.62.156.108:22', 'socks.cse.cuhk.edu.hk:5070', 'proxy.cse.cuhk.edu.hk:5070', '403 bytes', '426 bytes', '<1 sec', 'video-hkg3-2.xx.fbcdn.net:443', '58373 bytes (57.0 KB)', '8896991 bytes (8.48 MB)', '02:25', 'tcpconn6.tencent.com:443', '182.254.114.110:80']`\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from utils.sample_byword import sample_byword\n",
    "\n",
    "datasets = ['Thunderbird']\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(\n",
    "    f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    print(sample_byword(df, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证按' '分词有多少词，以及含数字的词占多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from post_process import correct_single_template\n",
    "datasets = ['BGL' ,'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac','Android','Hadoop', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets = ['Linux']\n",
    "\n",
    "\n",
    "def tokenize(log_content, tokenize_pattern=r'[ ,]'):\n",
    "    words = re.split(tokenize_pattern, log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if word.startswith('/') and len(word) > 1:\n",
    "            words[index] = ''\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "list = []\n",
    "list2 = []\n",
    "k = 0\n",
    "temp = ['structured', 'templates', 'Content', 'EventTemplate']\n",
    "tokens = []\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f\"dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv\")\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "\n",
    "    print('-' * 20)\n",
    "    print(dataset)\n",
    "\n",
    "    # 验证长度为1-3的tempalte是否含有<*>\n",
    "    # NOTE: 长度为1时，不用解析，2-3仍然需要解析\n",
    "    # DS\n",
    "    # NOTE: 没有template中含有'  '\n",
    "    # BL+US\n",
    "    # NOTE: boolen变量主要出现在Android中 null:Mac, Android root:many datasets admin:OpenSSH, Thunderbird\n",
    "    # BL = ['true', 'false']\n",
    "    # US = ['null', 'root', 'admin']\n",
    "    # DG\n",
    "    # NOTE: 没有template含有纯数字\n",
    "    #\n",
    "    tem = []\n",
    "    for log, template in zip(logs, templates):\n",
    "        if '/' in log and template not in tem:\n",
    "            words = tokenize(log)\n",
    "            tem.append(template)\n",
    "            print(words)\n",
    "            print(template)\n",
    "            print('-' * 20)\n",
    "            \n",
    "            # print('-' * 20)\n",
    "    # 验证按' '分词有多少词，以及含数字的词占多少\n",
    "    # list2.append(len(list))    \n",
    "    # total_words = 0\n",
    "    # words_with_numbers = 0\n",
    "\n",
    "    # for s in list:\n",
    "    #     words = s.split()\n",
    "    #     total_words += len(words)\n",
    "    #     words_with_numbers += len([word for word in words if re.search(r'\\d', word)])\n",
    "    # a+=total_words\n",
    "    # b+=words_with_numbers\n",
    "\n",
    "\n",
    "# print(f\"{b} / {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "def closest_template(target, templates):\n",
    "    min_distance = float('inf')\n",
    "    closest_template = None\n",
    "    for template in templates:\n",
    "        d = distance(target, template)\n",
    "        if d < min_distance:\n",
    "            min_distance = d\n",
    "            closest_template = template\n",
    "    return closest_template\n",
    "\n",
    "templates = [\n",
    "    '<*> close, <*> bytes sent, <*> bytes received, lifetime <1',\n",
    "    'proxy.cse.cuhk.edu.hk:<*> close, <*> bytes (<*> KB) sent, <*> bytes (<*> KB) received, lifetime <1',\n",
    "    '<*> close, <*> bytes sent, <*> bytes (<*> KB) received, lifetime <*>',\n",
    "    '<*> close, <*> bytes (<*> KB) sent, <*> bytes (<*> MB) received, lifetime <*>',\n",
    "]\n",
    "\n",
    "print(closest_template('proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01', templates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(target, lst, n=3):\n",
    "    vectors = [np.array([ord(c) for c in s]).reshape(1, -1) for s in lst]\n",
    "    target_vector = np.array([ord(c) for c in target]).reshape(1, -1)\n",
    "    similarities = cosine_similarity(vectors, target_vector)\n",
    "    most_similar_indices = np.argsort(similarities, axis=0)[-n:].flatten()[::-1]\n",
    "    return [lst[i] for i in most_similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# read log\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "# define a set\n",
    "list = []\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    print('-' * 20)\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log,template in zip(logs, templates):    \n",
    "        if 'kb' in log.lower() and template not in list:\n",
    "            print(log)\n",
    "            list.append(template)\n",
    "\n",
    "\n",
    "list = ['sec', 'KB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看logpub中oracle template不合理的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_process import correct_single_template\n",
    "import pandas as pd\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Linux HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "# define a set\n",
    "list = []\n",
    "for dataset in datasets:\n",
    "    print('+' * 20 + dataset)\n",
    "    df = pd.read_csv(\n",
    "        f'dataset\\{dataset}\\{dataset}_2k.log_templates_corrected.csv')\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for template in templates:\n",
    "        if correct_single_template(template) != template:\n",
    "            print(correct_single_template(template))\n",
    "            print(template)\n",
    "            print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to conclude the types of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def check_variable(variable):\n",
    "    variable = variable.strip()\n",
    "    if variable.startswith('/'):\n",
    "        return 'path'\n",
    "    if variable.startswith('0x'):\n",
    "        return 'address'\n",
    "    if re.match(r'\\b-?\\d+(\\.\\d+)?\\b|\\b0x[0-9a-fA-F]+\\b', variable):\n",
    "        return 'number'\n",
    "    if re.match( r'^[a-zA-Z]+$', variable):\n",
    "        return 'word'\n",
    "    if re.match(  r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}(?::\\d{1,5})?\\b', variable):\n",
    "        return 'ip'\n",
    "    else:\n",
    "        return 'null'\n",
    "\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "\n",
    "dict = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    dict[dataset] = {}\n",
    "    print('-' * 20)\n",
    "    print(dataset + ':')\n",
    "    print('-' * 20)\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    been_used = []\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in been_used:\n",
    "            been_used.append(template)\n",
    "            variables = extract_variables(log, template)\n",
    "            for variable in variables:\n",
    "                type = check_variable(variable)\n",
    "                if type != 'null':\n",
    "                    if type not in dict[dataset]:\n",
    "                        dict[dataset][type] = 1\n",
    "                    else:\n",
    "                        dict[dataset][type] += 1\n",
    "                else:\n",
    "                    print(variable)\n",
    "\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the unseen accurray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from evaluator import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "# table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Linux HealthApp Apache Proxifier OpenSSH OpenStack Mac' # logpub\n",
    "datasets = table_order.split(' ')\n",
    "a = 0\n",
    "b = 0\n",
    "for dataset in datasets:\n",
    "    file = '0125_0shot'\n",
    "    input = f'outputs/parser/{file}/{dataset}.csv'  \n",
    "    output = f'outputs/unseen/{file}/{dataset}.csv'\n",
    "    os.makedirs(f'outputs/unseen/{file}', exist_ok=True)\n",
    "    df = pd.read_csv(input)\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    freq = Counter(templates)\n",
    "    logs_after = []\n",
    "    templates_after = []\n",
    "    unseen_templates = [item for item, count in freq.items() if count == 1]\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template in unseen_templates:\n",
    "            logs_after.append(log)\n",
    "            templates_after.append(template)\n",
    "    accuracy_exact_string_matching = accuracy_score(templates_after, logs_after, normalize=False)\n",
    "    length = len(logs_after)\n",
    "    a += length\n",
    "    b += accuracy_exact_string_matching\n",
    "    dataset = ' ' * (12 - len(dataset)) + dataset\n",
    "    print('%s: len of unseen log: %.4d, Message-Level Accuracy: %.4f' %(dataset, length ,accuracy_exact_string_matching/length))\n",
    "    \n",
    "\n",
    "print(b/a)\n",
    "\n",
    "# 81.0 71.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2686100\n",
      "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/95960536-049b-41f6-9049-05fc479b6a7c HTTP/1.1\" status: 200 len: 1708 time: 0.1721809\n",
      "10.11.21.136,10.11.10.1 \"GET /latest/meta-data/placement/availability-zone HTTP/1.1\" status: 200 len: 120 time: 0.2186198\n",
      "10.11.21.131,10.11.10.1 \"GET /openstack/2013-10-17/user_data HTTP/1.1\" status: 404 len: 176 time: 0.0015490\n",
      "10.11.21.135,10.11.10.1 \"GET /openstack/2012-08-10/meta_data.json HTTP/1.1\" status: 200 len: 264 time: 0.3158371\n",
      "0.009488344192504883\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils.sample_byword import sample_byword\n",
    "import pandas as pd\n",
    "# dataset = 'OpenStack'\n",
    "# df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "# display(sample_byword(df, 5))\n",
    "logs = '''10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2686100\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2688880\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2576380\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1959479\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1910 time: 0.2753000\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2590120\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1575 time: 0.1929309\n",
    "10.11.21.135,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.0010960\n",
    "10.11.21.136,10.11.10.1 \"GET /latest/meta-data/placement/availability-zone HTTP/1.1\" status: 200 len: 120 time: 0.2186198\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2716691\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2558119\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2777491\n",
    "10.11.21.137,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.2337520\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.2773421\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1759 time: 0.3940661\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1874 time: 0.1995878\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1910 time: 0.2654040\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2709410\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2501280\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1754098\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.4256971\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2617428\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2696431\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2692139\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 211 time: 0.1072969\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2648020\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2826021\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2544250\n",
    "10.11.21.134,10.11.10.1 \"GET /openstack/2013-10-17/vendor_data.json HTTP/1.1\" status: 200 len: 124 time: 0.0007591\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2732208\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2498078\n",
    "10.11.21.132,10.11.10.1 \"GET /latest/meta-data/ami-launch-index HTTP/1.1\" status: 200 len: 117 time: 0.0007291\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2779088\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1874 time: 0.1882880\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2643161\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2630000\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/95960536-049b-41f6-9049-05fc479b6a7c HTTP/1.1\" status: 200 len: 1708 time: 0.1721809\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2646649\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2672610\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1969190\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.3656721\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2748618\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2636631\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1583 time: 0.1820800\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2691431\n",
    "10.11.21.131,10.11.10.1 \"GET /openstack/2013-10-17/user_data HTTP/1.1\" status: 404 len: 176 time: 0.0015490\n",
    "10.11.10.1 \"GET /v2/54fadb412c4e40cdbaed9335e4c35a9e/servers/detail HTTP/1.1\" status: 200 len: 1893 time: 0.2734950\n",
    "10.11.21.141,10.11.10.1 \"GET /latest/meta-data/ HTTP/1.1\" status: 200 len: 328 time: 0.2267661\n",
    "10.11.21.132,10.11.10.1 \"GET /latest/meta-data/local-hostname HTTP/1.1\" status: 200 len: 130 time: 0.0007689\n",
    "10.11.21.135,10.11.10.1 \"GET /openstack/2012-08-10/meta_data.json HTTP/1.1\" status: 200 len: 264 time: 0.3158371'''\n",
    "\n",
    "logs_list = logs.split('\\n')\n",
    "templates_list = ['<*> \"GET <*>\" status: <*> len: <*> time: <*>'] * len(logs_list)\n",
    "\n",
    "df = pd.DataFrame({'Content': logs_list, 'EventTemplate': templates_list})\n",
    "t1 = time.time()\n",
    "sample_byword(df, 5, showLogs=True)\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1514038440000', '7007', '548365', '8661', '12361', '27173954')\n"
     ]
    }
   ],
   "source": [
    "from utils.sample_byword import matches_template\n",
    "import re\n",
    "log = 'getTodayTotalDetailSteps = 1514038440000##7007##548365##8661##12361##27173954'\n",
    "template = 'getTodayTotalDetailSteps = <*>##<*>##<*>##<*>##<*>##<*>'\n",
    "cache_pair = [log,template]\n",
    "matches = matches_template(log ,cache_pair)\n",
    "print(matches)\n",
    "\n",
    "\n",
    "# pattern_parts = template.split(\"<*>\")\n",
    "# pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "# regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "# regex = \"^\" + regex_pattern + \"$\"  \n",
    "# matches = re.search(regex, log)\n",
    "# print(matches.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.postprocess import correct_single_template\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
