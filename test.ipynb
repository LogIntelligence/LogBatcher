{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51978/51978 [00:00<00:00, 2885086.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607680172380623\n",
      "44741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.evaluator import evaluate\n",
    "dataset = 'Apache'\n",
    "output_file = f'outputs/parser/LogBatcher_0shot_32candidate_10batchsize_2000chunksize_full/{dataset}_2k.log_structured.csv'\n",
    "groundtruth_file = f'dataset/{dataset}/{dataset}_full.log_structured.csv'\n",
    "# a =evaluate(output_file=output_file, groundtruth_file=groundtruth_file,dataset=dataset)\n",
    "# print(a)\n",
    "df1 = pd.read_csv(output_file)\n",
    "logs1 = df1['EventTemplate'].tolist()\n",
    "df2 = pd.read_csv(groundtruth_file)\n",
    "logs2 = df2['EventTemplate'].tolist()\n",
    "\n",
    "count = 0\n",
    "length = len(logs1)\n",
    "for i in tqdm(range(length)):\n",
    "    if logs1[i] == logs2[i]:\n",
    "        count += 1\n",
    "print(count / length)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.postprocess import correct_single_template\n",
    "\n",
    "print(correct_single_template('<*> \"<*> <*> 1.2.3.4:<*> status: <*> len: <*> time: <*>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "dataset = 'HDFS'\n",
    "groundtruth_file = f'dataset/{dataset}/{dataset}_full.log_structured.csv'\n",
    "df = pd.read_csv(groundtruth_file)\n",
    "# 定义更新规则的函数\n",
    "def update_content(content):\n",
    "    # 使用正则表达式匹配并更新内容\n",
    "    pattern = r'(BLOCK\\* NameSystem\\.allocateBlock: [^ ]*) ([^ ]*)'\n",
    "    replacement = r'\\1. \\2'\n",
    "    return re.sub(pattern, replacement, content)\n",
    "\n",
    "# 更新Content列\n",
    "df['Content'] = df['Content'].apply(update_content)\n",
    "\n",
    "# 保存修改后的CSV文件\n",
    "df.to_csv('updated_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLA:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3344087it [09:51, 5650.37it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m groundtruth_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_full.log_structured.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# groundtruth_file = 'dataset/BGL/BGL_2k.log_structured_corrected.csv'\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m a \u001b[38;5;241m=\u001b[39m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroundtruth_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroundtruth_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# df1 = pd.read_csv('outputs/parser/LogBatcher_0shot_32candidate_10batchsize_2000chunksize_full/BGL_2k.log_structured.csv')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# logs1 = df1['EventTemplate'].tolist()\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# df2 = pd.read_csv('dataset/BGL/BGL_full.log_structured.csv')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(count / length)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(count)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Develop\\Code\\cluster_llm_parsing\\utils\\evaluator.py:39\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(output_file, groundtruth_file, dataset, mismatch)\u001b[0m\n\u001b[0;32m     37\u001b[0m         ed \u001b[38;5;241m=\u001b[39m edit_distance(i, j)\n\u001b[0;32m     38\u001b[0m         normalized_ed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ed \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(i), \u001b[38;5;28mlen\u001b[39m(j))\n\u001b[1;32m---> 39\u001b[0m         edit_distance_result\u001b[38;5;241m.\u001b[39mappend(ed)\n\u001b[0;32m     40\u001b[0m         normalized_ed_result\u001b[38;5;241m.\u001b[39mappend(normalized_ed)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# edit_distance_result_mean = np.mean(edit_distance_result)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# edit_distance_result_std = np.std(edit_distance_result)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# normalized_ed_result_mean = np.mean(normalized_ed_result)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\metrics\\distance.py:111\u001b[0m, in \u001b[0;36medit_distance\u001b[1;34m(s1, s2, substitution_cost, transpositions)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s1[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m s2[j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    110\u001b[0m             last_right_buf \u001b[38;5;241m=\u001b[39m j\n\u001b[1;32m--> 111\u001b[0m         \u001b[43m_edit_dist_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m            \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlast_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlast_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubstitution_cost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubstitution_cost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtranspositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranspositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     last_left_t[s1[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lev[len1][len2]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\metrics\\distance.py:60\u001b[0m, in \u001b[0;36m_edit_dist_step\u001b[1;34m(lev, i, j, s1, s2, last_left, last_right, substitution_cost, transpositions)\u001b[0m\n\u001b[0;32m     57\u001b[0m     d \u001b[38;5;241m=\u001b[39m lev[last_left \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][last_right \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m-\u001b[39m last_left \u001b[38;5;241m+\u001b[39m j \u001b[38;5;241m-\u001b[39m last_right \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# pick the cheapest\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m lev[i][j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.evaluator import evaluate\n",
    "dataset = 'BGL'\n",
    "output_file = f'outputs/parser/LogBatcher_0shot_32candidate_10batchsize_2000chunksize_full/{dataset}_2k.log_structured.csv'\n",
    "# output_file ='outputs/parser/LogBatcher_0shot_32candidate_10batchsize/BGL_2k.log_structured.csv'\n",
    "groundtruth_file = f'dataset/{dataset}/{dataset}_full.log_structured.csv'\n",
    "# groundtruth_file = 'dataset/BGL/BGL_2k.log_structured_corrected.csv'\n",
    "a =evaluate(output_file=output_file, groundtruth_file=groundtruth_file,dataset=dataset)\n",
    "print(a)\n",
    "# df1 = pd.read_csv('outputs/parser/LogBatcher_0shot_32candidate_10batchsize_2000chunksize_full/BGL_2k.log_structured.csv')\n",
    "# logs1 = df1['EventTemplate'].tolist()\n",
    "# df2 = pd.read_csv('dataset/BGL/BGL_full.log_structured.csv')\n",
    "# logs2 = df2['EventTemplate'].tolist()\n",
    "\n",
    "# count = 0\n",
    "# length = len(logs1)\n",
    "# for i in tqdm(range(length)):\n",
    "#     if logs1[i] == logs2[i]:\n",
    "#         count += 1\n",
    "# print(count / length)\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogBatcher_0shot_32candidate_10batchsize_2000chunksize_full_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "11646 232.92\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import tiktoken\n",
    "def count_message_tokens(messages, model_name):\n",
    "    # 根据模型名称加载合适的编码器\n",
    "    if model_name == \"gpt-4\":\n",
    "        encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    elif model_name == \"gpt-3.5-turbo\":\n",
    "        encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    else:\n",
    "        raise ValueError(\"未知的模型名称\")\n",
    "\n",
    "    # 初始化token计数\n",
    "    token_count = 0\n",
    "\n",
    "    # 计算每个消息的token数\n",
    "    for message in messages:\n",
    "        role_tokens = encoder.encode(message['role'])\n",
    "        content_tokens = encoder.encode(message['content'])\n",
    "        token_count += len(role_tokens) + \\\n",
    "            len(content_tokens) + 4  # 加上特殊的消息分隔符的token数\n",
    "\n",
    "    return token_count\n",
    "\n",
    "dataset = 'BGL'\n",
    "\n",
    "counts_token = 0\n",
    "counts_message = 0\n",
    "\n",
    "# 存储解析后的日志列表\n",
    "message_list = []\n",
    "# load every message\n",
    "with open('outputs/cost/LogBatcher_0shot_32candidate_10batchsize_2000chunksize_full_time.json', 'r') as file:\n",
    "    for line in file:\n",
    "        if line.strip() == '[':\n",
    "            list_str = ''\n",
    "            start_load = True\n",
    "        if line.strip() == ']':\n",
    "            list_str += line\n",
    "            message = json.loads(list_str)\n",
    "            message_list.append(message)\n",
    "            start_load = False\n",
    "        if start_load:\n",
    "            list_str += line\n",
    "print(len(message_list))\n",
    "\n",
    "# remove the same log messages\n",
    "# def make_hashable(log_list):\n",
    "#     return tuple(tuple(sorted(d.items())) for d in log_list)\n",
    "# message_list = list(set(make_hashable(log_list) for log_list in message_list))\n",
    "# message_list = [list(map(dict, log_list)) for log_list in message_list]\n",
    "# print(len(message_list))\n",
    "\n",
    "for message in message_list:\n",
    "    counts_token += count_message_tokens(message, 'gpt-3.5-turbo')\n",
    "    counts_message += 1\n",
    "print(counts_token, counts_token/counts_message)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
