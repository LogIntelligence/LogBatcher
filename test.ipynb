{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words as the demonstration\n",
    "api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import openai\n",
    "import backoff\n",
    "from parsing import postprocessing\n",
    "\n",
    "api_key = \"sk-mE91TMZY8yikxpif8fBa64F0BaBa4d76BcCdD0Cb13F437D2\"\n",
    "client = OpenAI(\n",
    "    base_url=\"https://oneapi.xty.app/v1\",  # 中转url\n",
    "    api_key=api_key,                      # api_key\n",
    "    http_client=httpx.Client(\n",
    "        proxies=\"http://127.0.0.1:7890\"  # 代理地址\n",
    "    ),\n",
    ")\n",
    "\n",
    "@backoff.on_exception(backoff.expo, (openai.APIStatusError, openai.InternalServerError), max_tries=5)\n",
    "def get_responce(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip('\\n')\n",
    "    # return response\n",
    "\n",
    "logs = ['proxy.cse.cuhk.edu.hk:5070 open through proxy proxy.cse.cuhk.edu.hk:5070 HTTPS',\n",
    "        '183.62.156.108:22 open through proxy socks.cse.cuhk.edu.hk:5070 SOCKS5',\n",
    "        '223.167.104.147:80 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - Could not resolve proxy.cse.cuhk.edu.hk error 11001',\n",
    "        'qa.sockets.stackexchange.com:443 error : Could not connect to proxy proxy.cse.cuhk.edu.hk:5070 - connection attempt failed with error 10061',\n",
    "        'mtalk.google.com:5228 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy server cannot establish a connection',\n",
    "        'tcpconn4.tencent.com:80 error : Could not connect through proxy proxy.cse.cuhk.edu.hk:5070 - Proxy closed the connection unexpectedly.',\n",
    "        'tcpconn6.tencent.com:443 error : A connection request was canceled before the completion.',\n",
    "        'proxy.cse.cuhk.edu.hk:5070 close, 2933 bytes (2.86 KB) sent, 11721005 bytes (11.1 MB) received, lifetime 02:48',\n",
    "        'proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 353 bytes received, lifetime <1 sec']\n",
    "\n",
    "prompt1 = '''You will be provided with some varaibles, you must discard the variables that are similar or of the same type.\n",
    "print the remaining variables in a list delimited by backticks.\n",
    "['183.62.156.108:22', 'socks.cse.cuhk.edu.hk:5070', 'proxy.cse.cuhk.edu.hk:5070', '403 bytes', '426 bytes', '<1 sec', 'video-hkg3-2.xx.fbcdn.net:443', '58373 bytes (57.0 KB)', '8896991 bytes (8.48 MB)', '02:25', 'tcpconn6.tencent.com:443', '182.254.114.110:80']'''\n",
    "\n",
    "prompt = '''You will be provided with some varaibles and a log message delimited by backticks. You must abstract variables with `{{placeholders}}` to extract the corresponding template.\n",
    "Print the input log's template delimited by backticks.\n",
    "varaibles:\n",
    "`['183.62.156.108:22', 'socks.cse.cuhk.edu.hk:5070', 'proxy.cse.cuhk.edu.hk:5070', '403 bytes', '426 bytes', '<1 sec', 'video-hkg3-2.xx.fbcdn.net:443', '58373 bytes (57.0 KB)', '8896991 bytes (8.48 MB)', '02:25', 'tcpconn6.tencent.com:443', '182.254.114.110:80']`\n",
    "log message:\n",
    "'''\n",
    "for log in logs:\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': f\"{prompt}`{log}`\"},\n",
    "    ]\n",
    "    # \n",
    "    print(postprocessing(get_responce(messages)))\n",
    "\n",
    "# print(get_responce([{'role': 'user', 'content': prompt1}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPP to sample the words from logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "df = pd.read_csv('dataset\\Proxifier\\Proxifier_2k.log_structured_corrected.csv')\n",
    "def dpp_sample(S, k):\n",
    "    # S: similarity matrix\n",
    "    # k: number of items to sample\n",
    "    n = S.shape[0]\n",
    "    \n",
    "    # Initialize empty set Y\n",
    "    Y = set()\n",
    "    \n",
    "    for _ in range(k):\n",
    "        best_i = -1\n",
    "        best_p = -1\n",
    "        \n",
    "        for i in range(n):\n",
    "            if i not in Y:\n",
    "                # Compute determinant of submatrix\n",
    "                det_Yi = np.linalg.det(S[np.ix_(list(Y) + [i], list(Y) + [i])])\n",
    "                \n",
    "                # Compute probability of adding i to Y\n",
    "                p_add = det_Yi / (1 + det_Yi)\n",
    "                \n",
    "                if p_add > best_p:\n",
    "                    best_p = p_add\n",
    "                    best_i = i\n",
    "        \n",
    "        # Add best item to Y\n",
    "        Y.add(best_i)\n",
    "    \n",
    "    return list(Y)\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "logs = df['Content'].tolist()\n",
    "templates = df['EventTemplate'].tolist()\n",
    "# vars = []\n",
    "# for log, template in zip(logs, templates):\n",
    "#     variables = extract_variables(log, template)\n",
    "#     for var in variables:\n",
    "#         if var not in vars and \"proxy.cse.cuhk.edu.hk\" not in var and \"bytes\" not in var:\n",
    "#             vars.append(var)\n",
    "# print(len(vars))\n",
    "# for var in vars:\n",
    "#     print(var)  \n",
    "\n",
    "# 使用 TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(logs)  # logs 是你的文本日志列表\n",
    "\n",
    "# 转换为 numpy array 以便后续计算\n",
    "tfidf_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "vars = []\n",
    "# 计算余弦相似度矩阵\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "# result = dpp_sample(similarity_matrix, 5)\n",
    "# 设置固定的随机种子\n",
    "random.seed(0)\n",
    "\n",
    "# 从1到2000中选择5个不重复的随机数\n",
    "result = random.sample(range(0, 2000), 5)\n",
    "\n",
    "print(result)\n",
    "for i in result:\n",
    "    variables = extract_variables(logs[i], templates[i])\n",
    "    for var in variables:\n",
    "        if var not in vars:\n",
    "            vars.append(var)\n",
    "    print(logs[i])\n",
    "print(vars)\n",
    "# proxy.cse.cuhk.edu.hk:5070\n",
    "# \n",
    "# 403 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross project test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the target dataset\n",
    "dataset = 'Apache'\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "datasets.remove(dataset)\n",
    "global demonstration_templates\n",
    "global demonstration_logs\n",
    "demonstration_templates = []\n",
    "demonstration_logs = []\n",
    "for d in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{d}\\{d}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template  in zip(list1, list2):\n",
    "        if template not in demonstration_templates:\n",
    "            demonstration_templates.append(template)\n",
    "            demonstration_logs.append(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "\n",
    "def find_closest_matches(target, list1, list2, n=5):\n",
    "    matches = difflib.get_close_matches(target, list1, n=n,cutoff=0.1)\n",
    "    indices = [list1.index(match) for match in matches]\n",
    "    return [list1[index] for index in indices],[list2[index] for index in indices]\n",
    "\n",
    "list1, list2 = find_closest_matches('PacketResponder 1 for block blk_38865049064139660 terminating', demonstration_templates, demonstration_logs)\n",
    "\n",
    "print(list1)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证按' '分词有多少词，以及含数字的词占多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from post_process import correct_single_template\n",
    "datasets = ['BGL' ,'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac','Android','Hadoop', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets = ['Linux']\n",
    "\n",
    "\n",
    "def tokenize(log_content, tokenize_pattern=r'[ ,]'):\n",
    "    words = re.split(tokenize_pattern, log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if word.startswith('/') and len(word) > 1:\n",
    "            words[index] = ''\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "list = []\n",
    "list2 = []\n",
    "k = 0\n",
    "temp = ['structured', 'templates', 'Content', 'EventTemplate']\n",
    "tokens = []\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f\"dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv\")\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "\n",
    "    print('-' * 20)\n",
    "    print(dataset)\n",
    "\n",
    "    # 验证长度为1-3的tempalte是否含有<*>\n",
    "    # NOTE: 长度为1时，不用解析，2-3仍然需要解析\n",
    "    # DS\n",
    "    # NOTE: 没有template中含有'  '\n",
    "    # BL+US\n",
    "    # NOTE: boolen变量主要出现在Android中 null:Mac, Android root:many datasets admin:OpenSSH, Thunderbird\n",
    "    # BL = ['true', 'false']\n",
    "    # US = ['null', 'root', 'admin']\n",
    "    # DG\n",
    "    # NOTE: 没有template含有纯数字\n",
    "    #\n",
    "    tem = []\n",
    "    for log, template in zip(logs, templates):\n",
    "        if '/' in log and template not in tem:\n",
    "            words = tokenize(log)\n",
    "            tem.append(template)\n",
    "            print(words)\n",
    "            print(template)\n",
    "            print('-' * 20)\n",
    "            \n",
    "            # print('-' * 20)\n",
    "    # 验证按' '分词有多少词，以及含数字的词占多少\n",
    "    # list2.append(len(list))    \n",
    "    # total_words = 0\n",
    "    # words_with_numbers = 0\n",
    "\n",
    "    # for s in list:\n",
    "    #     words = s.split()\n",
    "    #     total_words += len(words)\n",
    "    #     words_with_numbers += len([word for word in words if re.search(r'\\d', word)])\n",
    "    # a+=total_words\n",
    "    # b+=words_with_numbers\n",
    "\n",
    "\n",
    "# print(f\"{b} / {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "def closest_template(target, templates):\n",
    "    min_distance = float('inf')\n",
    "    closest_template = None\n",
    "    for template in templates:\n",
    "        d = distance(target, template)\n",
    "        if d < min_distance:\n",
    "            min_distance = d\n",
    "            closest_template = template\n",
    "    return closest_template\n",
    "\n",
    "templates = [\n",
    "    '<*> close, <*> bytes sent, <*> bytes received, lifetime <1',\n",
    "    'proxy.cse.cuhk.edu.hk:<*> close, <*> bytes (<*> KB) sent, <*> bytes (<*> KB) received, lifetime <1',\n",
    "    '<*> close, <*> bytes sent, <*> bytes (<*> KB) received, lifetime <*>',\n",
    "    '<*> close, <*> bytes (<*> KB) sent, <*> bytes (<*> MB) received, lifetime <*>',\n",
    "]\n",
    "\n",
    "print(closest_template('proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01', templates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成候选的labelled logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "\n",
    "data = {}\n",
    "templates = []\n",
    "logs = []\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template  in zip(list1, list2):\n",
    "        if template not in templates:\n",
    "            templates.append(template)\n",
    "            logs.append(log)\n",
    "    # 'logs': logs_c, 'templates': templates_c,\n",
    "    print(dataset, len(logs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(target, lst, n=3):\n",
    "    vectors = [np.array([ord(c) for c in s]).reshape(1, -1) for s in lst]\n",
    "    target_vector = np.array([ord(c) for c in target]).reshape(1, -1)\n",
    "    similarities = cosine_similarity(vectors, target_vector)\n",
    "    most_similar_indices = np.argsort(similarities, axis=0)[-n:].flatten()[::-1]\n",
    "    return [lst[i] for i in most_similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# read log\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "# define a set\n",
    "list = []\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    print('-' * 20)\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log,template in zip(logs, templates):    \n",
    "        if 'kb' in log.lower() and template not in list:\n",
    "            print(log)\n",
    "            list.append(template)\n",
    "\n",
    "\n",
    "list = ['sec', 'KB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看logpub中oracle template不合理的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_process import correct_single_template\n",
    "import pandas as pd\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Linux HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "# define a set\n",
    "list = []\n",
    "for dataset in datasets:\n",
    "    print('+' * 20 + dataset)\n",
    "    df = pd.read_csv(\n",
    "        f'dataset\\{dataset}\\{dataset}_2k.log_templates_corrected.csv')\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for template in templates:\n",
    "        if correct_single_template(template) != template:\n",
    "            print(correct_single_template(template))\n",
    "            print(template)\n",
    "            print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to conclude the types of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def check_variable(variable):\n",
    "    variable = variable.strip()\n",
    "    if variable.startswith('/'):\n",
    "        return 'path'\n",
    "    if variable.startswith('0x'):\n",
    "        return 'address'\n",
    "    if re.match(r'\\b-?\\d+(\\.\\d+)?\\b|\\b0x[0-9a-fA-F]+\\b', variable):\n",
    "        return 'number'\n",
    "    if re.match( r'^[a-zA-Z]+$', variable):\n",
    "        return 'word'\n",
    "    if re.match(  r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}(?::\\d{1,5})?\\b', variable):\n",
    "        return 'ip'\n",
    "    else:\n",
    "        return 'null'\n",
    "\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "\n",
    "dict = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    dict[dataset] = {}\n",
    "    print('-' * 20)\n",
    "    print(dataset + ':')\n",
    "    print('-' * 20)\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    been_used = []\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in been_used:\n",
    "            been_used.append(template)\n",
    "            variables = extract_variables(log, template)\n",
    "            for variable in variables:\n",
    "                type = check_variable(variable)\n",
    "                if type != 'null':\n",
    "                    if type not in dict[dataset]:\n",
    "                        dict[dataset][type] = 1\n",
    "                    else:\n",
    "                        dict[dataset][type] += 1\n",
    "                else:\n",
    "                    print(variable)\n",
    "\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the unseen accurray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from evaluator import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "# table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Linux HealthApp Apache Proxifier OpenSSH OpenStack Mac' # logpub\n",
    "datasets = table_order.split(' ')\n",
    "a = 0\n",
    "b = 0\n",
    "for dataset in datasets:\n",
    "    file = '0125_0shot'\n",
    "    input = f'outputs/parser/{file}/{dataset}.csv'  \n",
    "    output = f'outputs/unseen/{file}/{dataset}.csv'\n",
    "    os.makedirs(f'outputs/unseen/{file}', exist_ok=True)\n",
    "    df = pd.read_csv(input)\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    freq = Counter(templates)\n",
    "    logs_after = []\n",
    "    templates_after = []\n",
    "    unseen_templates = [item for item, count in freq.items() if count == 1]\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template in unseen_templates:\n",
    "            logs_after.append(log)\n",
    "            templates_after.append(template)\n",
    "    accuracy_exact_string_matching = accuracy_score(templates_after, logs_after, normalize=False)\n",
    "    length = len(logs_after)\n",
    "    a += length\n",
    "    b += accuracy_exact_string_matching\n",
    "    dataset = ' ' * (12 - len(dataset)) + dataset\n",
    "    print('%s: len of unseen log: %.4d, Message-Level Accuracy: %.4f' %(dataset, length ,accuracy_exact_string_matching/length))\n",
    "    \n",
    "\n",
    "print(b/a)\n",
    "\n",
    "# 81.0 71.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
