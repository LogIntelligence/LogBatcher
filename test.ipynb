{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import openai\n",
    "import backoff\n",
    "\n",
    "api_key = \"sk-mE91TMZY8yikxpif8fBa64F0BaBa4d76BcCdD0Cb13F437D2\"\n",
    "client = OpenAI(\n",
    "    base_url=\"https://oneapi.xty.app/v1\",  # 中转url\n",
    "    api_key=api_key,                      # api_key\n",
    "    http_client=httpx.Client(\n",
    "        proxies=\"http://127.0.0.1:7890\"  # 代理地址\n",
    "    ),\n",
    ")\n",
    "\n",
    "@backoff.on_exception(backoff.expo, (openai.APIStatusError, openai.InternalServerError), max_tries=5)\n",
    "def get_responce(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip('\\n')\n",
    "    # return response\n",
    "\n",
    "\n",
    "logs = '''`Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-slave-07:55904]`'''\n",
    "system_prompt = '''here are some types of variables, descriptions and examples in log message:\n",
    "<OID>: Identification information of an object. \n",
    "Added attempt `1445144423722` to list of failed maps.\n",
    "<LOI>: Location information of an object. \n",
    "Adding path spec: `/mapreduce`.\n",
    "<OBN>: Name of an object. \n",
    "ServerFileSystem domain `root10-local` is full.\n",
    "<TID>: Type information of an object or an action. \n",
    "Using configuration type `1`.\n",
    "<SID>: Status of a switch variable. \n",
    "Saw change in network reachability (isReachable= `2`).\n",
    "<TDA>: Time or duration of an action. \n",
    "Scheduled snapshot period at `10` second(s).\n",
    "<CRS>: Information of computing resource. \n",
    "Combo kernel: `126MB` LOWMEM available.\n",
    "<OBA>: Amount of an object. \n",
    "Total of `23` ddr error(s) detected and corrected.\n",
    "<STC>: Status code of an object or an action. \n",
    "mod-jk child workerEnv in error state `7`.\n",
    "<OTP>: Other information does not belong to the above categories. \n",
    "calvisitor kernel: payload Data `0700` to list of failed maps. You will be provided with a log message delimited by backticks.. You must abstract variables with `{{type}}` to extract the corresponding template.\n",
    "Print the input log's template delimited by backticks.\n",
    "log message: `Remoting started; listening on addresses :[akka.tcp://sparkExecutorActorSystem@mesos-slave-07:55904]`'''\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': system_prompt},\n",
    "]\n",
    "# \n",
    "print(get_responce(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing_copy import get_candidate_template\n",
    "logs = ['connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005']\n",
    "templates = ['connection from <*> (<*>) at <*>', 'Kerberos authentication failed',\n",
    "             'input_userauth_request: invalid user  [preauth]']\n",
    "print(get_candidate_template(templates, logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "log = \"connection from 203.101.45.59 () at Sun Jul  3 10:05:25 2005\"\n",
    "template = \"connection from <*> () at <*>\"\n",
    "\n",
    "# 将模板中的\"<*>\"替换为正则表达式的通配符\".*?\"\n",
    "template = template.replace(\"<*>\", \"(.*?)\")\n",
    "\n",
    "# 使用正则表达式匹配日志字符串\n",
    "match = re.search(template, log)\n",
    "\n",
    "# 提取匹配的字符串\n",
    "if match:\n",
    "    print(match.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HDFS: group Accuracy: 1.0000, Message-Level Accuracy: 0.7950, Edit Distance: 1.1800\n",
      "      Hadoop: group Accuracy: 0.9855, Message-Level Accuracy: 0.6070, Edit Distance: 20.2145\n",
      "       Spark: group Accuracy: 0.9220, Message-Level Accuracy: 0.8790, Edit Distance: 2.0495\n",
      "   Zookeeper: group Accuracy: 0.9945, Message-Level Accuracy: 0.9655, Edit Distance: 0.3505\n",
      "         BGL: group Accuracy: 0.9750, Message-Level Accuracy: 0.9520, Edit Distance: 1.0655\n",
      "         HPC: group Accuracy: 0.8355, Message-Level Accuracy: 0.7605, Edit Distance: 2.7165\n",
      " Thunderbird: group Accuracy: 0.9820, Message-Level Accuracy: 0.6285, Edit Distance: 4.1245\n",
      "       Linux: group Accuracy: 0.3530, Message-Level Accuracy: 0.4910, Edit Distance: 5.0515\n",
      "   HealthApp: group Accuracy: 0.7395, Message-Level Accuracy: 0.5360, Edit Distance: 7.4655\n",
      "      Apache: group Accuracy: 1.0000, Message-Level Accuracy: 0.9780, Edit Distance: 0.2300\n",
      "   Proxifier: group Accuracy: 1.0000, Message-Level Accuracy: 0.0180, Edit Distance: 29.8805\n",
      "     OpenSSH: group Accuracy: 0.5820, Message-Level Accuracy: 0.9770, Edit Distance: 0.2470\n",
      "   OpenStack: group Accuracy: 0.5025, Message-Level Accuracy: 0.4650, Edit Distance: 3.8850\n",
      "         Mac: group Accuracy: 0.8215, Message-Level Accuracy: 0.4650, Edit Distance: 14.5205\n",
      "avg---------: group Accuracy: 0.8352, Message-Level Accuracy: 0.6798, Edit Distance: 6.6415\n"
     ]
    }
   ],
   "source": [
    "from evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Linux HealthApp Apache Proxifier OpenSSH OpenStack Mac' # logpub\n",
    "datasets = table_order.split(' ')\n",
    "m,n,p,q = [],[],[],[]\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/logpub_0shot/{dataset}.csv'  # Fifth_=_0.1\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a,b,c,d = evaluate(file, dataset)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print('avg---------: group Accuracy: %.4f, Message-Level Accuracy: %.4f, Edit Distance: %.4f' % (sum(m)/len(m), sum(n)/len(n), sum(p)/len(p)))\n",
    "\n",
    "# 81.0 71.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate\n",
    "evaluate('outputs/parser/0125_1shot_brandnew/Linux.csv', 'Linux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the target dataset\n",
    "dataset = 'Apache'\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "datasets.remove(dataset)\n",
    "global demonstration_templates\n",
    "global demonstration_logs\n",
    "demonstration_templates = []\n",
    "demonstration_logs = []\n",
    "for d in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{d}\\{d}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template  in zip(list1, list2):\n",
    "        if template not in demonstration_templates:\n",
    "            demonstration_templates.append(template)\n",
    "            demonstration_logs.append(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "\n",
    "def find_closest_matches(target, list1, list2, n=5):\n",
    "    matches = difflib.get_close_matches(target, list1, n=n,cutoff=0.1)\n",
    "    indices = [list1.index(match) for match in matches]\n",
    "    return [list1[index] for index in indices],[list2[index] for index in indices]\n",
    "\n",
    "list1, list2 = find_closest_matches('PacketResponder 1 for block blk_38865049064139660 terminating', demonstration_templates, demonstration_logs)\n",
    "\n",
    "print(list1)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def replace_numbers_with_zero(text):\n",
    "    return re.sub(r'\\d+(\\.\\d+)?', '0', text)\n",
    "\n",
    "\n",
    "input = 'Failed password for root from 5.36.59.76 port 42393 ssh2'\n",
    "print(replace_numbers_with_zero(input))\n",
    "print(replace_numbers_with_zero(replace_numbers_with_zero(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "# datasets = ['OpenSSH']\n",
    "m, n, p, q = [], [], [], []\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/Fifth_=_0.1/{dataset}.csv'  # Fourth_guding\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a, b, c, d = evaluate(file, dataset)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print(sum(m)/len(m))\n",
    "print(sum(n)/len(n))\n",
    "print(sum(p)/len(p))\n",
    "\n",
    "# 81.0 71.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Linux'\n",
    "df = pd.read_csv(\n",
    "    f\"dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv\")\n",
    "list = df['EventTemplate'].tolist()\n",
    "templates = []\n",
    "for item in list:\n",
    "    if item not in templates:\n",
    "        templates.append(item)\n",
    "print(len(templates))\n",
    "for template in templates:\n",
    "    print(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证按' '分词有多少词，以及含数字的词占多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from post_process import correct_single_template\n",
    "datasets = ['BGL' ,'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac','Android','Hadoop', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets = ['Linux']\n",
    "\n",
    "\n",
    "def tokenize(log_content, tokenize_pattern=r'[ ,]'):\n",
    "    words = re.split(tokenize_pattern, log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if word.startswith('/') and len(word) > 1:\n",
    "            words[index] = ''\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "list = []\n",
    "list2 = []\n",
    "k = 0\n",
    "temp = ['structured', 'templates', 'Content', 'EventTemplate']\n",
    "tokens = []\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f\"dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv\")\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "\n",
    "    print('-' * 20)\n",
    "    print(dataset)\n",
    "\n",
    "    # 验证长度为1-3的tempalte是否含有<*>\n",
    "    # NOTE: 长度为1时，不用解析，2-3仍然需要解析\n",
    "    # DS\n",
    "    # NOTE: 没有template中含有'  '\n",
    "    # BL+US\n",
    "    # NOTE: boolen变量主要出现在Android中 null:Mac, Android root:many datasets admin:OpenSSH, Thunderbird\n",
    "    # BL = ['true', 'false']\n",
    "    # US = ['null', 'root', 'admin']\n",
    "    # DG\n",
    "    # NOTE: 没有template含有纯数字\n",
    "    #\n",
    "    tem = []\n",
    "    for log, template in zip(logs, templates):\n",
    "        if '/' in log and template not in tem:\n",
    "            words = tokenize(log)\n",
    "            tem.append(template)\n",
    "            print(words)\n",
    "            print(template)\n",
    "            print('-' * 20)\n",
    "            \n",
    "            # print('-' * 20)\n",
    "    # 验证按' '分词有多少词，以及含数字的词占多少\n",
    "    # list2.append(len(list))    \n",
    "    # total_words = 0\n",
    "    # words_with_numbers = 0\n",
    "\n",
    "    # for s in list:\n",
    "    #     words = s.split()\n",
    "    #     total_words += len(words)\n",
    "    #     words_with_numbers += len([word for word in words if re.search(r'\\d', word)])\n",
    "    # a+=total_words\n",
    "    # b+=words_with_numbers\n",
    "\n",
    "\n",
    "# print(f\"{b} / {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4 call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "client = OpenAI(\n",
    "    base_url=\"https://4.0.996444.icu/v1\", \n",
    "    api_key=\"sk-1XhqpmIwxu9nCuCD51FcA00948F54eDf9cA4765998Ce8f0d\",\n",
    "    http_client=httpx.Client(\n",
    "        proxies=\"http://127.0.0.1:7890\"  # 代理地址\n",
    "    ),\n",
    ")\n",
    "message_list = [\n",
    "     {\"role\": \"user\", \"content\": \"Hello\"}, \n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=message_list,\n",
    "    temperature=0,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(response)\n",
    "response = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "tokenize_pattern = r'[#= ,]'\n",
    "log_content = 'xy=123,z=789,7#8#9 sex you '\n",
    "words = re.split(tokenize_pattern, log_content)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(log_content, tokenize_pattern=r'[ ,]'):\n",
    "    words = re.split(tokenize_pattern, log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if word.startswith('/') and len(word) > 1:\n",
    "            words[index] = ''\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "\n",
    "# root, admin, \n",
    "\n",
    "logs = []\n",
    "logs.append(\n",
    "    'proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01')\n",
    "logs.append(\n",
    "    'proxy.cse.cuhk.edu.hk:5070 close, 451 bytes sent, 18846 bytes (18.4 KB) received, lifetime <1 sec')\n",
    "logs.append('proxy.cse.cuhk.edu.hk:5070 close, 1165 bytes (1.13 KB) sent, 3098 bytes (3.02 KB) received, lifetime 00:01')\n",
    "\n",
    "for log in logs:\n",
    "\n",
    "    print(tokenize(log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drain3 import TemplateMiner\n",
    "\n",
    "# 创建一个模板挖掘器实例\n",
    "miner = TemplateMiner()\n",
    "\n",
    "\n",
    "# 添加一些日志行\n",
    "logs = list(set(prompt.split('\\n')))\n",
    "for log in logs:\n",
    "    result = miner.add_log_message(log)\n",
    "    if result is not None:\n",
    "        print(f\"New template: {result}\")\n",
    "\n",
    "# 打印所有的日志模板\n",
    "print(\"Log templates:\")\n",
    "for template in miner.drain.clusters:\n",
    "    print(template.get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "def closest_template(target, templates):\n",
    "    min_distance = float('inf')\n",
    "    closest_template = None\n",
    "    for template in templates:\n",
    "        d = distance(target, template)\n",
    "        if d < min_distance:\n",
    "            min_distance = d\n",
    "            closest_template = template\n",
    "    return closest_template\n",
    "\n",
    "templates = [\n",
    "    '<*> close, <*> bytes sent, <*> bytes received, lifetime <1',\n",
    "    'proxy.cse.cuhk.edu.hk:<*> close, <*> bytes (<*> KB) sent, <*> bytes (<*> KB) received, lifetime <1',\n",
    "    '<*> close, <*> bytes sent, <*> bytes (<*> KB) received, lifetime <*>',\n",
    "    '<*> close, <*> bytes (<*> KB) sent, <*> bytes (<*> MB) received, lifetime <*>',\n",
    "]\n",
    "\n",
    "print(closest_template('proxy.cse.cuhk.edu.hk:5070 close, 0 bytes sent, 0 bytes received, lifetime 00:01', templates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成候选的labelled logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "\n",
    "data = {}\n",
    "templates = []\n",
    "logs = []\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template  in zip(list1, list2):\n",
    "        if template not in templates:\n",
    "            templates.append(template)\n",
    "            logs.append(log)\n",
    "    # 'logs': logs_c, 'templates': templates_c,\n",
    "    print(dataset, len(logs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(target, lst, n=3):\n",
    "    vectors = [np.array([ord(c) for c in s]).reshape(1, -1) for s in lst]\n",
    "    target_vector = np.array([ord(c) for c in target]).reshape(1, -1)\n",
    "    similarities = cosine_similarity(vectors, target_vector)\n",
    "    most_similar_indices = np.argsort(similarities, axis=0)[-n:].flatten()[::-1]\n",
    "    return [lst[i] for i in most_similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# read log\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "# define a set\n",
    "list = []\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    print('-' * 20)\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log,template in zip(logs, templates):    \n",
    "        if 'kb' in log.lower() and template not in list:\n",
    "            print(log)\n",
    "            list.append(template)\n",
    "\n",
    "\n",
    "list = ['sec', 'KB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看logpub中oracle template不合理的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++HDFS\n",
      "++++++++++++++++++++Hadoop\n",
      "maxContainerCapability: <memory:<*>, vCores:<*>\n",
      "maxContainerCapability: <memory:<*>, vCores:<*>>\n",
      "--------------------\n",
      "++++++++++++++++++++Spark\n",
      "++++++++++++++++++++Zookeeper\n",
      "******* GOODBYE <*> ********\n",
      "******* GOODBYE <*>:<*> ********\n",
      "--------------------\n",
      "Accepted socket connection from <*>\n",
      "Accepted socket connection from <*>:<*>\n",
      "--------------------\n",
      "Cannot open channel to <*> at election address <*>\n",
      "Cannot open channel to <*> at election address <*>:<*>\n",
      "--------------------\n",
      "Client attempting to establish new session at <*>\n",
      "Client attempting to establish new session at <*>:<*>\n",
      "--------------------\n",
      "Client attempting to renew session <*> at <*>\n",
      "Client attempting to renew session <*> at <*>:<*>\n",
      "--------------------\n",
      "Closed socket connection for client <*> (no session established for client)\n",
      "Closed socket connection for client <*>:<*> (no session established for client)\n",
      "--------------------\n",
      "Closed socket connection for client <*> which had sessionid <*>\n",
      "Closed socket connection for client <*>:<*> which had sessionid <*>\n",
      "--------------------\n",
      "Connection request from old client <*>; will be dropped if server is in r-o mode\n",
      "Connection request from old client <*>:<*>; will be dropped if server is in r-o mode\n",
      "--------------------\n",
      "Established session <*> with negotiated timeout <*> for client <*>\n",
      "Established session <*> with negotiated timeout <*> for client <*>:<*>\n",
      "--------------------\n",
      "My election bind port: <*>/<*>\n",
      "My election bind port: <*>/<*>:<*>\n",
      "--------------------\n",
      "Received connection request <*>\n",
      "Received connection request <*>:<*>\n",
      "--------------------\n",
      "++++++++++++++++++++BGL\n",
      "++++++++++++++++++++HPC\n",
      "++++++++++++++++++++Thunderbird\n",
      "User <*>, coming from <*>, authenticated.\n",
      "User #<*>#, coming from #<*>#, authenticated.\n",
      "--------------------\n",
      "++++++++++++++++++++Linux\n",
      "authentication failure; logname= uid=<*> euid=<*> tty=<*> ruser= rhost=<*> user=<*>\n",
      "authentication failure; logname= uid=<*> euid=<*> tty=<*> ruser= rhost=<*>  user=<*>\n",
      "--------------------\n",
      "<*> LOGIN ON <*>\n",
      "ROOT LOGIN ON <*>\n",
      "--------------------\n",
      "ANONYMOUS FTP LOGIN FROM <*>, (anonymous)\n",
      "ANONYMOUS FTP LOGIN FROM <*>,  (anonymous)\n",
      "--------------------\n",
      "Kernel command line: ro <*>=<*>=<*> rhgb quiet\n",
      "Kernel command line: ro root=<*>=<*> rhgb quiet\n",
      "--------------------\n",
      "SELinux: Initializing.\n",
      "SELinux:  Initializing.\n",
      "--------------------\n",
      "SELinux: Starting in permissive mode\n",
      "SELinux:  Starting in permissive mode\n",
      "--------------------\n",
      "<*>: Registering secondary module capability\n",
      "<*>:  Registering secondary module capability\n",
      "--------------------\n",
      "Initializing random number generator: succeeded\n",
      "Initializing random number generator:  succeeded\n",
      "--------------------\n",
      "Starting pcmcia: succeeded\n",
      "Starting pcmcia:  succeeded\n",
      "--------------------\n",
      "Setting network parameters: succeeded\n",
      "Setting network parameters:  succeeded\n",
      "--------------------\n",
      "Bringing up loopback interface: succeeded\n",
      "Bringing up loopback interface:  succeeded\n",
      "--------------------\n",
      "SELinux: Registering netfilter hooks\n",
      "SELinux:  Registering netfilter hooks\n",
      "--------------------\n",
      "++++++++++++++++++++HealthApp\n",
      "getTodayBasicStandardSteps= <*><*><*>\n",
      "getTodayBasicStandardSteps= <*>##<*>##<*>\n",
      "--------------------\n",
      "getTodayTotalDetailSteps = <*>#<*><*><*><*>#<*>\n",
      "getTodayTotalDetailSteps = <*>##<*>##<*>##<*>##<*>##<*>\n",
      "--------------------\n",
      "++++++++++++++++++++Apache\n",
      "++++++++++++++++++++Proxifier\n",
      "++++++++++++++++++++OpenSSH\n",
      "++++++++++++++++++++OpenStack\n",
      "++++++++++++++++++++Mac\n",
      "Arranged view frame: {{<*>, <*>} {<*>, <*>}}\n",
      "Arranged view frame: {{<*>, <*>}\n",
      "{<*>, <*>}}\n",
      "--------------------\n",
      "Page bounds {{<*>, <*>} {<*>, <*>}}\n",
      "Page bounds {{<*>, <*>}\n",
      "{<*>, <*>}}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from post_process import correct_single_template\n",
    "import pandas as pd\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Linux HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "# define a set\n",
    "list = []\n",
    "for dataset in datasets:\n",
    "    print('+' * 20 + dataset)\n",
    "    df = pd.read_csv(\n",
    "        f'dataset\\{dataset}\\{dataset}_2k.log_templates_corrected.csv')\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for template in templates:\n",
    "        if correct_single_template(template) != template:\n",
    "            print(correct_single_template(template))\n",
    "            print(template)\n",
    "            print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to conclude the types of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def check_variable(variable):\n",
    "    variable = variable.strip()\n",
    "    if variable.startswith('/'):\n",
    "        return 'path'\n",
    "    if variable.startswith('0x'):\n",
    "        return 'address'\n",
    "    if re.match(r'\\b-?\\d+(\\.\\d+)?\\b|\\b0x[0-9a-fA-F]+\\b', variable):\n",
    "        return 'number'\n",
    "    if re.match( r'^[a-zA-Z]+$', variable):\n",
    "        return 'word'\n",
    "    if re.match(  r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}(?::\\d{1,5})?\\b', variable):\n",
    "        return 'ip'\n",
    "    else:\n",
    "        return 'null'\n",
    "\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "\n",
    "dict = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    dict[dataset] = {}\n",
    "    print('-' * 20)\n",
    "    print(dataset + ':')\n",
    "    print('-' * 20)\n",
    "    df = pd.read_csv(f'dataset\\{dataset}\\{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    been_used = []\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in been_used:\n",
    "            been_used.append(template)\n",
    "            variables = extract_variables(log, template)\n",
    "            for variable in variables:\n",
    "                type = check_variable(variable)\n",
    "                if type != 'null':\n",
    "                    if type not in dict[dataset]:\n",
    "                        dict[dataset][type] = 1\n",
    "                    else:\n",
    "                        dict[dataset][type] += 1\n",
    "                else:\n",
    "                    print(variable)\n",
    "\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the unseen accurray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HDFS: len of unseen log: 0002, Message-Level Accuracy: 0.0000\n",
      "      Hadoop: len of unseen log: 0069, Message-Level Accuracy: 0.2464\n",
      "       Spark: len of unseen log: 0019, Message-Level Accuracy: 0.5789\n",
      "   Zookeeper: len of unseen log: 0019, Message-Level Accuracy: 0.2105\n",
      "         BGL: len of unseen log: 0044, Message-Level Accuracy: 0.0909\n",
      "         HPC: len of unseen log: 0010, Message-Level Accuracy: 0.5000\n",
      " Thunderbird: len of unseen log: 0095, Message-Level Accuracy: 0.3579\n",
      "     Windows: len of unseen log: 0020, Message-Level Accuracy: 0.4500\n",
      "       Linux: len of unseen log: 0091, Message-Level Accuracy: 0.3407\n",
      "     Android: len of unseen log: 0051, Message-Level Accuracy: 0.2353\n",
      "   HealthApp: len of unseen log: 0031, Message-Level Accuracy: 0.3226\n",
      "      Apache: len of unseen log: 0000, Message-Level Accuracy: nan\n",
      "   Proxifier: len of unseen log: 0000, Message-Level Accuracy: nan\n",
      "     OpenSSH: len of unseen log: 0005, Message-Level Accuracy: 0.2000\n",
      "   OpenStack: len of unseen log: 0002, Message-Level Accuracy: 0.0000\n",
      "         Mac: len of unseen log: 0135, Message-Level Accuracy: 0.2074\n",
      "0.2799325463743676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoy\\AppData\\Local\\Temp\\ipykernel_12388\\3483590769.py:35: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print('%s: len of unseen log: %.4d, Message-Level Accuracy: %.4f' %(dataset, length ,accuracy_exact_string_matching/length))\n",
      "C:\\Users\\xiaoy\\AppData\\Local\\Temp\\ipykernel_12388\\3483590769.py:35: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print('%s: len of unseen log: %.4d, Message-Level Accuracy: %.4f' %(dataset, length ,accuracy_exact_string_matching/length))\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from evaluator import evaluate\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "# table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Linux HealthApp Apache Proxifier OpenSSH OpenStack Mac' # logpub\n",
    "datasets = table_order.split(' ')\n",
    "a = 0\n",
    "b = 0\n",
    "for dataset in datasets:\n",
    "    file = '0125_0shot'\n",
    "    input = f'outputs/parser/{file}/{dataset}.csv'  \n",
    "    output = f'outputs/unseen/{file}/{dataset}.csv'\n",
    "    os.makedirs(f'outputs/unseen/{file}', exist_ok=True)\n",
    "    df = pd.read_csv(input)\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    freq = Counter(templates)\n",
    "    logs_after = []\n",
    "    templates_after = []\n",
    "    unseen_templates = [item for item, count in freq.items() if count == 1]\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template in unseen_templates:\n",
    "            logs_after.append(log)\n",
    "            templates_after.append(template)\n",
    "    accuracy_exact_string_matching = accuracy_score(templates_after, logs_after, normalize=False)\n",
    "    length = len(logs_after)\n",
    "    a += length\n",
    "    b += accuracy_exact_string_matching\n",
    "    dataset = ' ' * (12 - len(dataset)) + dataset\n",
    "    print('%s: len of unseen log: %.4d, Message-Level Accuracy: %.4f' %(dataset, length ,accuracy_exact_string_matching/length))\n",
    "    \n",
    "\n",
    "print(b/a)\n",
    "\n",
    "# 81.0 71.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
