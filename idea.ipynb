{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量处理\n",
    "\n",
    "1. try some tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import reassign_clusters\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize(log_content):\n",
    "    list = ['/', 'kb', 'sec', 'byte', 'mb']\n",
    "    words = re.split(r'[ ,]', log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "        if any(i in word.lower() for i in list):\n",
    "            words[index] = ''\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "def vectorize(tokenized_logs):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    return vectorizer.fit_transform(tokenized_logs)\n",
    "\n",
    "\n",
    "def cluster(vectorized_logs, num_clusters='10', cluster_method='kmeans'):\n",
    "    if cluster_method == 'kmeans':\n",
    "        cluster = KMeans(n_clusters=num_clusters)\n",
    "    if cluster_method == 'dbscan':\n",
    "        cluster = DBSCAN(eps=0.1, min_samples=5)\n",
    "    cluster.fit(vectorized_logs)\n",
    "    labels = cluster.labels_\n",
    "    cluster_nums = max(labels) + 1\n",
    "    return labels, cluster_nums\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "dataset = 'Proxifier'\n",
    "\n",
    "df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "\n",
    "# 选择某一列，例如'column_name'\n",
    "\n",
    "logs = df['Content']\n",
    "templates = df['EventTemplate']\n",
    "\n",
    "# 将该列转换为列表\n",
    "column_list = logs.tolist()\n",
    "tokenized_logs = [tokenize(content) for content in column_list]\n",
    "labels, cluster_nums = cluster(\n",
    "    vectorize(tokenized_logs), cluster_method='dbscan')\n",
    "\n",
    "labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cluster_nums:', cluster_nums)\n",
    "\n",
    "for index, label in enumerate(labels):\n",
    "    if label == 1:\n",
    "        print(logs[index])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = 'Warning: we failed to resolve data source name an14 an15 an16 an17 an18 an19 an20 an21 an22 an23 an24 an25 an26 an27 an28 an29 an30 an31 an32 an33 an34 an35 an36 an37 an38 an39 an40 an41 an42 an43 an44 an45 an46 an47 an48 an49 an50 an51 an52 an53 an54 an55 an56 an57 an58 an59 an60 an61 an62 an63 an64 an65 an66 an67 an68 an69 an70 an71 an72 an73 an74 an75 an76 an77 an78 an79 an80 an81 an82 an83 an84 an85 an86 an87 an88 an89 an90 an91 an92 an93 an94 an95 an96 an97 an98 an99 an100 an101 an102 an103 an104 an105 an106 an107 an108 an109 an110 an111 an112 an113 an114 an115 an116 an117 an118 an119 an120 an121 an122 an123 an124 an125 an126 an127 an128'\n",
    "print(len(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in sample:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种聚类方式：将所有数字替换为0，不经过分词直接聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "def tokenize(log_content):\n",
    "\n",
    "    words = re.split(r'[ ,]', log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if word.startswith('/') and len(word) > 1:\n",
    "            words[index] = ''\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "\n",
    "# 假设你的日志信息已经被处理成了一个列表的列表，如下：\n",
    "dataset = 'Linux'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "\n",
    "# 选择某一列，例如'column_name'\n",
    "logs = df['Content'].tolist()\n",
    "logs = [tokenize(log) for log in logs]\n",
    "\n",
    "# 使用TransactionEncoder将日志转换为布尔值矩阵\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(logs).transform(logs)\n",
    "\n",
    "# 创建一个DataFrame\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# 使用apriori找到频繁项集\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cluster_nums:', cluster_nums)\n",
    "num = 0\n",
    "logs_test = []\n",
    "for i, l in enumerate(list(labels)):\n",
    "\n",
    "    if l == 0:  # 13是异常的模板\n",
    "\n",
    "        print(logs[i])\n",
    "        logs_test.append(logs[i])\n",
    "        num += 1\n",
    "\n",
    "# 75 + 45 + 30\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "m, n, p, q = [], [], [], []\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/Fourth_guding/{dataset}.csv'  # Fourth_guding\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a, b, c, d = evaluate(file, dataset)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print(sum(m)/len(m))\n",
    "print(sum(n)/len(n))\n",
    "print(sum(p)/len(p))\n",
    "\n",
    "# 81.0 71.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_logs = [tokenize(content) for content in logs]\n",
    "labels = cluster(vectorize(tokenized_logs), method='dbscan')\n",
    "num = 0\n",
    "for i, l in enumerate(labels):\n",
    "\n",
    "    if l == 0:  # 12, 14 , 17, 29是异常的模板\n",
    "        print(logs[i])\n",
    "        num += 1\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "logs_1 = []\n",
    "for tokenized_log in tokenized_logs:\n",
    "    logs_1.append(' '.join(tokenized_log))\n",
    "freq = Counter(logs_1)\n",
    "print(freq)\n",
    "count1 = [0,0]\n",
    "count2 = [0,0]\n",
    "for key, value in freq.items():\n",
    "    if value == 1:\n",
    "        count1[0] += 1\n",
    "        count1[1] += 1\n",
    "    else:\n",
    "        count2[0] += 1\n",
    "        count2[1] += value\n",
    "print(count1)\n",
    "print(count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示每一批分组情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过三个参数选取最合适的n值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def tokenize(log_content):\n",
    "    words = re.split(r'[#= ,]', log_content)\n",
    "    words = [word for word in words if not re.search(r'\\d', word)]\n",
    "    return words\n",
    "\n",
    "\n",
    "def vectorize(tokenized_logs):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    return vectorizer.fit_transform(tokenized_logs)\n",
    "\n",
    "\n",
    "def cluster(vectorized_logs, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(vectorized_logs)\n",
    "    # 计算聚类误差\n",
    "    cluster_error = kmeans.inertia_\n",
    "    # 计算轮廓系数\n",
    "    silhouette_avg = silhouette_score(vectorized_logs, kmeans.labels_)\n",
    "    # 计算Calinski-Harabasz指数\n",
    "    calinski_harabasz_avg = calinski_harabasz_score(vectorized_logs.toarray(), kmeans.labels_)\n",
    "    return kmeans.labels_, cluster_error, silhouette_avg, calinski_harabasz_avg\n",
    "\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# 读取CSV文件\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "\n",
    "    # 选择某一列，例如'column_name'\n",
    "    column = df['Content']\n",
    "\n",
    "    # 将该列转换为列表\n",
    "    column_list = column.tolist()\n",
    "\n",
    "    tokenized_logs = [tokenize(content) for content in column_list]\n",
    "\n",
    "\n",
    "    vectorized_logs = vectorize(tokenized_logs)\n",
    "\n",
    "    range_n_clusters = range(250, 251)\n",
    "    # 聚类误差\n",
    "    errors = []\n",
    "\n",
    "    # 轮廓系数\n",
    "    silhouettes = []\n",
    "\n",
    "    # Calinski-Harabasz指数\n",
    "    calinski_harabasz_scores = []\n",
    "\n",
    "    # for n_clusters in range_n_clusters:\n",
    "    n_clusters = 250\n",
    "    # 聚类\n",
    "    labels, cluster_error, silhouette_avg, calinski_harabasz_avg = cluster(vectorized_logs, n_clusters)\n",
    "\n",
    "    # errors.append(cluster_error)\n",
    "    # silhouettes.append(silhouette_avg)\n",
    "    # calinski_harabasz_scores.append(calinski_harabasz_avg)\n",
    "\n",
    "# # 绘制聚类误差图\n",
    "# plt.figure()\n",
    "# plt.plot(range_n_clusters, errors, 'o-')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Cluster error')\n",
    "# plt.title('The Elbow Method')\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制轮廓系数图\n",
    "# plt.figure()\n",
    "# plt.plot(range_n_clusters, silhouettes, 'o-')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Silhouette Coefficient')\n",
    "# plt.title('The Silhouette Method')\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制Calinski-Harabasz指数图\n",
    "# plt.figure()\n",
    "# plt.plot(range_n_clusters, calinski_harabasz_scores, 'o-')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Calinski-Harabasz Index')\n",
    "# plt.title('The Calinski-Harabasz Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 假设log_data是我们的日志数据，已经被转换为数值向量\n",
    "log_data = np.random.rand(2000, 5)  # 这只是一个示例，你需要用你的实际数据替换这里\n",
    "\n",
    "# 读取CSV文件\n",
    "dataset = 'HPC'\n",
    "df = pd.read_csv(\n",
    "    f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "logs = df['Content']\n",
    "# 使用TF-IDF将日志信息转换为数值向量\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(logs)\n",
    "# 初始化DBSCAN对象\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "\n",
    "# 对数据进行拟合\n",
    "dbscan.fit(X)\n",
    "\n",
    "# 获取聚类标签\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for i, l in enumerate(labels):\n",
    "    if l == -1:  # 12, 14 , 17, 29是异常的模板\n",
    "        print(logs[i])\n",
    "        num += 1\n",
    "# 75 + 45 + 30\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_entropy(lst):\n",
    "    # 计算列表中每个元素出现的频率\n",
    "    counter = Counter(lst)\n",
    "    probs = [count / len(lst) for count in counter.values()]\n",
    "\n",
    "    # 计算信息熵\n",
    "    entropy = -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "    return entropy\n",
    "def select_log_template_pairs_based_on_entropy(pairs, num_examples):\n",
    "    # 计算每个对的信息熵\n",
    "    entropies = [(pair, calculate_entropy(list(pair[0]) + list(pair[1])))\n",
    "                 for pair in pairs]\n",
    "\n",
    "    # 根据信息熵对对进行排序\n",
    "    sorted_pairs = sorted(entropies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择信息熵最高的对\n",
    "    selected_pairs = sorted_pairs[:num_examples]\n",
    "\n",
    "    return [pair for pair, entropy in selected_pairs]\n",
    "\n",
    "# discard the target dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets.remove('BGL')\n",
    "demonstration_templates = []\n",
    "demonstration_logs = []\n",
    "pairs = []\n",
    "for d in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{d}\\{d}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(list1, list2):\n",
    "        if template not in demonstration_templates:\n",
    "            pairs.append((log, template))\n",
    "            demonstration_templates.append(template)\n",
    "            demonstration_logs.append(log)\n",
    "\n",
    "list =  select_log_template_pairs_based_on_entropy(pairs, 10)\n",
    "for log, template in list:\n",
    "    print(log)\n",
    "    print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = print899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# discard the target dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets.remove('Mac')\n",
    "templates = []\n",
    "logs = []\n",
    "pairs = []\n",
    "for d in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{d}\\{d}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(list1, list2):\n",
    "        if template not in templates:\n",
    "            pairs.append((log, template))\n",
    "            templates.append(template)\n",
    "            logs.append(log)\n",
    "\n",
    "print(len(logs))\n",
    "\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def calculate_entropy(values):\n",
    "    counter = Counter(lst)\n",
    "    probs = [count / len(lst) for count in counter.values()]\n",
    "\n",
    "    # 计算信息熵\n",
    "    entropy = -sum(p * math.log2(p) for p in probs)\n",
    "    value_counts = defaultdict(int)\n",
    "    for value in values:\n",
    "        value_counts[value] += 1\n",
    "    entropy = 0\n",
    "    for count in value_counts.values():\n",
    "        p = count / len(values)\n",
    "        entropy -= p * math.log(p, 2)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# 收集每个模板动态部分的所有可能值\n",
    "variable_values = defaultdict(list)\n",
    "for i, (log, template) in enumerate(zip(logs, templates)):\n",
    "    variables = extract_variables(log, template)\n",
    "    for variable in variables:\n",
    "        variable_values[i].append(variable)\n",
    "\n",
    "print(variable_values)\n",
    "\n",
    "# 计算每个动态部分的信息熵\n",
    "entropies = {}\n",
    "for var_index, values in variable_values.items():\n",
    "    entropy = calculate_entropy(values)\n",
    "    entropies[var_index] = entropy\n",
    "\n",
    "n = 10  # 举例，找出熵最大的2个对\n",
    "sorted_entropies = sorted(\n",
    "    entropies.items(), key=lambda item: item[1], reverse=True)\n",
    "top_n_indices = [index for index, entropy in sorted_entropies[:n]]\n",
    "\n",
    "for index in top_n_indices:\n",
    "    print(f'Log: {logs[index]}\\nTemplate: {templates[index]}\\nEntropy: {entropies[index]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
