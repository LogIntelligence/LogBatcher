{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBCSAN Clustering\n",
    "`另一种聚类方式：将所有数字替换为0，不经过分词直接聚类`\n",
    "``` python\n",
    "re.sub(r'\\d+(\\.\\d+)?', '0', text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Clustering HealthApp dataset...\n",
      "  (0, 100)\t1.0\n",
      "  (1, 97)\t1.0\n",
      "  (2, 16)\t0.6248332566193072\n",
      "  (2, 11)\t0.5520794333347835\n",
      "  (2, 98)\t0.5520794333347835\n",
      "  (3, 12)\t0.8517032026021748\n",
      "  (3, 105)\t0.5240244790820354\n",
      "  (4, 45)\t0.5668201002003888\n",
      "  (4, 121)\t0.5825439786011101\n",
      "  (4, 64)\t0.5825439786011101\n",
      "  (5, 72)\t1.0\n",
      "  (6, 125)\t1.0\n",
      "  (7, 100)\t1.0\n",
      "  (8, 156)\t0.7071067811865475\n",
      "  (8, 31)\t0.7071067811865475\n",
      "  (9, 155)\t0.7071067811865475\n",
      "  (9, 30)\t0.7071067811865475\n",
      "  (10, 0)\t0.7064072898835545\n",
      "  (10, 8)\t0.7078055812151892\n",
      "  (11, 97)\t1.0\n",
      "  (12, 100)\t1.0\n",
      "  (13, 100)\t1.0\n",
      "  (14, 97)\t1.0\n",
      "  (15, 72)\t1.0\n",
      "  (16, 125)\t1.0\n",
      "  :\t:\n",
      "  (1987, 105)\t0.6960412725776134\n",
      "  (1988, 13)\t0.7180017735831414\n",
      "  (1988, 105)\t0.6960412725776134\n",
      "  (1989, 13)\t0.7180017735831414\n",
      "  (1989, 105)\t0.6960412725776134\n",
      "  (1990, 13)\t0.7180017735831414\n",
      "  (1990, 105)\t0.6960412725776134\n",
      "  (1991, 13)\t0.7180017735831414\n",
      "  (1991, 105)\t0.6960412725776134\n",
      "  (1992, 13)\t0.7180017735831414\n",
      "  (1992, 105)\t0.6960412725776134\n",
      "  (1993, 13)\t0.7180017735831414\n",
      "  (1993, 105)\t0.6960412725776134\n",
      "  (1994, 13)\t0.7180017735831414\n",
      "  (1994, 105)\t0.6960412725776134\n",
      "  (1995, 13)\t0.7180017735831414\n",
      "  (1995, 105)\t0.6960412725776134\n",
      "  (1996, 13)\t0.7180017735831414\n",
      "  (1996, 105)\t0.6960412725776134\n",
      "  (1997, 13)\t0.7180017735831414\n",
      "  (1997, 105)\t0.6960412725776134\n",
      "  (1998, 13)\t0.7180017735831414\n",
      "  (1998, 105)\t0.6960412725776134\n",
      "  (1999, 13)\t0.7180017735831414\n",
      "  (1999, 105)\t0.6960412725776134\n",
      "onStandStepChanged 3579\n",
      "onStandStepChanged <*>\n",
      "260\n",
      "==================================================\n",
      "onExtend:1514038530000 14 0 4\n",
      "onExtend:<*> <*> <*> <*>\n",
      "273\n",
      "==================================================\n",
      "onReceive action: android.intent.action.SCREEN_ON\n",
      "onReceive action: android.intent.action.SCREEN_ON\n",
      "17\n",
      "==================================================\n",
      "processHandleBroadcastAction action:android.intent.action.SCREEN_ON\n",
      "processHandleBroadcastAction action:android.intent.action.SCREEN_ON\n",
      "17\n",
      "==================================================\n",
      "flush sensor data\n",
      "flush sensor data\n",
      "17\n",
      "==================================================\n",
      " getTodayTotalDetailSteps = 1514039760000##7189##549659##8661##16256##28479559\n",
      "getTodayTotalDetailSteps = <*>##<*>##<*>##<*>##<*>##<*>\n",
      "242\n",
      "==================================================\n",
      "setTodayTotalDetailSteps=1514038440000##7007##548365##8661##12361##27173954\n",
      "setTodayTotalDetailSteps=<*>##<*>##<*>##<*>##<*>##<*>\n",
      "241\n",
      "==================================================\n",
      "calculateCaloriesWithCache totalCalories=126775\n",
      "calculateCaloriesWithCache totalCalories=<*>\n",
      "241\n",
      "==================================================\n",
      "calculateAltitudeWithCache totalAltitude=240\n",
      "calculateAltitudeWithCache totalAltitude=<*>\n",
      "241\n",
      "==================================================\n",
      "REPORT : 7007 5002 150089 240\n",
      "REPORT : <*> <*> <*> <*>\n",
      "136\n",
      "==================================================\n",
      "onReceive action: android.intent.action.SCREEN_OFF\n",
      "onReceive action: android.intent.action.SCREEN_OFF\n",
      "17\n",
      "==================================================\n",
      "processHandleBroadcastAction action:android.intent.action.TIME_TICK\n",
      "processHandleBroadcastAction action:android.intent.action.TIME_TICK\n",
      "144\n",
      "==================================================\n",
      "new date =20171223, type=40004,5112.0,old=4985.0\n",
      "new date =<*>, type=<*>,<*>,old=<*>\n",
      "17\n",
      "==================================================\n",
      "Count : 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import reassign_clusters, cluster, vectorize, tokenize,Cluster\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "datasets = ['HealthApp']\n",
    "\n",
    "for dataset in datasets:\n",
    "    print('-' * 50)\n",
    "    print(f'Clustering {dataset} dataset...')\n",
    "\n",
    "    # load the dataset\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "\n",
    "    # tokenize -> vectorize -> cluster -> reassign_clusters\n",
    "    tokenized_logs = [tokenize(log) for log in logs]\n",
    "    print(vectorize(tokenized_logs))\n",
    "    labels, cluster_nums = cluster(vectorize(tokenized_logs))\n",
    "    labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)\n",
    "\n",
    "    # store the logs in the cluster\n",
    "    inputs = []\n",
    "    for i in range(cluster_nums):\n",
    "        inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "    for i, label in enumerate(labels):\n",
    "        inputs[label][0] = label\n",
    "        inputs[label][1].append(logs[i])\n",
    "        inputs[label][2].append(i)\n",
    "        if inputs[label][3] == '':\n",
    "            inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "    clusters = []\n",
    "    for input in inputs:\n",
    "        c = Cluster(*input, remove_duplicate= True)\n",
    "        clusters.append(c)\n",
    "    Count = 0\n",
    "    for c in clusters:    \n",
    "        if len(c.indexs) > 10:\n",
    "            print(c.logs[0])\n",
    "            print(c.oracle_template)\n",
    "            print(len(c.indexs))\n",
    "            print('=' * 50)\n",
    "            Count+=1\n",
    "    print(f'Count : {Count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clusters: 75\n",
      "len of templates: 75\n",
      "onReceive action: android.intent.action.SCREEN_ON\n",
      "onReceive action: android.intent.action.SCREEN_ON\n",
      "========================================\n",
      "processHandleBroadcastAction action:android.intent.action.SCREEN_ON\n",
      "processHandleBroadcastAction action:android.intent.action.SCREEN_ON\n",
      "========================================\n",
      "flush sensor data\n",
      "flush sensor data\n",
      "========================================\n",
      "onReceive action: android.intent.action.SCREEN_OFF\n",
      "onReceive action: android.intent.action.SCREEN_OFF\n",
      "========================================\n",
      "processHandleBroadcastAction action:android.intent.action.TIME_TICK\n",
      "processHandleBroadcastAction action:android.intent.action.TIME_TICK\n",
      "========================================\n",
      "getBinderPackageName packageName = com.huawei.health\n",
      "getBinderPackageName packageName = com.huawei.health\n",
      "========================================\n",
      "needAutoSync autoSyncSwitch is open\n",
      "needAutoSync autoSyncSwitch is open\n",
      "========================================\n",
      "initDataPrivacy the dataPrivacy switch is open, start push health data!\n",
      "initDataPrivacy the dataPrivacy switch is open, start push health data!\n",
      "========================================\n",
      "initDataPrivacy the dataPrivacy is true\n",
      "initDataPrivacy the dataPrivacy is <*>\n",
      "========================================\n",
      "initUserPrivacy the userPrivacy switch is open, start push user data!\n",
      "initUserPrivacy the userPrivacy switch is open, start push user data!\n",
      "========================================\n",
      "initUserPrivacy the userPrivacy is true\n",
      "initUserPrivacy the userPrivacy is <*>\n",
      "========================================\n",
      "ifCanSync not! no cloud version\n",
      "ifCanSync not! no cloud version\n",
      "========================================\n",
      "sendSyncFailedBroadcast\n",
      "sendSyncFailedBroadcast\n",
      "========================================\n",
      "isScreenOn true\n",
      "isScreenOn <*>\n",
      "========================================\n",
      "screen status unknown,think screen on\n",
      "screen status unknown,think screen on\n",
      "========================================\n",
      "flushTempCacheToDB by stand\n",
      "flushTempCacheToDB by stand\n",
      "========================================\n",
      "getAppContext() isAppValid health or wear, packageName = com.huawei.health\n",
      "getAppContext() isAppValid health or wear, packageName = <*>\n",
      "========================================\n",
      "uploadStaticsToDB failed message=true\n",
      "uploadStaticsToDB failed message=<*>\n",
      "========================================\n",
      "checkInsertStatus stepSum or calorieSum is enough\n",
      "checkInsertStatus stepSum or calorieSum is enough\n",
      "========================================\n",
      "checkInsertStatus stepStatSum or calorieStatSum is enough\n",
      "checkInsertStatus stepStatSum or calorieStatSum is enough\n",
      "========================================\n",
      "setWriteDBLastDataMinute success\n",
      "setWriteDBLastDataMinute success\n",
      "========================================\n",
      "startTimer start autoSync\n",
      "startTimer start autoSync\n",
      "========================================\n",
      "initEnviroument\n",
      "initEnviroument\n",
      "========================================\n",
      "getStepCounterStatus\n",
      "getStepCounterStatus\n",
      "========================================\n",
      " getStepCounterStatus= true\n",
      "getStepCounterStatus= <*>\n",
      "========================================\n",
      "reStartStepCounter\n",
      "reStartStepCounter\n",
      "========================================\n",
      "registersensorsuccess: true\n",
      "registersensorsuccess: <*>\n",
      "========================================\n",
      "clear()\n",
      "clear()\n",
      "========================================\n",
      "tryToRecordAsBasicStepData bWrite true\n",
      "tryToRecordAsBasicStepData bWrite <*>\n",
      "========================================\n",
      "closeNotification...\n",
      "closeNotification...\n",
      "========================================\n",
      "deleteHealthNotification()\n",
      "deleteHealthNotification()\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print('num of clusters:', cluster_nums)\n",
    "print('len of templates:', len(set(templates)))\n",
    "\n",
    "# store the logs in the cluster\n",
    "inputs = []\n",
    "for i in range(cluster_nums):\n",
    "    inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "for i, label in enumerate(labels):\n",
    "    inputs[label][0] = label\n",
    "    inputs[label][1].append(logs[i])\n",
    "    inputs[label][2].append(i)\n",
    "    if inputs[label][3] == '':\n",
    "        inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "clusters = []\n",
    "for input in inputs:\n",
    "    c = Cluster(*input, remove_duplicate= True)\n",
    "    clusters.append(c)\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     print(f'cluster {cluster.label}: {len(cluster.logs)} logs, {len(cluster.indexs)} indexs')\n",
    "\n",
    "# num = 27\n",
    "# print('cluster:', num)\n",
    "# print('length:', len(clusters[num].indexs))\n",
    "# print('template:', clusters[num].oracle_template)\n",
    "# print('len of set:', len(clusters[num].logs))\n",
    "# print('-'*20)\n",
    "# for log in clusters[num].logs:\n",
    "#     print(log)\n",
    "# print('='*40)\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     if len(set(cluster.logs)) == 1 and not any(char.isdigit() for char in cluster.logs[0]):\n",
    "#         print(f\"{cluster.logs[0]}\\n{cluster.oracle_template}\")\n",
    "#         print('='*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HDFS: group Accuracy: 1.0000, Message-Level Accuracy: 1.0000, Edit Distance: 0.0000, Normalized Edit Distance: 1.000000\n",
      "      Hadoop: group Accuracy: 0.9890, Message-Level Accuracy: 0.8865, Edit Distance: 6.5760, Normalized Edit Distance: 0.951896\n",
      "       Spark: group Accuracy: 0.9230, Message-Level Accuracy: 0.9135, Edit Distance: 0.6600, Normalized Edit Distance: 0.984732\n",
      "   Zookeeper: group Accuracy: 0.9745, Message-Level Accuracy: 0.9675, Edit Distance: 0.5705, Normalized Edit Distance: 0.991814\n",
      "         BGL: group Accuracy: 0.9925, Message-Level Accuracy: 0.9375, Edit Distance: 1.9295, Normalized Edit Distance: 0.986919\n",
      "         HPC: group Accuracy: 0.9525, Message-Level Accuracy: 0.9430, Edit Distance: 0.3755, Normalized Edit Distance: 0.995286\n",
      " Thunderbird: group Accuracy: 0.9135, Message-Level Accuracy: 0.8525, Edit Distance: 2.2000, Normalized Edit Distance: 0.951577\n",
      "     Windows: group Accuracy: 1.0000, Message-Level Accuracy: 0.6095, Edit Distance: 12.0005, Normalized Edit Distance: 0.795368\n",
      "       Linux: group Accuracy: 0.9960, Message-Level Accuracy: 0.7280, Edit Distance: 2.4395, Normalized Edit Distance: 0.967769\n",
      "     Android: group Accuracy: 0.8605, Message-Level Accuracy: 0.7340, Edit Distance: 4.5260, Normalized Edit Distance: 0.945881\n",
      "   HealthApp: group Accuracy: 0.9950, Message-Level Accuracy: 0.9800, Edit Distance: 0.5605, Normalized Edit Distance: 0.990098\n",
      "      Apache: group Accuracy: 1.0000, Message-Level Accuracy: 0.9780, Edit Distance: 0.2300, Normalized Edit Distance: 0.996156\n",
      "     OpenSSH: group Accuracy: 1.0000, Message-Level Accuracy: 0.9755, Edit Distance: 1.0610, Normalized Edit Distance: 0.989056\n",
      "   OpenStack: group Accuracy: 1.0000, Message-Level Accuracy: 0.9820, Edit Distance: 0.4005, Normalized Edit Distance: 0.994646\n",
      "         Mac: group Accuracy: 0.8120, Message-Level Accuracy: 0.5245, Edit Distance: 10.9110, Normalized Edit Distance: 0.863470\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>GA</th>\n",
       "      <th>PA</th>\n",
       "      <th>ED</th>\n",
       "      <th>N_ED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>HDFS</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hadoop</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.8865</td>\n",
       "      <td>6.5760</td>\n",
       "      <td>0.95190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Spark</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.9135</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>0.98473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Zookeeper</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>0.9675</td>\n",
       "      <td>0.5705</td>\n",
       "      <td>0.99181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BGL</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>1.9295</td>\n",
       "      <td>0.98692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HPC</td>\n",
       "      <td>0.9525</td>\n",
       "      <td>0.9430</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.99529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Thunderbird</td>\n",
       "      <td>0.9135</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>2.2000</td>\n",
       "      <td>0.95158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Windows</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>12.0005</td>\n",
       "      <td>0.79537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linux</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>2.4395</td>\n",
       "      <td>0.96777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Android</td>\n",
       "      <td>0.8605</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>4.5260</td>\n",
       "      <td>0.94588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HealthApp</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.99010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Apache</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.99616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>OpenSSH</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>1.0610</td>\n",
       "      <td>0.98906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>OpenStack</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.4005</td>\n",
       "      <td>0.99465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mac</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.5245</td>\n",
       "      <td>10.9110</td>\n",
       "      <td>0.86347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>avg</td>\n",
       "      <td>0.9606</td>\n",
       "      <td>0.8675</td>\n",
       "      <td>2.9627</td>\n",
       "      <td>0.96031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import evaluate_all_datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "table = evaluate_all_datasets(\n",
    "    'LogBatcher_0shot_32candidate_10batchsize_with_random', send_email=False)\n",
    "HTML(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         BGL: group Accuracy: 0.9870, Message-Level Accuracy: 0.9410, Edit Distance: 1.2245, Normalized Edit Distance: 0.989453\n",
      "        HDFS: group Accuracy: 1.0000, Message-Level Accuracy: 1.0000, Edit Distance: 0.0000, Normalized Edit Distance: 1.000000\n",
      "   HealthApp: group Accuracy: 0.9195, Message-Level Accuracy: 0.9140, Edit Distance: 2.5880, Normalized Edit Distance: 0.961141\n",
      "   OpenStack: group Accuracy: 1.0000, Message-Level Accuracy: 0.9820, Edit Distance: 0.3585, Normalized Edit Distance: 0.995158\n",
      "     OpenSSH: group Accuracy: 1.0000, Message-Level Accuracy: 0.9755, Edit Distance: 1.0610, Normalized Edit Distance: 0.989056\n",
      "         HPC: group Accuracy: 0.9525, Message-Level Accuracy: 0.9430, Edit Distance: 0.3350, Normalized Edit Distance: 0.995339\n",
      "   Zookeeper: group Accuracy: 0.9945, Message-Level Accuracy: 0.9875, Edit Distance: 0.4305, Normalized Edit Distance: 0.994671\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluate_single_dataset\n",
    "datasets = ['BGL', 'HDFS', 'HealthApp', 'OpenStack', 'OpenSSH', 'HPC', 'Zookeeper']\n",
    "for dataset in datasets:\n",
    "    evaluate_single_dataset(\n",
    "        f'outputs/parser/Test_10shot_with_pruning/{dataset}_2k.log_structured.csv', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out some strings' freq in the whole logs or templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BGL ----------------\n",
      "Processing HDFS ----------------\n",
      "Processing Linux ----------------\n",
      "Processing HealthApp ----------------\n",
      "Processing OpenStack ----------------\n",
      "Processing OpenSSH ----------------\n",
      "Processing HPC ----------------\n",
      "Processing Zookeeper ----------------\n",
      "Processing Mac ----------------\n",
      "Processing Hadoop ----------------\n",
      "Processing Android ----------------\n",
      "Processing Windows ----------------\n",
      "Processing Apache ----------------\n",
      "Processing Thunderbird ----------------\n",
      "Processing Spark ----------------\n",
      "1334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import tokenize\n",
    "from utils.sample_byword import extract_variables\n",
    "from utils.postprocess import correct_single_template\n",
    "from utils.postprocess import extract_variables\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "count = 0\n",
    "count_templates = []\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset} ----------------\")\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    # templates = list(set(templates))\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in count_templates:\n",
    "            count_templates.append(template)\n",
    "            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample based on entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sample import sample_based_on_entropy\n",
    "\n",
    "# datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "#         'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# for dataset in datasets:\n",
    "#     pair = sample_based_on_entropy(dataset, 1)\n",
    "#     print(pair[0][0])\n",
    "dataset = 'HDFS'\n",
    "pairs = sample_based_on_entropy(dataset, 1)\n",
    "for pair in pairs:\n",
    "    print(f\"{pair[0][0]}\\n{pair[0][1]}\\n{'-'*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation Count -- num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4: 15 + 28 + 5 + 18 + 10\n",
    "# 5: 1 + 2\n",
    "# 6: 15 + 28 + 5 + 18 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7139 509.92857142857144\n",
      "22342 204.9724770642202\n",
      "6609 200.27272727272728\n",
      "9222 174.0\n",
      "23509 195.90833333333333\n",
      "6639 144.32608695652175\n",
      "25522 157.54320987654322\n",
      "8501 170.02\n",
      "17485 158.95454545454547\n",
      "31405 196.28125\n",
      "10799 143.98666666666668\n",
      "1234 205.66666666666666\n",
      "5853 225.1153846153846\n",
      "16935 413.0487804878049\n",
      "73047 216.7566765578635\n",
      "266241 198.39120715350222\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_message_tokens(messages, model_name):\n",
    "    # 根据模型名称加载合适的编码器\n",
    "    if model_name == \"gpt-4\":\n",
    "        encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    elif model_name == \"gpt-3.5-turbo\":\n",
    "        encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    else:\n",
    "        raise ValueError(\"未知的模型名称\")\n",
    "\n",
    "    # 初始化token计数\n",
    "    token_count = 0\n",
    "\n",
    "    # 计算每个消息的token数\n",
    "    for message in messages:\n",
    "        role_tokens = encoder.encode(message['role'])\n",
    "        content_tokens = encoder.encode(message['content'])\n",
    "        token_count += len(role_tokens) + \\\n",
    "            len(content_tokens) + 4  # 加上特殊的消息分隔符的token数\n",
    "\n",
    "    return token_count\n",
    "\n",
    "\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "logs = {}\n",
    "counts_token = {}\n",
    "counts_message = {}\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs[dataset] = df['Content'].tolist()\n",
    "    counts_token[dataset] = 0\n",
    "    counts_message[dataset] = 0\n",
    "for log in logs['HealthApp']:\n",
    "    log = log.strip()\n",
    "\n",
    "# 存储解析后的日志列表\n",
    "message_list = []\n",
    "# load every message\n",
    "file = 'cost_lilac_32_3.json'\n",
    "with open('outputs/cost/cost_logbatcher_32_1.json', 'r') as file:\n",
    "    for line in file:\n",
    "        if line.strip() == '[':\n",
    "            list_str = ''\n",
    "            start_load = True\n",
    "        if line.strip() == ']':\n",
    "            list_str += line\n",
    "            message = json.loads(list_str)\n",
    "            message_list.append(message)\n",
    "            start_load = False\n",
    "        if start_load:\n",
    "            list_str += line\n",
    "# print(len(message_list))\n",
    "for message in message_list:\n",
    "    # for LILAC\n",
    "    log = message[-1]['content'].split('\\n')[0].replace('Log message: `', '').replace('`', '')\n",
    "    # for LogBatcher\n",
    "    # log = message[-1]['content'].split('\\n')[0] \n",
    "    # print(log)\n",
    "    for dataset in datasets:\n",
    "        if log in logs[dataset]:\n",
    "            counts_token[dataset] += count_message_tokens(message, 'gpt-3.5-turbo')\n",
    "            counts_message[dataset] += 1\n",
    "            break\n",
    "        if dataset == 'Mac':\n",
    "            print(log)\n",
    "for dataset in datasets:\n",
    "    print(counts_token[dataset], counts_token[dataset] / counts_message[dataset]  )\n",
    "\n",
    "print(sum(counts_token.values()), sum(counts_token.values()) / sum(counts_message.values()))\n",
    "\n",
    "# remove the same log messages\n",
    "\n",
    "# def make_hashable(log_list):\n",
    "\n",
    "#     return tuple(tuple(sorted(d.items())) for d in log_list)\n",
    "# unique_lists = list(set(make_hashable(log_list) for log_list in message_list))\n",
    "\n",
    "# unique_big_list = [list(map(dict, log_list)) for log_list in unique_lists]\n",
    "# print(len(unique_big_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
