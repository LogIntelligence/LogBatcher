{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBCSAN Clustering\n",
    "`另一种聚类方式：将所有数字替换为0，不经过分词直接聚类`\n",
    "``` python\n",
    "re.sub(r'\\d+(\\.\\d+)?', '0', text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils.cluster import reassign_clusters, cluster, vectorize, tokenize,Cluster\n",
    "\n",
    "# select the dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "dataset = 'HDFS'\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "logs = df['Content'].tolist()\n",
    "templates = df['EventTemplate'].tolist()\n",
    "\n",
    "# tokenize -> vectorize -> cluster -> reassign_clusters\n",
    "tokenized_logs = [tokenize(log) for log in logs]\n",
    "labels, cluster_nums = cluster(vectorize(tokenized_logs))\n",
    "labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num of clusters:', cluster_nums)\n",
    "\n",
    "\n",
    "# store the logs in the cluster\n",
    "inputs = []\n",
    "for i in range(cluster_nums):\n",
    "    inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "for i, label in enumerate(labels):\n",
    "    inputs[label][0] = label\n",
    "    inputs[label][1].append(logs[i])\n",
    "    inputs[label][2].append(i)\n",
    "    if inputs[label][3] == '':\n",
    "        inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "# check the cluster k\n",
    "k = 5\n",
    "lengh_cluster = len(inputs[k][1])\n",
    "print('cluster ', k)\n",
    "print('length:', lengh_cluster)\n",
    "print('template:', inputs[k][3])\n",
    "print('-'*20)\n",
    "for log in set(inputs[k][1]):\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils.evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "m,n,p,q = [],[],[],[]\n",
    "\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/0125_0shot/{dataset}.csv'  # Fifth_=_0.1\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a,b,c,d = evaluate(file, dataset)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print('avg---------: group Accuracy: %.4f, Message-Level Accuracy: %.4f, Edit Distance: %.4f' % (sum(m)/len(m), sum(n)/len(n), sum(p)/len(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## caculatee the information entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def calculate_entropy(lst):\n",
    "    # 计算列表中每个元素出现的频率\n",
    "\n",
    "    # list to str\n",
    "    # print(''.join(lst))\n",
    "\n",
    "    counter = Counter(lst)\n",
    "    probs = [count / len(lst) for count in counter.values()]\n",
    "\n",
    "    # 计算信息熵\n",
    "    entropy = -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "    return entropy\n",
    "def select_log_template_pairs_based_on_entropy(pairs, num_examples):\n",
    "    # 计算每个对的信息熵\n",
    "    entropies = [(pair, calculate_entropy(list(pair[0]) + list(pair[1])))  # list(pair[0]) + list(pair[1]) / extract_variables(pair[0], pair[1])\n",
    "                 for pair in pairs]\n",
    "\n",
    "    # 根据信息熵对对进行排序\n",
    "    sorted_pairs = sorted(entropies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择信息熵最高的对\n",
    "    selected_pairs = sorted_pairs[:num_examples]\n",
    "\n",
    "    return [pair for pair, entropy in selected_pairs]\n",
    "\n",
    "# discard the target dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets.remove('BGL')\n",
    "demonstration_templates = []\n",
    "demonstration_logs = []\n",
    "pairs = []\n",
    "for d in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{d}\\{d}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(list1, list2):\n",
    "        if template not in demonstration_templates:\n",
    "            pairs.append((log, template))\n",
    "            demonstration_templates.append(template)\n",
    "            demonstration_logs.append(log)\n",
    "\n",
    "list =  select_log_template_pairs_based_on_entropy(pairs, 10)\n",
    "for log, template in list:\n",
    "    print(log)\n",
    "    print(template)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
