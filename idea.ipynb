{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBCSAN Clustering\n",
    "`另一种聚类方式：将所有数字替换为0，不经过分词直接聚类`\n",
    "``` python\n",
    "re.sub(r'\\d+(\\.\\d+)?', '0', text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Android dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils.cluster import reassign_clusters, cluster, vectorize, tokenize,Cluster\n",
    "import time\n",
    "\n",
    "# select the dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# num_list = []\n",
    "# datasets = ['OpenStack']\n",
    "# for dataset in datasets:\n",
    "dataset = 'Android'\n",
    "print(f'Processing {dataset} dataset...')\n",
    "# load the dataset\n",
    "df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "logs = df['Content'].tolist()\n",
    "# logs.extend(df['Content'].tolist())\n",
    "templates = df['EventTemplate'].tolist()\n",
    "\n",
    "# tokenize -> vectorize -> cluster -> reassign_clusters\n",
    "tokenized_logs = [tokenize(log) for log in logs]\n",
    "labels, cluster_nums = cluster(vectorize(tokenized_logs))\n",
    "labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clusters: 174\n",
      "len of templates: 158\n",
      "cluster: 0\n",
      "length: 199\n",
      "template: ready=<*>,policy=<*>,wakefulness=<*>,wksummary=<*>,uasummary=<*>,bootcompleted=<*>,boostinprogress=<*>,waitmodeenable=<*>,mode=<*>,manual=<*>,auto=<*>,adj=<*>.0userId=<*>\n",
      "--------------------\n",
      "ready=true,policy=3,wakefulness=1,wksummary=0x0,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "ready=true,policy=3,wakefulness=1,wksummary=0x1,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "ready=true,policy=3,wakefulness=1,wksummary=0x23,uasummary=0x1,bootcompleted=true,boostinprogress=false,waitmodeenable=false,mode=false,manual=38,auto=-1,adj=0.0userId=0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print('num of clusters:', cluster_nums)\n",
    "print('len of templates:', len(set(templates)))\n",
    "\n",
    "# store the logs in the cluster\n",
    "inputs = []\n",
    "for i in range(cluster_nums):\n",
    "    inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "for i, label in enumerate(labels):\n",
    "    inputs[label][0] = label\n",
    "    inputs[label][1].append(logs[i])\n",
    "    inputs[label][2].append(i)\n",
    "    if inputs[label][3] == '':\n",
    "        inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "num = 0\n",
    "print('cluster:', num)\n",
    "print('length:', len(inputs[num][1]))\n",
    "print('template:', inputs[num][3])\n",
    "print('-'*20)\n",
    "for log in set(inputs[num][1]):\n",
    "    print(log)\n",
    "print('='*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>GA</th>\n",
       "      <th>PA</th>\n",
       "      <th>ED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>HDFS</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Hadoop</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.851</td>\n",
       "      <td>7.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Spark</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.897</td>\n",
       "      <td>2.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Zookeeper</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BGL</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.912</td>\n",
       "      <td>2.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HPC</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Thunderbird</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.866</td>\n",
       "      <td>2.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Windows</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.966</td>\n",
       "      <td>1.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Linux</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.694</td>\n",
       "      <td>2.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Android</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.566</td>\n",
       "      <td>8.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HealthApp</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.766</td>\n",
       "      <td>6.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Apache</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Proxifier</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.026</td>\n",
       "      <td>21.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>OpenSSH</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>OpenStack</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.462</td>\n",
       "      <td>32.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mac</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.440</td>\n",
       "      <td>14.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>avg</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.768</td>\n",
       "      <td>6.409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import evaluate_all_datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "table = evaluate_all_datasets('Test2', send_email=False)\n",
    "HTML(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out some strings' freq in the whole logs or templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import tokenize\n",
    "from utils.sample_byword import extract_variables\n",
    "\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "from_log = False # True for log, False for template\n",
    "check_string = ['.','/'] # check if the log or template contains the string\n",
    "\n",
    "count_templates = []\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset} ----------------\")\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in count_templates:\n",
    "            count_templates.append(template)\n",
    "            if from_log and all(str in log for str in check_string):\n",
    "                print(f\"{log}\\n{template}\")\n",
    "            elif not from_log and all(str in template for str in check_string):\n",
    "                print(f\"{log}\\n{template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample based on entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.demonstrations_sample import sample_based_on_entropy\n",
    "\n",
    "# datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "#         'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# for dataset in datasets:\n",
    "#     pair = sample_based_on_entropy(dataset, 1)\n",
    "#     print(pair[0][0])\n",
    "dataset = 'HDFS'\n",
    "pairs = sample_based_on_entropy(dataset, 1)\n",
    "for pair in pairs:\n",
    "    print(f\"{pair[0][0]}\\n{pair[0][1]}\\n{'-'*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation Count -- num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4: 15 + 28 + 5 + 18 + 10\n",
    "# 5: 1 + 2\n",
    "# 6: 15 + 28 + 5 + 18 + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "from utils.cluster import tokenize\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "a,b = 0,0\n",
    "pattern = r'^[a-zA-Z]+[0-9]+$'\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset} ----------------\")\n",
    "    list_log = []\n",
    "    list_tmp = []\n",
    "    print('-'*20)\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    logs = df['Content'].tolist()\n",
    "    freq = Counter(templates)\n",
    "    \n",
    "    for template,log in zip(templates,logs):\n",
    "        tokens = template.split()\n",
    "        for token in tokens:\n",
    "            if ':' in token and '<*>' in token:\n",
    "                # print(f\"{template}\\n{log}\\n{'-'*20}\")\n",
    "                list_tmp.append(template)\n",
    "                list_log.append(log)\n",
    "                break\n",
    "            \n",
    "    for tmp in list_tmp:\n",
    "        print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.postprocess import correct_single_template\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "# datasets = ['Linux']\n",
    "\n",
    "count_list = []\n",
    "shot = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset} dataset...\")\n",
    "    with open (f'outputs/parser/Test/{dataset}.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    Read = False\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        if 'len=' in line and 'cluster' in line:\n",
    "            if count > 5:\n",
    "                print(length)\n",
    "                for tmp in tmp_list:\n",
    "                    print(tmp.strip('\\n'))\n",
    "                shot+=1\n",
    "            count = 0\n",
    "            tmp_list = []\n",
    "            parts = line.strip().split(\"len=\")\n",
    "            if len(parts) == 2:  # 确保字符串中包含\"len=\"\n",
    "                tmp = parts[1]\n",
    "                length = int(tmp)\n",
    "            if length > 50:\n",
    "                Read = True\n",
    "            else:\n",
    "                Read = False\n",
    "        else:\n",
    "            if Read:\n",
    "                tmp_list.append(line)\n",
    "                # print(line)\n",
    "                count += 1\n",
    "\n",
    "print(shot/len(datasets))\n",
    "# [147, 49, 156, 98, 65, 202, 45, 61, 82, 361, 143, 164, 93, 43, 203, 69]\n",
    "# [137, 49, 121, 94, 62, 56, 45, 56, 78, 322, 135, 141, 86, 43, 175, 63]\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from utils.sample_byword import extract_variables\n",
    "\n",
    "matches = extract_variables(\n",
    "    '1 is over than 2 as result', '<*> is over than <*> as result')\n",
    "if matches == []:\n",
    "    print('no matches')\n",
    "if matches == ():\n",
    "    print(\"2\")\n",
    "else:\n",
    "    print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<*> is <*>, logname=\n"
     ]
    }
   ],
   "source": [
    "template = '<*> is <*>, logname=<*>'\n",
    "list2 = ['1', '2', '']\n",
    "list1 = template.split('<*>')\n",
    "template2 = list1[0]\n",
    "for index, tmp in enumerate(list2):\n",
    "    if tmp != '':\n",
    "        template2 += '<*>' + list1[index + 1]\n",
    "    else:\n",
    "        template2 += list1[index + 1]\n",
    "print(template2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('{{1}} is over {{2}}, logname = {{3}}', '<*> is over <*>, logname = ')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.postprocess import post_process\n",
    "\n",
    "post_process('`{{1}} is over {{2}}, logname = {{3}}`', '1 is over 2, logname = ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
