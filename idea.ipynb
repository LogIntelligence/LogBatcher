{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量处理\n",
    "\n",
    "1. try some tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\anaconda\\envs\\langchain\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from parsing import reassign_clusters\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(log_content):\n",
    "\n",
    "    words = re.split(r'[ ,]', log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if word.startswith('/') and len(word) > 1:\n",
    "            words[index] = ''\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "def vectorize(tokenized_logs):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    return vectorizer.fit_transform(tokenized_logs)\n",
    "\n",
    "\n",
    "def cluster(vectorized_logs, num_clusters='10', cluster_method='kmeans'):\n",
    "    if cluster_method == 'kmeans':\n",
    "        cluster = KMeans(n_clusters=num_clusters)\n",
    "    if cluster_method == 'dbscan':\n",
    "        cluster = DBSCAN(eps=0.1, min_samples=5)\n",
    "    cluster.fit(vectorized_logs)\n",
    "    labels = cluster.labels_\n",
    "    cluster_nums = max(labels) + 1\n",
    "    return labels, cluster_nums\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "dataset = 'Thunderbird'\n",
    "\n",
    "df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "\n",
    "# 选择某一列，例如'column_name'\n",
    "\n",
    "logs = df['Content']\n",
    "templates = df['EventTemplate']\n",
    "\n",
    "# 将该列转换为列表\n",
    "column_list = logs.tolist()\n",
    "tokenized_logs = [tokenize(content) for content in column_list]\n",
    "labels, cluster_nums = cluster(\n",
    "    vectorize(tokenized_logs), cluster_method='dbscan')\n",
    "\n",
    "labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_nums: 179\n",
      "Reading included configuration file: /etc/xinetd.d/chargen [file=/etc/xinetd.conf] [line=15]\n",
      "Reading included configuration file: /etc/xinetd.d/chargen-udp [file=/etc/xinetd.d/chargen-udp] [line=18]\n",
      "Reading included configuration file: /etc/xinetd.d/cups-lpd [file=/etc/xinetd.d/cups-lpd] [line=17]\n",
      "Reading included configuration file: /etc/xinetd.d/daytime [file=/etc/xinetd.d/daytime] [line=11]\n",
      "Reading included configuration file: /etc/xinetd.d/daytime-udp [file=/etc/xinetd.d/daytime-udp] [line=15]\n",
      "Reading included configuration file: /etc/xinetd.d/echo [file=/etc/xinetd.d/echo] [line=15]\n",
      "Reading included configuration file: /etc/xinetd.d/echo-udp [file=/etc/xinetd.d/echo-udp] [line=14]\n",
      "Reading included configuration file: /etc/xinetd.d/eklogin [file=/etc/xinetd.d/eklogin] [line=15]\n",
      "Reading included configuration file: /etc/xinetd.d/gssftp [file=/etc/xinetd.d/gssftp] [line=13]\n",
      "Reading included configuration file: /etc/xinetd.d/klogin [file=/etc/xinetd.d/klogin] [line=14]\n",
      "Reading included configuration file: /etc/xinetd.d/kshell [file=/etc/xinetd.d/kshell] [line=13]\n",
      "Reading included configuration file: /etc/xinetd.d/ktalk [file=/etc/xinetd.d/ktalk] [line=13]\n",
      "Reading included configuration file: /etc/xinetd.d/rsync [file=/etc/xinetd.d/rsync] [line=12]\n",
      "Reading included configuration file: /etc/xinetd.d/tftp [file=/etc/xinetd.d/tftp] [line=13]\n",
      "Reading included configuration file: /etc/xinetd.d/time [file=/etc/xinetd.d/time] [line=18]\n",
      "Reading included configuration file: /etc/xinetd.d/time-udp [file=/etc/xinetd.d/time-udp] [line=17]\n"
     ]
    }
   ],
   "source": [
    "print('cluster_nums:', cluster_nums)\n",
    "\n",
    "for index, label in enumerate(labels):\n",
    "    if label == 25:\n",
    "        print(logs[index])\n",
    "\n",
    "# inputs = []\n",
    "# for i in range(cluster_nums):\n",
    "#     inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "# for i, label in enumerate(labels):\n",
    "#     inputs[label][0] = label\n",
    "#     inputs[label][1].append(logs[i])\n",
    "#     inputs[label][2].append(i)\n",
    "#     if inputs[label][3] == '':\n",
    "#         inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "# sample = []\n",
    "# for input in inputs:\n",
    "#     sample.append(input[1][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    }
   ],
   "source": [
    "str = 'Warning: we failed to resolve data source name an14 an15 an16 an17 an18 an19 an20 an21 an22 an23 an24 an25 an26 an27 an28 an29 an30 an31 an32 an33 an34 an35 an36 an37 an38 an39 an40 an41 an42 an43 an44 an45 an46 an47 an48 an49 an50 an51 an52 an53 an54 an55 an56 an57 an58 an59 an60 an61 an62 an63 an64 an65 an66 an67 an68 an69 an70 an71 an72 an73 an74 an75 an76 an77 an78 an79 an80 an81 an82 an83 an84 an85 an86 an87 an88 an89 an90 an91 an92 an93 an94 an95 an96 an97 an98 an99 an100 an101 an102 an103 an104 an105 an106 an107 an108 an109 an110 an111 an112 an113 an114 an115 an116 an117 an118 an119 an120 an121 an122 an123 an124 an125 an126 an127 an128'\n",
    "print(len(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
      "check pass; user unknown\n",
      "authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
      "session opened for user cyrus by (uid=0)\n",
      "session closed for user cyrus\n",
      "ALERT exited abnormally with [1]\n",
      "session opened for user news by (uid=0)\n",
      "session closed for user news\n",
      "connection from 24.54.76.216 (24-54-76-216.bflony.adelphia.net) at Fri Jun 17 07:07:00 2005\n",
      "session opened for user test by (uid=509)\n",
      "session closed for user test\n",
      "connection from 82.252.162.81 (lns-vlq-45-tou-82-252-162-81.adsl.proxad.net) at Sat Jun 18 02:08:10 2005\n",
      "cupsd shutdown succeeded\n",
      "cupsd startup succeeded\n",
      "restart.\n",
      "connection from 222.33.90.199 () at Mon Jun 20 03:40:59 2005\n",
      "connection from 210.245.165.136 () at Wed Jun 22 13:16:30 2005\n",
      "connection from 218.69.108.57 () at Fri Jun 24 18:55:11 2005\n",
      "authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=massive.merukuru.org\n",
      "connection from 210.118.170.95 () at Sat Jun 25 09:20:24 2005\n",
      "authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=troi.bluesky-technologies.com  user=root\n",
      "Authentication failed from 163.27.187.39 (163.27.187.39): Permission denied in replay cache code\n",
      "Kerberos authentication failed\n",
      "Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort\n",
      "connection from 202.82.200.188 () at Fri Jul  1 07:57:30 2005\n",
      "connection from 203.101.45.59 (dsl-Chn-static-059.45.101.203.touchtelindia.net) at Sun Jul  3 10:05:25 2005\n",
      "connection from 63.197.98.106 (adsl-63-197-98-106.dsl.mtry01.pacbell.net) at Mon Jul  4 12:52:44 2005\n",
      "connection from 211.72.2.106 () at Tue Jul  5 13:52:21 2005\n",
      "connection from 211.72.151.162 () at Wed Jul  6 18:00:56 2005\n",
      "connection from 202.82.200.188 () at Thu Jul  7 16:33:52 2005\n",
      "connection from 211.57.88.250 () at Sat Jul  9 11:35:59 2005\n",
      "connection from 206.196.21.129 (host129.206.196.21.maximumasp.com) at Sat Jul  9 22:53:19 2005\n",
      "connection from 217.187.83.139 () at Sun Jul 10 03:55:15 2005\n",
      "connection from 211.72.151.162 () at Mon Jul 18 03:26:48 2005\n",
      "notify question section contains no SOA\n",
      "User unknown timed out after 900 seconds at Sat Jun 18 02:23:10 2005\n",
      "Received SNMP packet(s) from 67.170.148.126\n",
      "*** info [mice.c(1766)]:\n",
      "imps2: Auto-detected intellimouse PS/2\n",
      "session opened for user root by LOGIN(uid=0)\n",
      "ROOT LOGIN ON tty2\n",
      "session closed for user root\n",
      "removing device node '/udev/vcsa2'\n",
      "creating device node '/udev/vcs2'\n",
      "authentication failure; logname= uid=0 euid=0 tty=:0 ruser= rhost=\n",
      "Couldn't authenticate user\n",
      "ANONYMOUS FTP LOGIN FROM 84.102.20.2,  (anonymous)\n",
      "getpeername (ftpd): Transport endpoint is not connected\n",
      "warning: can't get client address: Connection reset by peer\n",
      "syslogd startup succeeded\n",
      "klogd 1.4.1, log source = /proc/kmsg started.\n",
      "Linux version 2.6.5-1.358 (bhcompile@bugs.build.redhat.com) (gcc version 3.3.3 20040412 (Red Hat Linux 3.3.3-7)) #1 Sat May 8 09:04:50 EDT 2004\n",
      "BIOS-provided physical RAM map:\n",
      "BIOS-e820: 0000000000000000 - 00000000000a0000 (usable)\n",
      "BIOS-e820: 00000000000f0000 - 0000000000100000 (reserved)\n",
      "0MB HIGHMEM available.\n",
      "126MB LOWMEM available.\n",
      "zapping low mappings.\n",
      "klogd startup succeeded\n",
      "On node 0 totalpages: 32430\n",
      "DMA zone: 4096 pages, LIFO batch:1\n",
      "Normal zone: 28334 pages, LIFO batch:6\n",
      "irqbalance startup succeeded\n",
      "HighMem zone: 0 pages, LIFO batch:1\n",
      "DMI 2.3 present.\n",
      "ACPI disabled because your bios is from 2000 and too old\n",
      "You can enable it with acpi=force\n",
      "Built 1 zonelists\n",
      "Kernel command line: ro root=LABEL=/ rhgb quiet\n",
      "mapped 4G/4G trampoline to ffff3000.\n",
      "Initializing CPU#0\n",
      "CPU 0 irqstacks, hard=02345000 soft=02344000\n",
      "portmap startup succeeded\n",
      "PID hash table entries: 512 (order 9: 4096 bytes)\n",
      "Detected 731.219 MHz processor.\n",
      "Version 1.0.6 Starting\n",
      "Using tsc for high-res timesource\n",
      "rpc.statd startup succeeded\n",
      "Console: colour VGA+ 80x25\n",
      "Memory: 125312k/129720k available (1540k kernel code, 3860k reserved, 599k data, 144k init, 0k highmem)\n",
      "Calibrating delay loop... 1445.88 BogoMIPS\n",
      "Security Scaffold v1.0.0 initialized\n",
      "SELinux:  Initializing.\n",
      "SELinux:  Starting in permissive mode\n",
      "There is already a security framework initialized, register_security failed.\n",
      "Failure registering capabilities with the kernel\n",
      "selinux_register_security:  Registering secondary module capability\n",
      "Capability LSM initialized\n",
      "rpc.idmapd startup succeeded\n",
      "Dentry cache hash table entries: 16384 (order: 4, 65536 bytes)\n",
      "Inode-cache hash table entries: 8192 (order: 3, 32768 bytes)\n",
      "Mount-cache hash table entries: 512 (order: 0, 4096 bytes)\n",
      "CPU: L1 I cache: 16K, L1 D cache: 16K\n",
      "CPU: L2 cache: 256K\n",
      "Intel machine check architecture supported.\n",
      "Intel machine check reporting enabled on CPU#0.\n",
      "CPU: Intel Pentium III (Coppermine) stepping 06\n",
      "Enabling fast FPU save and restore... done.\n",
      "Enabling unmasked SIMD FPU exception support... done.\n",
      "Checking 'hlt' instruction... OK.\n",
      "Initializing random number generator:  succeeded\n",
      "POSIX conformance testing by UNIFIX\n",
      "NET: Registered protocol family 16\n",
      "PCI: PCI BIOS revision 2.10 entry at 0xfc0ce, last bus=1\n",
      "PCI: Using configuration type 1\n",
      "mtrr: v2.0 (20020519)\n",
      "ACPI: Subsystem revision 20040326\n",
      "ACPI: Interpreter disabled.\n",
      "Linux Plug and Play Support v0.97 (c) Adam Belay\n",
      "Starting pcmcia:  succeeded\n",
      "usbcore: registered new driver usbfs\n",
      "usbcore: registered new driver hub\n",
      "ACPI: ACPI tables contain no PCI IRQ routing entries\n",
      "PCI: Invalid ACPI-PCI IRQ routing table\n",
      "PCI: Probing PCI hardware\n",
      "PCI: Probing PCI hardware (bus 00)\n",
      "Transparent bridge - 0000:00:1e.0\n",
      "PCI: Using IRQ router PIIX/ICH [8086/2410] at 0000:00:1f.0\n",
      "apm: BIOS version 1.2 Flags 0x03 (Driver version 1.16ac)\n",
      "audit: initializing netlink socket (disabled)\n",
      "kernel.core_uses_pid = 1\n",
      "HCI daemon ver 2.4 started\n",
      "hcid startup succeeded\n",
      "audit(1122475266.4294965305:0): initialized\n",
      "Setting network parameters:  succeeded\n",
      "sdpd startup succeeded\n",
      "sdpd v1.5 started\n",
      "Total HugeTLB memory allocated, 0\n",
      "Bringing up loopback interface:  succeeded\n",
      "VFS: Disk quotas dquot_6.5.1\n",
      "Dquot-cache hash table entries: 1024 (order 0, 4096 bytes)\n",
      "SELinux:  Registering netfilter hooks\n",
      "Initializing Cryptographic API\n",
      "pci_hotplug: PCI Hot Plug PCI Core version: 0.5\n",
      "isapnp: Scanning for PnP cards...\n",
      "isapnp: No Plug & Play device found\n",
      "Real Time Clock Driver v1.12\n",
      "Linux agpgart interface v0.100 (c) Dave Jones\n"
     ]
    }
   ],
   "source": [
    "for log in sample:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种聚类方式：将所有数字替换为0，不经过分词直接聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support                     itemsets\n",
      "0    0.3085                         (())\n",
      "1    0.3735                        (Jul)\n",
      "2    0.4560                         (at)\n",
      "3    0.4620                 (connection)\n",
      "4    0.4670                       (from)\n",
      "5    0.3680                       (user)\n",
      "6    0.3085                     (at, ())\n",
      "7    0.3085             ((), connection)\n",
      "8    0.3085                   ((), from)\n",
      "9    0.3735                    (at, Jul)\n",
      "10   0.3735            (Jul, connection)\n",
      "11   0.3735                  (Jul, from)\n",
      "12   0.4545             (at, connection)\n",
      "13   0.4545                   (at, from)\n",
      "14   0.4620           (connection, from)\n",
      "15   0.3085         (at, (), connection)\n",
      "16   0.3085               (at, (), from)\n",
      "17   0.3085       ((), connection, from)\n",
      "18   0.3735        (at, Jul, connection)\n",
      "19   0.3735              (at, Jul, from)\n",
      "20   0.3735      (Jul, connection, from)\n",
      "21   0.4545       (at, connection, from)\n",
      "22   0.3085   (at, (), connection, from)\n",
      "23   0.3735  (at, Jul, connection, from)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "def tokenize(log_content):\n",
    "\n",
    "    words = re.split(r'[ ,]', log_content)\n",
    "    for index, word in enumerate(words):\n",
    "        if word.startswith('/') and len(word) > 1:\n",
    "            words[index] = ''\n",
    "        if '=' in word:\n",
    "            words[index] = word.split('=')[0]\n",
    "        if re.search(r'\\d', word):\n",
    "            words[index] = ''\n",
    "\n",
    "    words = [word for word in words if word]   # remove null\n",
    "    return words\n",
    "\n",
    "\n",
    "# 假设你的日志信息已经被处理成了一个列表的列表，如下：\n",
    "dataset = 'Linux'\n",
    "\n",
    "df = pd.read_csv(\n",
    "    f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "\n",
    "# 选择某一列，例如'column_name'\n",
    "logs = df['Content'].tolist()\n",
    "logs = [tokenize(log) for log in logs]\n",
    "\n",
    "# 使用TransactionEncoder将日志转换为布尔值矩阵\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(logs).transform(logs)\n",
    "\n",
    "# 创建一个DataFrame\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# 使用apriori找到频繁项集\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cluster_nums:', cluster_nums)\n",
    "num = 0\n",
    "logs_test = []\n",
    "for i, l in enumerate(list(labels)):\n",
    "\n",
    "    if l == 0:  # 13是异常的模板\n",
    "\n",
    "        print(logs[i])\n",
    "        logs_test.append(logs[i])\n",
    "        num += 1\n",
    "\n",
    "# 75 + 45 + 30\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "m, n, p, q = [], [], [], []\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/Fourth_guding/{dataset}.csv'  # Fourth_guding\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a, b, c, d = evaluate(file, dataset)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print(sum(m)/len(m))\n",
    "print(sum(n)/len(n))\n",
    "print(sum(p)/len(p))\n",
    "\n",
    "# 81.0 71.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_logs = [tokenize(content) for content in logs]\n",
    "labels = cluster(vectorize(tokenized_logs), method='dbscan')\n",
    "num = 0\n",
    "for i, l in enumerate(labels):\n",
    "\n",
    "    if l == 0:  # 12, 14 , 17, 29是异常的模板\n",
    "        print(logs[i])\n",
    "        num += 1\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "logs_1 = []\n",
    "for tokenized_log in tokenized_logs:\n",
    "    logs_1.append(' '.join(tokenized_log))\n",
    "freq = Counter(logs_1)\n",
    "print(freq)\n",
    "count1 = [0,0]\n",
    "count2 = [0,0]\n",
    "for key, value in freq.items():\n",
    "    if value == 1:\n",
    "        count1[0] += 1\n",
    "        count1[1] += 1\n",
    "    else:\n",
    "        count2[0] += 1\n",
    "        count2[1] += value\n",
    "print(count1)\n",
    "print(count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示每一批分组情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过三个参数选取最合适的n值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def tokenize(log_content):\n",
    "    words = re.split(r'[#= ,]', log_content)\n",
    "    words = [word for word in words if not re.search(r'\\d', word)]\n",
    "    return words\n",
    "\n",
    "\n",
    "def vectorize(tokenized_logs):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    return vectorizer.fit_transform(tokenized_logs)\n",
    "\n",
    "\n",
    "def cluster(vectorized_logs, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(vectorized_logs)\n",
    "    # 计算聚类误差\n",
    "    cluster_error = kmeans.inertia_\n",
    "    # 计算轮廓系数\n",
    "    silhouette_avg = silhouette_score(vectorized_logs, kmeans.labels_)\n",
    "    # 计算Calinski-Harabasz指数\n",
    "    calinski_harabasz_avg = calinski_harabasz_score(vectorized_logs.toarray(), kmeans.labels_)\n",
    "    return kmeans.labels_, cluster_error, silhouette_avg, calinski_harabasz_avg\n",
    "\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# 读取CSV文件\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "\n",
    "    # 选择某一列，例如'column_name'\n",
    "    column = df['Content']\n",
    "\n",
    "    # 将该列转换为列表\n",
    "    column_list = column.tolist()\n",
    "\n",
    "    tokenized_logs = [tokenize(content) for content in column_list]\n",
    "\n",
    "\n",
    "    vectorized_logs = vectorize(tokenized_logs)\n",
    "\n",
    "    range_n_clusters = range(250, 251)\n",
    "    # 聚类误差\n",
    "    errors = []\n",
    "\n",
    "    # 轮廓系数\n",
    "    silhouettes = []\n",
    "\n",
    "    # Calinski-Harabasz指数\n",
    "    calinski_harabasz_scores = []\n",
    "\n",
    "    # for n_clusters in range_n_clusters:\n",
    "    n_clusters = 250\n",
    "    # 聚类\n",
    "    labels, cluster_error, silhouette_avg, calinski_harabasz_avg = cluster(vectorized_logs, n_clusters)\n",
    "\n",
    "    # errors.append(cluster_error)\n",
    "    # silhouettes.append(silhouette_avg)\n",
    "    # calinski_harabasz_scores.append(calinski_harabasz_avg)\n",
    "\n",
    "# # 绘制聚类误差图\n",
    "# plt.figure()\n",
    "# plt.plot(range_n_clusters, errors, 'o-')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Cluster error')\n",
    "# plt.title('The Elbow Method')\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制轮廓系数图\n",
    "# plt.figure()\n",
    "# plt.plot(range_n_clusters, silhouettes, 'o-')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Silhouette Coefficient')\n",
    "# plt.title('The Silhouette Method')\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制Calinski-Harabasz指数图\n",
    "# plt.figure()\n",
    "# plt.plot(range_n_clusters, calinski_harabasz_scores, 'o-')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Calinski-Harabasz Index')\n",
    "# plt.title('The Calinski-Harabasz Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 假设log_data是我们的日志数据，已经被转换为数值向量\n",
    "log_data = np.random.rand(2000, 5)  # 这只是一个示例，你需要用你的实际数据替换这里\n",
    "\n",
    "# 读取CSV文件\n",
    "dataset = 'HPC'\n",
    "df = pd.read_csv(\n",
    "    f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "logs = df['Content']\n",
    "# 使用TF-IDF将日志信息转换为数值向量\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(logs)\n",
    "# 初始化DBSCAN对象\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "\n",
    "# 对数据进行拟合\n",
    "dbscan.fit(X)\n",
    "\n",
    "# 获取聚类标签\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for i, l in enumerate(labels):\n",
    "    if l == -1:  # 12, 14 , 17, 29是异常的模板\n",
    "        print(logs[i])\n",
    "        num += 1\n",
    "# 75 + 45 + 30\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [1, 2, 4, 4, 5]\n",
    "df = pd.DataFrame({'a': list1, 'b': list2})\n",
    "print(df['a'].value_counts())\n",
    "print(df['b'].value_counts())\n",
    "\n",
    "\n",
    "logIds = df['b'][df['b'] == 4].index\n",
    "print(logIds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
