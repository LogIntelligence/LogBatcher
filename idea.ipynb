{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBCSAN Clustering\n",
    "`另一种聚类方式：将所有数字替换为0，不经过分词直接聚类`\n",
    "``` python\n",
    "re.sub(r'\\d+(\\.\\d+)?', '0', text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utils.parser import Cluster_Parser\n",
    "from utils.cluster import Cluster\n",
    "label = 1\n",
    "logs = ['asd a 1' for i in range(100)]\n",
    "index = [i for i in range(200)]\n",
    "oracle_template = 'asd a <*>'\n",
    "\n",
    "input = [label, logs, index, oracle_template]\n",
    "c = Cluster(*input)\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "p = Cluster_Parser(config)\n",
    "f = open( f'test.txt', 'w')\n",
    "t = p.get_responce(f,c)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoyi\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils.cluster import reassign_clusters, cluster, vectorize, tokenize,Cluster\n",
    "\n",
    "# select the dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets = ['OpenStack']\n",
    "dataset = 'Hadoop'\n",
    "# load the dataset\n",
    "df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "logs = df['Content'].tolist()\n",
    "templates = df['EventTemplate'].tolist()\n",
    "\n",
    "# tokenize -> vectorize -> cluster -> reassign_clusters\n",
    "tokenized_logs = [tokenize(log) for log in logs]\n",
    "labels, cluster_nums = cluster(vectorize(tokenized_logs))\n",
    "labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clusters: 116\n",
      "len of templates: 114\n",
      "cluster: 8\n",
      "length: 10\n",
      "template: Got allocated containers <*>\n",
      "--------------------\n",
      "Got allocated containers 2\n",
      "Got allocated containers 1\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print('num of clusters:', cluster_nums)\n",
    "print('len of templates:', len(set(templates)))\n",
    "\n",
    "# store the logs in the cluster\n",
    "inputs = []\n",
    "for i in range(cluster_nums):\n",
    "    inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "for i, label in enumerate(labels):\n",
    "    inputs[label][0] = label\n",
    "    inputs[label][1].append(logs[i])\n",
    "    inputs[label][2].append(i)\n",
    "    if inputs[label][3] == '':\n",
    "        inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "num = 8\n",
    "print('cluster:', num)\n",
    "print('length:', len(inputs[num][1]))\n",
    "print('template:', inputs[num][3])\n",
    "print('-'*20)\n",
    "for log in set(inputs[num][1]):\n",
    "    print(log)\n",
    "print('='*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cluster k\n",
    "# k = 0\n",
    "# lengh_cluster = len(inputs[k][1])\n",
    "# print('cluster ', k)\n",
    "# print('length:', lengh_cluster)\n",
    "# print('template:', inputs[k][3])\n",
    "# print('-'*20)\n",
    "# for log in set(inputs[k][1]):\n",
    "#     print(log)\n",
    "\n",
    "#      len\n",
    "# Linux 0.5   tokenize '=' difference between (<*>) and () group first will help\n",
    "# HealthApp: 1   same length, 2 words different(80 logs) refine by difference of words will help\n",
    "# Zookeeper: 0 same length, 2 words different(12 logs)\n",
    "# Hadoop: 0 same length 1 words different(118 logs)\n",
    "# Spark: 0  same length 1 words different(149 logs)\n",
    "\n",
    "# good cluster datasets\n",
    "# HDFS OpenStack Proxifier HPC Mac Windows Apache Thunderbird\n",
    "# length solved datasets\n",
    "# BGL OpenSSH Android\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HDFS: group Accuracy: 1.0000, Message-Level Accuracy: 0.9425, Edit Distance: 0.0000\n",
      "      Hadoop: group Accuracy: 0.9835, Message-Level Accuracy: 0.6145, Edit Distance: 8.5465\n",
      "       Spark: group Accuracy: 0.9225, Message-Level Accuracy: 0.8820, Edit Distance: 2.2545\n",
      "   Zookeeper: group Accuracy: 0.9925, Message-Level Accuracy: 0.6445, Edit Distance: 1.6580\n",
      "         BGL: group Accuracy: 0.9665, Message-Level Accuracy: 0.9480, Edit Distance: 0.8035\n",
      "         HPC: group Accuracy: 0.8460, Message-Level Accuracy: 0.8030, Edit Distance: 1.7425\n",
      " Thunderbird: group Accuracy: 0.9765, Message-Level Accuracy: 0.6255, Edit Distance: 4.6205\n",
      "     Windows: group Accuracy: 0.9955, Message-Level Accuracy: 0.9645, Edit Distance: 1.0105\n",
      "       Linux: group Accuracy: 0.6235, Message-Level Accuracy: 0.6725, Edit Distance: 3.6630\n",
      "     Android: group Accuracy: 0.9735, Message-Level Accuracy: 0.6725, Edit Distance: 4.5530\n",
      "   HealthApp: group Accuracy: 0.9985, Message-Level Accuracy: 0.5840, Edit Distance: 5.5530\n",
      "      Apache: group Accuracy: 1.0000, Message-Level Accuracy: 0.9780, Edit Distance: 0.2300\n",
      "   Proxifier: group Accuracy: 1.0000, Message-Level Accuracy: 0.0265, Edit Distance: 16.8920\n",
      "     OpenSSH: group Accuracy: 0.6245, Message-Level Accuracy: 0.9690, Edit Distance: 0.3910\n",
      "   OpenStack: group Accuracy: 1.0000, Message-Level Accuracy: 0.4630, Edit Distance: 2.6535\n",
      "         Mac: group Accuracy: 0.8285, Message-Level Accuracy: 0.4530, Edit Distance: 14.9275\n",
      "avg---------: group Accuracy: 0.9207, Message-Level Accuracy: 0.7027, Edit Distance: 4.3437\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils.evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "m,n,p,q = [],[],[],[]\n",
    "\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/Test/{dataset}.csv'  # Fifth_=_0.1\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a,b,c,d = evaluate(file, dataset)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print('avg---------: group Accuracy: %.4f, Message-Level Accuracy: %.4f, Edit Distance: %.4f' % (sum(m)/len(m), sum(n)/len(n), sum(p)/len(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## caculate the information entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def calculate_entropy(lst):\n",
    "    # 计算列表中每个元素出现的频率\n",
    "\n",
    "    # list to str\n",
    "    # print(''.join(lst))\n",
    "\n",
    "    counter = Counter(lst)\n",
    "    probs = [count / len(lst) for count in counter.values()]\n",
    "\n",
    "    # 计算信息熵\n",
    "    entropy = -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "    return entropy\n",
    "def select_log_template_pairs_based_on_entropy(pairs, num_examples):\n",
    "    # 计算每个对的信息熵\n",
    "    entropies = [(pair, calculate_entropy(list(pair[0]) + list(pair[1])))  # list(pair[0]) + list(pair[1]) / extract_variables(pair[0], pair[1])\n",
    "                 for pair in pairs]\n",
    "\n",
    "    # 根据信息熵对对进行排序\n",
    "    sorted_pairs = sorted(entropies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择信息熵最高的对\n",
    "    selected_pairs = sorted_pairs[:num_examples]\n",
    "\n",
    "    return [pair for pair, entropy in selected_pairs]\n",
    "\n",
    "# discard the target dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets.remove('BGL')\n",
    "demonstration_templates = []\n",
    "demonstration_logs = []\n",
    "pairs = []\n",
    "for d in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{d}\\{d}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(list1, list2):\n",
    "        if template not in demonstration_templates:\n",
    "            pairs.append((log, template))\n",
    "            demonstration_templates.append(template)\n",
    "            demonstration_logs.append(log)\n",
    "\n",
    "list =  select_log_template_pairs_based_on_entropy(pairs, 10)\n",
    "for log, template in list:\n",
    "    print(log)\n",
    "    print(template)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similarity in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BGL ----------------\n",
      "rts: bad message header: expecting type <*> instead of type <*> (softheader=<*> <*> <*> <*>) PSR0=<*> PSR1=<*> PRXF=<*> PIXF=<*>\n",
      "Error receiving packet on tree network, expecting type <*> instead of type <*> (softheader=<*> <*> <*> <*>) PSR0=<*> PSR1=<*> PRXF=<*> PIXF=<*>\n",
      "<*> L3 EDRAM error(s) (dcr <*>) detected and corrected\n",
      "dbcr0=<*> dbsr=<*> ccr0=<*>\n",
      "r24=<*> r25=<*> r26=<*> r27=<*>\n",
      "critical input interrupt (unit=<*> bit=<*>): warning for tree C1 wire, suppressing further interrupts of same type\n",
      "size of scratchpad portion of L3.........<*> (<*>)\n",
      "<*> L3 EDRAM error(s) (dcr <*>) detected and corrected over <*> seconds\n",
      "fpr29=<*>\n",
      "Processing HDFS ----------------\n",
      "Processing Linux ----------------\n",
      "Processing HealthApp ----------------\n",
      "flush2DB result success\n",
      "Processing OpenStack ----------------\n",
      "Processing OpenSSH ----------------\n",
      "Failed password for invalid user <*> from <*> port <*> ssh2\n",
      "Failed password for <*> from <*> port <*> ssh2\n",
      "Failed none for invalid user <*> from <*> port <*> ssh2\n",
      "Accepted password for <*> from <*> port <*> ssh2\n",
      "Processing Proxifier ----------------\n",
      "<*> open through proxy <*> SOCKS5\n",
      "Processing HPC ----------------\n",
      "NIFF: node <*> detected a failed network connection on network <*> via interface alt0\n",
      "NIFF: node <*> has detected an available network connection on network <*> via interface ee0\n",
      "NIFF: node <*> has detected an available network connection on network <*> via interface alt0\n",
      "NIFF: node <*> has detected an available network connection on network <*> via interface scip0\n",
      "Processing Zookeeper ----------------\n",
      "Processing Mac ----------------\n",
      "mDNS_DeregisterInterface: Frequent transitions for interface awdl0 (<*>)\n",
      "ARPT: <*>: wl0: wl_update_tcpkeep_seq: Original Seq: <*>, Ack: <*>, Win size: <*>\n",
      "ARPT: <*>: wl0: MDNS: <*> SRV Recs, <*> TXT Recs\n",
      "AirPort: Link Down on awdl0. Reason <*> (Unspecified).\n",
      "ARPT: <*>: wl0: Roamed or switched channel, reason #<*>, bssid <*>, last RSSI <*>\n",
      "<*> successfully changed NAT64 ifstate from <*> to <*>\n",
      "ARPT: <*>: wl0: MDNS: IPV6 Addr: <*>\n",
      "AirPort: Link Up on en0\n",
      "AirPort: Link Up on awdl0\n",
      "mDNS_DeregisterInterface: Frequent transitions for interface en0 (<*>)\n",
      "ARPT: <*>: wl0: wl_update_tcpkeep_seq: Updated seq/ack/win from UserClient Seq <*>, Ack <*>, Win size <*>\n",
      "mDNS_RegisterInterface: Frequent transitions for interface en0 (<*>)\n",
      "ARPT: <*>: wl0: setup_keepalive: Local port: <*>, Remote port: <*>\n",
      "en0: channel changed to <*>\n",
      "ARPT: <*>: wl0: setup_keepalive: interval <*>, retry_interval <*>, retry_count <*>\n",
      "ARPT: <*>: wl0: MDNS: IPV4 Addr: <*>\n",
      "<*>-[NETClientConnection <*>]_block_invoke CI46 - Hit by torpedo! <*>\n",
      "Captive: [CNInfoNetworkActive:<*>] en0: SSID 'CalVisitor' making interface primary (cache indicates network not captive)\n",
      "Captive: CNPluginHandler en0: Evaluating\n",
      "AirPort: Link Down on en0. Reason <*> (Disassociated because station leaving).\n",
      "en0: BSSID changed to <*>\n",
      "en0: <*> country code set to '<*>'.\n",
      "ARPT: <*>: wl0: setup_keepalive: Seq: <*>, Ack: <*>, Win size: <*>\n",
      "WARNING: Type1 font data isn't in the correct format required by the Adobe Type <*> Font Format specification.\n",
      "Captive: CNPluginHandler en0: Authenticated\n",
      "ARPT: <*>: wl0: setup_keepalive: Local IP: <*>\n",
      "mDNS_RegisterInterface: Frequent transitions for interface awdl0 (<*>)\n",
      "ARPT: <*>: wl0: leaveModulePoweredForOffloads: Wi-Fi will stay on.\n",
      "Captive: CNPluginHandler en0: Inactive\n",
      "<*>: IPv6 address <*> has no prefix\n",
      "en0: Supported channels <*>\n",
      "ARPT: <*>: wl0: setup_keepalive: Remote IP: <*>\n",
      "hibernate_machine_init pagesDone <*> sum2 <*>, time: <*> ms, disk(<*>) <*> Mb/s, comp bytes: <*> time: <*> ms <*> Mb/s, crypt bytes: <*> time: <*> ms <*> Mb/s\n",
      "hibernate_machine_init: state <*>, image pages <*>, sum was <*>, imageSize <*>, image1Size <*>, conflictCount <*>, nextFree <*>\n",
      "en0: manual intervention required!\n",
      "-[NETClientConnection evaluateCrazyIvan46] CI46 - Perform CrazyIvan46! <*>\n",
      "Processing Hadoop ----------------\n",
      "loaded properties from hadoop-metrics2.properties\n",
      "Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server\n",
      "Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "Started HttpServer2$SelectChannelConnectorWithSafeStartup@<*>\n",
      "Processing Android ----------------\n",
      "printFreezingDisplayLogsopening app wtoken = AppWindowToken{<*> token=Token{<*> ActivityRecord{<*> u0 <*>/.<*> t761}}}, allDrawn= <*>, startingDisplayed = <*>, startingMoved = <*>, isRelaunching = <*>\n",
      "ready=<*>,policy=<*>,wakefulness=<*>,wksummary=<*>,uasummary=<*>,bootcompleted=<*>,boostinprogress=<*>,waitmodeenable=<*>,mode=<*>,manual=<*>,auto=<*>,adj=<*>.0userId=<*>\n",
      "Skipping AppWindowToken{<*> token=Token{<*> ActivityRecord{<*> u0 <*>}}} -- going to hide\n",
      "Screen frozen for <*> due to Window{<*> u0 <*>}\n",
      "START u0 {act=<*> flg=<*> cmp=<*> (has extras)} from uid <*> on display <*>\n",
      "isBluetoothA2dpOn...\n",
      "START u0 {act=<*> cat=[<*>] flg=<*> cmp=<*> bnds=<*>} from uid <*> on display <*>\n",
      "START u0 {flg=<*> cmp=<*> (has extras)} from uid <*> on display <*>\n",
      "Processing Windows ----------------\n",
      "Processing Apache ----------------\n",
      "jk2_init() Found child <*> in scoreboard slot <*>\n",
      "jk2_init() Can't find child <*> in scoreboard\n",
      "Processing Thunderbird ----------------\n",
      "<*>: to=<*>, ctladdr=<*> (0/0), delay=<*>, xdelay=<*>, mailer=relay, pri=<*>, relay=[<*>] [<*>], dsn=<*>, stat=Deferred: Connection refused by [<*>]\n",
      "DHCPDISCOVER from <*> via eth1\n",
      "DHCPOFFER on <*> to <*> via eth1\n",
      "DHCPDISCOVER from <*> via eth1: network A_net: no free leases\n",
      "DHCPACK on <*> to <*> via eth1\n",
      "DHCPREQUEST for <*> (<*>) from <*> via eth1\n",
      "DHCPREQUEST for <*> (<*>) from <*> via eth1: unknown lease <*>.\n",
      "<*> (v001 DELL PE BKC <*> MSFT <*>) @ <*>\n",
      "PCI <*> Bridge [PCI0] (<*>)\n",
      "Processor [<*>] (supports C1)\n",
      "L2 cache: <*>\n",
      "Trace cache: <*> uops, L1 D cache: <*>\n",
      "Mounted <*> (ext2 filesystem).\n",
      "MegaRAID Model: LD <*> RAID1 <*> Rev: <*>\n",
      "eth0: <*>: NIC Link is Up <*> Mbps Full Duplex\n",
      "sda1 sda2 sda3 sda4\n",
      "probe of vesafb0 failed with error <*>\n",
      "Processing Spark ----------------\n",
      "Slf4jLogger started\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import tokenize\n",
    "from utils.sample_byword import extract_variables\n",
    "\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "    'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "count_logs = []\n",
    "count_templates = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset} ----------------\")\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in count_templates:\n",
    "            count_templates.append(template)\n",
    "            if any(char.isdigit() for char in template):\n",
    "                print(f\"{template}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
