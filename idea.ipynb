{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBCSAN Clustering\n",
    "`另一种聚类方式：将所有数字替换为0，不经过分词直接聚类`\n",
    "``` python\n",
    "re.sub(r'\\d+(\\.\\d+)?', '0', text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BGL dataset...\n",
      "Processing HDFS dataset...\n",
      "Processing Linux dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing HealthApp dataset...\n",
      "Processing OpenStack dataset...\n",
      "Processing OpenSSH dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Proxifier dataset...\n",
      "Processing HPC dataset...\n",
      "Processing Zookeeper dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Mac dataset...\n",
      "Processing Hadoop dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Android dataset...\n",
      "Processing Windows dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Apache dataset...\n",
      "Processing Thunderbird dataset...\n",
      "Processing Spark dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils.cluster import reassign_clusters, cluster, vectorize, tokenize,Cluster\n",
    "\n",
    "# select the dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "num_list = []\n",
    "# datasets = ['OpenStack']\n",
    "for dataset in datasets:\n",
    "    print(f'Processing {dataset} dataset...')\n",
    "    # load the dataset\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "\n",
    "    # tokenize -> vectorize -> cluster -> reassign_clusters\n",
    "    tokenized_logs = [tokenize(log) for log in logs]\n",
    "    labels, cluster_nums = cluster(vectorize(tokenized_logs))\n",
    "    num_list.append(cluster_nums)\n",
    "    # labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 11, 33, 25, 40, 28, 6, 26, 18, 107, 28, 61, 8, 6, 26, 16]\n",
      "24.3125\n"
     ]
    }
   ],
   "source": [
    "print(num_list)\n",
    "print((sum(num_list)-14-3-77)/len(num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clusters: 61\n",
      "len of templates: 158\n",
      "cluster: 17\n",
      "length: 85\n",
      "template: HBM brightnessOut =<*>\n",
      "--------------------\n",
      "HBM brightnessOut =38\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print('num of clusters:', cluster_nums)\n",
    "print('len of templates:', len(set(templates)))\n",
    "\n",
    "# store the logs in the cluster\n",
    "inputs = []\n",
    "for i in range(cluster_nums):\n",
    "    inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "for i, label in enumerate(labels):\n",
    "    inputs[label][0] = label\n",
    "    inputs[label][1].append(logs[i])\n",
    "    inputs[label][2].append(i)\n",
    "    if inputs[label][3] == '':\n",
    "        inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "num = 17\n",
    "print('cluster:', num)\n",
    "print('length:', len(inputs[num][1]))\n",
    "print('template:', inputs[num][3])\n",
    "print('-'*20)\n",
    "for log in set(inputs[num][1]):\n",
    "    print(log)\n",
    "print('='*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cluster k\n",
    "# k = 0\n",
    "# lengh_cluster = len(inputs[k][1])\n",
    "# print('cluster ', k)\n",
    "# print('length:', lengh_cluster)\n",
    "# print('template:', inputs[k][3])\n",
    "# print('-'*20)\n",
    "# for log in set(inputs[k][1]):\n",
    "#     print(log)\n",
    "\n",
    "#      len\n",
    "# Linux 0.5   tokenize '=' difference between (<*>) and () group first will help\n",
    "# HealthApp: 1   same length, 2 words different(80 logs) refine by difference of words will help\n",
    "# Zookeeper: 0 same length, 2 words different(12 logs)\n",
    "# Hadoop: 0 same length 1 words different(118 logs)\n",
    "# Spark: 0  same length 1 words different(149 logs)\n",
    "\n",
    "# good cluster datasets\n",
    "# HDFS OpenStack Proxifier HPC Mac Windows Apache Thunderbird\n",
    "# length solved datasets\n",
    "# BGL OpenSSH Android\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HDFS: group Accuracy: 1.0000, Message-Level Accuracy: 1.0000, Edit Distance: 0.0000\n",
      "      Hadoop: group Accuracy: 0.9835, Message-Level Accuracy: 0.6145, Edit Distance: 8.5465\n",
      "       Spark: group Accuracy: 0.9220, Message-Level Accuracy: 0.8780, Edit Distance: 1.9155\n",
      "   Zookeeper: group Accuracy: 0.9925, Message-Level Accuracy: 0.6445, Edit Distance: 1.6580\n",
      "         BGL: group Accuracy: 0.9665, Message-Level Accuracy: 0.9475, Edit Distance: 0.8035\n",
      "         HPC: group Accuracy: 0.8455, Message-Level Accuracy: 0.8480, Edit Distance: 1.7425\n",
      " Thunderbird: group Accuracy: 0.9740, Message-Level Accuracy: 0.6265, Edit Distance: 4.6205\n",
      "     Windows: group Accuracy: 0.9955, Message-Level Accuracy: 0.9645, Edit Distance: 1.0105\n",
      "       Linux: group Accuracy: 0.6235, Message-Level Accuracy: 0.6040, Edit Distance: 3.6630\n",
      "     Android: group Accuracy: 0.9735, Message-Level Accuracy: 0.6725, Edit Distance: 4.5530\n",
      "   HealthApp: group Accuracy: 0.9985, Message-Level Accuracy: 0.7890, Edit Distance: 5.5530\n",
      "      Apache: group Accuracy: 1.0000, Message-Level Accuracy: 0.9780, Edit Distance: 0.2300\n",
      "   Proxifier: group Accuracy: 1.0000, Message-Level Accuracy: 0.0265, Edit Distance: 16.8920\n",
      "     OpenSSH: group Accuracy: 0.6245, Message-Level Accuracy: 0.9690, Edit Distance: 0.3910\n",
      "   OpenStack: group Accuracy: 1.0000, Message-Level Accuracy: 0.4630, Edit Distance: 2.6535\n",
      "         Mac: group Accuracy: 0.8285, Message-Level Accuracy: 0.4450, Edit Distance: 14.9275\n",
      "avg---------: group Accuracy: 0.9205, Message-Level Accuracy: 0.7169, Edit Distance: 4.3225\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils.evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "\n",
    "datasets = table_order.split(' ')\n",
    "m,n,p,q = [],[],[],[]\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/0125_0shot_refined_noTrunc/{dataset}.csv'  # Fifth_=_0.1\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a,b,c,d = evaluate(file, dataset,mismatch=True)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print('avg---------: group Accuracy: %.4f, Message-Level Accuracy: %.4f, Edit Distance: %.4f' % (sum(m)/len(m), sum(n)/len(n), sum(p)/len(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similarity in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import tokenize\n",
    "from utils.sample_byword import extract_variables\n",
    "\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "    'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "count_logs = []\n",
    "count_templates = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset} ----------------\")\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in count_templates:\n",
    "            count_templates.append(template)\n",
    "            if any(char.isdigit() for char in template):\n",
    "                print(f\"{template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-02 15:46:41.445 ksfetch[32435/0x7fff79824000] [lvl=2] main() ksfetch fetching URL (<NSMutableURLRequest: 0x1005110b0> { URL: https://tools.google.com/service/update2?cup2hreq=53f725cf03f511fab16f19e789ce64aa1eed72395fc246e9f1100748325002f4&cup2key=7:1132320327 }) to folder:/tmp/KSOutOfProcessFetcher.YH2CjY1tnx/download\n",
      "<*> ksfetch[<*>] [lvl=<*>] main() ksfetch fetching URL (<NSMutableURLRequest: <*> { URL: <*> }) to folder:<*>\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.demonstrations_sample import sample_based_on_entropy\n",
    "\n",
    "# datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "#         'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# for dataset in datasets:\n",
    "#     pair = sample_based_on_entropy(dataset, 1)\n",
    "#     print(pair[0][0])\n",
    "dataset = 'HDFS'\n",
    "pairs = sample_based_on_entropy(dataset, 1)\n",
    "for pair in pairs:\n",
    "    print(f\"{pair[0][0]}\\n{pair[0][1]}\\n{'-'*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGL r24=<*> r25=<*> r26=<*> r27=<*>\n",
      "Proxifier <*> open through proxy <*> SOCKS5\n",
      "[48, 17, 18, 4, 7, 2, 7, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "    'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "Count = [0 for _ in range(10)]\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    list_read = []\n",
    "    for template in templates:\n",
    "        if template not in list_read:\n",
    "            list_read.append(template)\n",
    "            if '3' in template:\n",
    "                Count[3] += 1\n",
    "            if '4' in template:\n",
    "                Count[4] += 1\n",
    "                # Mac 15 + 28 + 5 + 18 + 10\n",
    "            if '5' in template:\n",
    "                Count[5] += 1\n",
    "                print(dataset, template)\n",
    "                # 1 + 2\n",
    "            if '6' in template:\n",
    "                Count[6] += 1\n",
    "                # Mac 15 + 28 + 5 + 18 + 10\n",
    "\n",
    "                \n",
    "                \n",
    "# 87\n",
    "# 0\n",
    "print(Count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
