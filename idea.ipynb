{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBCSAN Clustering\n",
    "`另一种聚类方式：将所有数字替换为0，不经过分词直接聚类`\n",
    "``` python\n",
    "re.sub(r'\\d+(\\.\\d+)?', '0', text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BGL dataset...\n",
      "Processing HDFS dataset...\n",
      "Processing Linux dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing HealthApp dataset...\n",
      "Processing OpenStack dataset...\n",
      "Processing OpenSSH dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Proxifier dataset...\n",
      "Processing HPC dataset...\n",
      "Processing Zookeeper dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Mac dataset...\n",
      "Processing Hadoop dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Android dataset...\n",
      "Processing Windows dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Apache dataset...\n",
      "Processing Thunderbird dataset...\n",
      "Processing Spark dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Develop\\anaconda\\envs\\langchain38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils.cluster import reassign_clusters, cluster, vectorize, tokenize,Cluster\n",
    "\n",
    "# select the dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "num_list = []\n",
    "# datasets = ['OpenStack']\n",
    "for dataset in datasets:\n",
    "    print(f'Processing {dataset} dataset...')\n",
    "    # load the dataset\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "\n",
    "    # tokenize -> vectorize -> cluster -> reassign_clusters\n",
    "    tokenized_logs = [tokenize(log) for log in logs]\n",
    "    labels, cluster_nums = cluster(vectorize(tokenized_logs))\n",
    "    num_list.append(cluster_nums)\n",
    "    # labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 11, 33, 25, 40, 28, 6, 26, 18, 107, 28, 61, 8, 6, 26, 16]\n",
      "24.3125\n"
     ]
    }
   ],
   "source": [
    "print(num_list)\n",
    "print((sum(num_list)-14-3-77)/len(num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of clusters: 61\n",
      "len of templates: 158\n",
      "cluster: 17\n",
      "length: 85\n",
      "template: HBM brightnessOut =<*>\n",
      "--------------------\n",
      "HBM brightnessOut =38\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print('num of clusters:', cluster_nums)\n",
    "print('len of templates:', len(set(templates)))\n",
    "\n",
    "# store the logs in the cluster\n",
    "inputs = []\n",
    "for i in range(cluster_nums):\n",
    "    inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "for i, label in enumerate(labels):\n",
    "    inputs[label][0] = label\n",
    "    inputs[label][1].append(logs[i])\n",
    "    inputs[label][2].append(i)\n",
    "    if inputs[label][3] == '':\n",
    "        inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "num = 17\n",
    "print('cluster:', num)\n",
    "print('length:', len(inputs[num][1]))\n",
    "print('template:', inputs[num][3])\n",
    "print('-'*20)\n",
    "for log in set(inputs[num][1]):\n",
    "    print(log)\n",
    "print('='*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cluster k\n",
    "# k = 0\n",
    "# lengh_cluster = len(inputs[k][1])\n",
    "# print('cluster ', k)\n",
    "# print('length:', lengh_cluster)\n",
    "# print('template:', inputs[k][3])\n",
    "# print('-'*20)\n",
    "# for log in set(inputs[k][1]):\n",
    "#     print(log)\n",
    "\n",
    "#      len\n",
    "# Linux 0.5   tokenize '=' difference between (<*>) and () group first will help\n",
    "# HealthApp: 1   same length, 2 words different(80 logs) refine by difference of words will help\n",
    "# Zookeeper: 0 same length, 2 words different(12 logs)\n",
    "# Hadoop: 0 same length 1 words different(118 logs)\n",
    "# Spark: 0  same length 1 words different(149 logs)\n",
    "\n",
    "# good cluster datasets\n",
    "# HDFS OpenStack Proxifier HPC Mac Windows Apache Thunderbird\n",
    "# length solved datasets\n",
    "# BGL OpenSSH Android\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HDFS: group Accuracy: 1.0000, Message-Level Accuracy: 0.9995, Edit Distance: 0.0025\n",
      "      Hadoop: group Accuracy: 0.9925, Message-Level Accuracy: 0.9855, Edit Distance: 0.4690\n",
      "       Spark: group Accuracy: 0.9985, Message-Level Accuracy: 0.9960, Edit Distance: 0.1100\n",
      "   Zookeeper: group Accuracy: 0.9945, Message-Level Accuracy: 0.9900, Edit Distance: 0.0930\n",
      "         BGL: group Accuracy: 0.9900, Message-Level Accuracy: 0.9835, Edit Distance: 0.4530\n",
      "         HPC: group Accuracy: 0.9345, Message-Level Accuracy: 0.9670, Edit Distance: 0.3395\n",
      " Thunderbird: group Accuracy: 0.9865, Message-Level Accuracy: 0.9770, Edit Distance: 0.2580\n",
      "     Windows: group Accuracy: 0.9955, Message-Level Accuracy: 0.9880, Edit Distance: 0.9870\n",
      "       Linux: group Accuracy: 0.6305, Message-Level Accuracy: 0.8950, Edit Distance: 1.3375\n",
      "     Android: group Accuracy: 0.9520, Message-Level Accuracy: 0.8615, Edit Distance: 2.2785\n",
      "   HealthApp: group Accuracy: 1.0000, Message-Level Accuracy: 0.9960, Edit Distance: 0.0645\n",
      "      Apache: group Accuracy: 1.0000, Message-Level Accuracy: 1.0000, Edit Distance: 0.0000\n",
      "   Proxifier: group Accuracy: 1.0000, Message-Level Accuracy: 0.9985, Edit Distance: 0.0330\n",
      "     OpenSSH: group Accuracy: 0.6845, Message-Level Accuracy: 0.9905, Edit Distance: 0.0385\n",
      "   OpenStack: group Accuracy: 1.0000, Message-Level Accuracy: 0.9985, Edit Distance: 0.0705\n",
      "         Mac: group Accuracy: 0.8455, Message-Level Accuracy: 0.7920, Edit Distance: 4.1415\n",
      "avg---------: group Accuracy: 0.9378, Message-Level Accuracy: 0.9637, Edit Distance: 0.6673\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils.evaluator import evaluate\n",
    "import pandas as pd\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "            'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache Proxifier OpenSSH OpenStack Mac'\n",
    "\n",
    "datasets = table_order.split(' ')\n",
    "m,n,p,q = [],[],[],[]\n",
    "for dataset in datasets:\n",
    "    file = f'outputs/parser/Test1/{dataset}.csv'  # Fifth_=_0.1\n",
    "    # df = pd.read_csv(f'outputs/k_means/initial/{dataset}.csv')\n",
    "    # df2 =\n",
    "    a,b,c,d = evaluate(file, dataset,mismatch=True)\n",
    "    m.append(a)\n",
    "    n.append(b)\n",
    "    p.append(c)\n",
    "    q.append(d)\n",
    "\n",
    "print('avg---------: group Accuracy: %.4f, Message-Level Accuracy: %.4f, Edit Distance: %.4f' % (sum(m)/len(m), sum(n)/len(n), sum(p)/len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## caculate the information entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_variables(log, template):\n",
    "    # 将模板中的 <*> 替换为正则表达式的捕获组 (.*?)\n",
    "    # 为了避免正则表达式的特殊字符导致的问题，先将模板中除了 <*> 外的其他部分进行转义\n",
    "    # 然后将 <*> 替换为正则表达式的捕获组\n",
    "    # 这里假设模板中的 <*> 不紧邻正则特殊字符，如果有，需要更复杂的处理\n",
    "    pattern_parts = template.split(\"<*>\")\n",
    "    pattern_parts_escaped = [re.escape(part) for part in pattern_parts]\n",
    "    regex_pattern = \"(.*?)\".join(pattern_parts_escaped)\n",
    "    regex = \"^\" + regex_pattern + \"$\"  # 添加开始和结束锚点以确保完整匹配\n",
    "\n",
    "    matches = re.search(regex, log)\n",
    "    if matches:\n",
    "        return matches.groups()\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def calculate_entropy(lst):\n",
    "    # 计算列表中每个元素出现的频率\n",
    "\n",
    "    # list to str\n",
    "    # print(''.join(lst))\n",
    "\n",
    "    counter = Counter(lst)\n",
    "    probs = [count / len(lst) for count in counter.values()]\n",
    "\n",
    "    # 计算信息熵\n",
    "    entropy = -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "    return entropy\n",
    "def select_log_template_pairs_based_on_entropy(pairs, num_examples):\n",
    "    # 计算每个对的信息熵\n",
    "    entropies = [(pair, calculate_entropy(list(pair[0]) + list(pair[1])))  # list(pair[0]) + list(pair[1]) / extract_variables(pair[0], pair[1])\n",
    "                 for pair in pairs]\n",
    "\n",
    "    # 根据信息熵对对进行排序\n",
    "    sorted_pairs = sorted(entropies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 选择信息熵最高的对\n",
    "    selected_pairs = sorted_pairs[:num_examples]\n",
    "\n",
    "    return [pair for pair, entropy in selected_pairs]\n",
    "\n",
    "# discard the target dataset\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "            'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# datasets.remove('BGL')\n",
    "demonstration_templates = []\n",
    "demonstration_logs = []\n",
    "pairs = []\n",
    "for d in datasets:\n",
    "    df = pd.read_csv(f'dataset\\{d}\\{d}_2k.log_structured_corrected.csv')\n",
    "    list1 = df['Content'].tolist()\n",
    "    list2 = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(list1, list2):\n",
    "        if template not in demonstration_templates:\n",
    "            pairs.append((log, template))\n",
    "            demonstration_templates.append(template)\n",
    "            demonstration_logs.append(log)\n",
    "\n",
    "list =  select_log_template_pairs_based_on_entropy(pairs, 10)\n",
    "for log, template in list:\n",
    "    print(log)\n",
    "    print(template)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similarity in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import tokenize\n",
    "from utils.sample_byword import extract_variables\n",
    "\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC',\n",
    "    'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "count_logs = []\n",
    "count_templates = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing {dataset} ----------------\")\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(logs, templates):\n",
    "        if template not in count_templates:\n",
    "            count_templates.append(template)\n",
    "            if any(char.isdigit() for char in template):\n",
    "                print(f\"{template}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
