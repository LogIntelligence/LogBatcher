Output,EventTemplate
Created MRAppMaster for application appattempt_1445144423722_0020_000001,Created MRAppMaster for application <*>
Executing with tokens:,Executing with tokens:
"Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: 20 cluster_timestamp: 1445144423722 } attemptId: 1 } keyId: -127633188)","Kind: YARN_AM_RM_TOKEN, Service: , Ident: (appAttemptId { application_id { id: <*> cluster_timestamp: <*> } attemptId: <*> } keyId: <*>)"
Using mapred newApiCommitter.,Using mapred newApiCommitter.
OutputCommitter set in config null,OutputCommitter set in config <*>
OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter,OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
Emitting job history data to the timeline server is not enabled,Emitting job history data to the timeline server is not enabled
loaded properties from hadoop-metrics2.properties,loaded properties from hadoop-metrics2.properties
Scheduled snapshot period at 10 second(s).,Scheduled snapshot period at <*> second(s).
MRAppMaster metrics system started,MRAppMaster metrics system started
Adding job token for job_1445144423722_0020 to jobTokenSecretManager,Adding job token for <*> to jobTokenSecretManager
Not uberizing job_1445144423722_0020 because: not enabled; too many maps; too much input;,Not uberizing <*> because: not enabled; too many maps; too much input;
Input size for job job_1445144423722_0020 = 1256521728. Number of splits = 10,Input size for job <*> = <*>. Number of splits = <*>
Number of reduces for job job_1445144423722_0020 = 1,Number of reduces for job <*> = <*>
job_1445144423722_0020Job Transitioned from NEW to INITED,<*> Transitioned from NEW to INITED
"MRAppMaster launching normal, non-uberized, multi-container job job_1445144423722_0020.","MRAppMaster launching normal, non-uberized, multi-container job <*>."
Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server,Adding protocol org.apache.hadoop.mapreduce.v2.api.MRClientProtocolPB to the server
Instantiated MRClientService at MININT-FNANLI5.fareast.corp.microsoft.com/10.86.169.121:62260,Instantiated MRClientService at MININT-<*>
Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog,Logging to <*>(org.mortbay.log) via <*>
Http request log for http.requests.mapreduce is not defined,Http request log for http.requests.mapreduce is not defined
Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter),Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce,Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context mapreduce
Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static,Added filter AM_PROXY_FILTER (class=org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter) to context static
Jetty bound to port 62267,Jetty bound to port <*>
jetty-6.1.26,jetty-<*>
Extract jar:file:/D:/hadoop-2.6.0-localbox/share/hadoop/yarn/hadoop-yarn-common-2.6.0-SNAPSHOT.jar!/webapps/mapreduce to C:\Users\msrabi\AppData\Local\Temp\Jetty_0_0_0_0_62267_mapreduce____.8n7xum\webapp,Extract jar:file:<*> to <*>
Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:62267,Started HttpServer2$SelectChannelConnectorWithSafeStartup@<*>
Web app /mapreduce started at 62267,Web app <*> started at <*>
Registered webapp guice modules,Registered webapp guice modules
JOB_CREATE job_1445144423722_0020,JOB_CREATE <*>
nodeBlacklistingEnabled:true,nodeBlacklistingEnabled:<*>
maxTaskFailuresPerNode is 3,maxTaskFailuresPerNode is <*>
blacklistDisablePercent is 33,blacklistDisablePercent is <*>
Connecting to ResourceManager at msra-sa-41/10.190.173.170:8030,Connecting to ResourceManager at <*>
"maxContainerCapability: <memory:8192, vCores:32>","maxContainerCapability: <memory:<*>, vCores:<*>>"
queue: default,queue: default
Upper limit on the thread pool size is 500,Upper limit on the thread pool size is <*>
yarn.client.max-cached-nodemanagers-proxies : 0,yarn.client.max-cached-nodemanagers-proxies : <*>
job_1445144423722_0020Job Transitioned from INITED to SETUP,<*> Transitioned from INITED to SETUP
Processing the event EventType: JOB_SETUP,Processing the event EventType: JOB_SETUP
job_1445144423722_0020Job Transitioned from SETUP to RUNNING,<*> Transitioned from SETUP to RUNNING
"mapResourceRequest:<memory:1024, vCores:1>","mapResourceRequest:<memory:<*>, vCores:<*>"
"Event Writer setup for JobId: job_1445144423722_0020, File: hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job_1445144423722_0020_1.jhist","Event Writer setup for JobId: <*>, File: <*>"
"reduceResourceRequest:<memory:1024, vCores:1>","reduceResourceRequest:<memory:<*>, vCores:<*>"
The job-jar file on the remote FS is hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.jar,The job-jar file on the remote FS is <*>
The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.xml,The job-conf file on the remote FS is <*>
Adding #0 tokens and #1 secret keys for NM use for launching container,Adding #<*> tokens and #<*> secret keys for NM use for launching container
Size of containertokens_dob is 1,Size of containertokens_dob is <*>
Putting shuffle token in serviceData,Putting shuffle token in serviceData
"Cannot assign container Container: [ContainerId: container_1445144423722_0020_01_000012, NodeId: MSRA-SA-39.fareast.corp.microsoft.com:28345, NodeHttpAddress: MSRA-SA-39.fareast.corp.microsoft.com:8042, Resource: <memory:1024, vCores:1>, Priority: 20, Token: Token { kind: ContainerToken, service: 172.22.149.145:28345 }, ] for a map as either  container memory less than required <memory:1024, vCores:1> or no pending map tasks - maps.isEmpty=true","Cannot assign container Container: [ContainerId: <*>, NodeId: <*>, NodeHttpAddress: <*>, Resource: <memory:<*>, vCores:<*>, Priority: <*>, Token: Token { kind: ContainerToken, service: <*> }, ] for a map as either container memory less than required <memory:<*>, vCores:<*> or no pending map tasks - maps.isEmpty=<*>"
Container complete event for unknown container id container_1445144423722_0020_01_000012,Container complete event for unknown container id <*>
Done acknowledgement from attempt_1445144423722_0020_m_000003_0,Done acknowledgement from <*>
attempt_1445144423722_0020_m_000003_0 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP,<*> TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP
attempt_1445144423722_0020_m_000003_0 TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED,<*> TaskAttempt Transitioned from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED
Task succeeded with attempt attempt_1445144423722_0020_m_000003_0,Task succeeded with attempt <*>
task_1445144423722_0020_m_000003 Task Transitioned from RUNNING to SUCCEEDED,<*> Task Transitioned from RUNNING to SUCCEEDED
Num completed Tasks: 1,Num completed Tasks: <*>
Reduce slow start threshold reached. Scheduling reduces.,Reduce slow start threshold reached. Scheduling reduces.
All maps assigned. Ramping up all remaining reduces:1,All maps assigned. Ramping up all remaining reduces:<*>
DefaultSpeculator.addSpeculativeAttempt -- we are speculating task_1445144423722_0020_m_000000,DefaultSpeculator.addSpeculativeAttempt -- we are speculating <*>
We launched 1 speculations.  Sleeping 15000 milliseconds.,We launched <*> speculations. Sleeping <*> milliseconds.
Scheduling a redundant attempt for task task_1445144423722_0020_m_000000,Scheduling a redundant attempt for task <*>
Diagnostics report from attempt_1445144423722_0020_m_000003_0: Container killed by the ApplicationMaster.,Diagnostics report from <*>: Container killed by the ApplicationMaster.
"Slow ReadProcessor read fields took 65020ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.86.169.121:50010, 10.190.173.170:50010]","Slow ReadProcessor read fields took <*> (threshold=<*>); ack: seqno: <*> status: SUCCESS status: ERROR downstreamAckTimeNanos: <*>, targets: [<*>, <*>]"
DFSOutputStream ResponseProcessor exception  for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731,DFSOutputStream ResponseProcessor exception for block BP-<*>
"Error Recovery for block BP-1347369012-10.190.173.170-1444972147527:blk_1073743512_2731 in pipeline 10.86.169.121:50010, 10.190.173.170:50010: bad datanode 10.190.173.170:50010","Error Recovery for block BP-<*> in pipeline <*>, <*>: bad datanode <*>"
DataStreamer Exception,DataStreamer Exception
Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@7317849d,Error writing History Event: org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent@<*>
"Thread Thread[eventHandlingThread,5,main] threw an Exception.","Thread Thread[eventHandlingThread,<*>,main] threw an Exception."
