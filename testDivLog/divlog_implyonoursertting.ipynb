{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling from other 15 datasets (DPP + KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from openai.embeddings_utils import cosine_similarity\n",
    "import tqdm\n",
    "\n",
    "def dpp(kernel_matrix, max_length, epsilon=1E-10):\n",
    "    item_size = kernel_matrix.shape[0]\n",
    "    cis = np.zeros((max_length, item_size))\n",
    "    di2s = np.copy(np.diag(kernel_matrix))\n",
    "    selected_items = list()\n",
    "    selected_item = np.argmax(di2s)\n",
    "    selected_items.append(selected_item)\n",
    "    while len(selected_items) < max_length:\n",
    "        k = len(selected_items) - 1\n",
    "        ci_optimal = cis[:k, selected_item]\n",
    "        di_optimal = math.sqrt(di2s[selected_item])\n",
    "        elements = kernel_matrix[selected_item, :]\n",
    "        eis = (elements - np.dot(ci_optimal, cis[:k, :])) / di_optimal\n",
    "        cis[k, :] = eis\n",
    "        di2s -= np.square(eis)\n",
    "        selected_item = np.argmax(di2s)\n",
    "        selected_items.append(selected_item)\n",
    "    return selected_items\n",
    "\n",
    "def getDppIndex(log_emb_list, \n",
    "                item_size,    # log dataset size\n",
    "                split_ratio):\n",
    "\n",
    "    max_length = int(item_size * split_ratio)\n",
    "    feature_vectors = np.array(log_emb_list) \n",
    "\n",
    "    # standarization no need for log embeddings\n",
    "    feature_vectors /= np.linalg.norm(feature_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    # calculate similarity matrix of log embeddings\n",
    "    similarities = np.dot(feature_vectors, feature_vectors.T) \n",
    "\n",
    "    t = time.time()\n",
    "    result = dpp(similarities, max_length)\n",
    "    result.sort()\n",
    "    print('DPP algorithm running time: ' + '\\t' + \"{0:.4e}\".format(time.time() - t))\n",
    "    return result\n",
    "\n",
    "def generateLuMap(test_embeddings, candidate_embeddings, logs, look_up_map_path):\n",
    "    lookUpMap = {}\n",
    "    for test_idx in range(2000):\n",
    "        dis_dict = {}\n",
    "        for cand_idx in range(300):\n",
    "            dis_dict[cosine_similarity(test_embeddings[test_idx], candidate_embeddings[cand_idx])] = cand_idx\n",
    "        # get a list in sorted key (descending order), key = cosine similarity\n",
    "        sorted_list = []\n",
    "        for key in sorted(dis_dict, reverse=True): \n",
    "            sorted_list.append(dis_dict[key])\n",
    "        # dict: {log_message : list of similar candidate indexes in order}\n",
    "        lookUpMap[logs[test_idx]] = sorted_list\n",
    "\n",
    "    # write the map into a json file\n",
    "    with open(look_up_map_path, 'w') as file:\n",
    "        file.write(json.dumps(lookUpMap))\n",
    "    return lookUpMap\n",
    "\n",
    "\n",
    "def getNearest(log,lookUpMap, N=5):\n",
    "    cand_list = lookUpMap[log]\n",
    "    result = cand_list[0:N]\n",
    "    return result\n",
    "\n",
    "# generate a prompt in str for a specific log message\n",
    "def generateDemonstrations(log, lookUpMap, candidate_logs, candidate_templates, nearest_num=5,):\n",
    "    idxes = getNearest(log, lookUpMap, nearest_num)\n",
    "    prompt = \"\"\n",
    "    result = []\n",
    "    for index in idxes:\n",
    "        result.append({\"role\": \"user\", \"content\": candidate_logs[index]})\n",
    "        result.append({\"role\": \"assistant\", \"content\": '`'+candidate_templates[index] + '`'})\n",
    "    return result\n",
    "\n",
    "datasets = [\"HDFS\", \"Spark\", \"BGL\", \"Windows\", \"Linux\", \"Android\", \"Mac\", \"Hadoop\", \"HealthApp\", \"OpenSSH\", \"Thunderbird\", \"Proxifier\", \"Apache\", \"HPC\", \"Zookeeper\", \"OpenStack\"]\n",
    "\n",
    "for test_dataset in datasets:\n",
    "    print(f\"processing {test_dataset}...\")\n",
    "\n",
    "    # discard the target dataset\n",
    "    train_datasets = [dataset for dataset in datasets if dataset != test_dataset]\n",
    "\n",
    "    # load test logs, templates and embeddings\n",
    "    file = open(f\"DivLog\\embeddings\\{test_dataset}.json\")\n",
    "    emb_map = json.load(file)\n",
    "    file.close()\n",
    "    df = pd.read_csv(f\"Divlog\\loghub_2k\\{test_dataset}\\{test_dataset}_2k.log_structured.csv\")\n",
    "    test_logs = df[\"Content\"].values.tolist()\n",
    "    test_templates = df[\"EventTemplate\"].values.tolist()\n",
    "    test_embeddings = [emb_map[log] for log in test_logs]\n",
    "\n",
    "\n",
    "    # load train logs, templates and embeddings\n",
    "    logs = []\n",
    "    templates = []\n",
    "    embeddings = []\n",
    "    for dataset in train_datasets:\n",
    "        file = open(f\"DivLog\\embeddings\\{dataset}.json\")\n",
    "        emb_map = json.load(file)\n",
    "        file.close()\n",
    "        df = pd.read_csv(f\"Divlog\\loghub_2k\\{dataset}\\{dataset}_2k.log_structured.csv\")\n",
    "        log_list = df[\"Content\"].values.tolist()\n",
    "        template_list = df[\"EventTemplate\"].values.tolist()\n",
    "        for log,template in zip(log_list, template_list):\n",
    "            logs.append(log)\n",
    "            templates.append(template)\n",
    "            embeddings.append(emb_map[log])\n",
    "\n",
    "    # get candidate set using dpp\n",
    "    candidate_index = getDppIndex(embeddings, len(logs), 0.01)\n",
    "    candidate_logs = [logs[i] for i in candidate_index]\n",
    "    candidate_templates = [templates[i] for i in candidate_index]\n",
    "    candidate_embeddings = [embeddings[i] for i in candidate_index]\n",
    "\n",
    "    # generate lookup map\n",
    "    look_up_map_path = f\"DivLog\\lookup_map\\{test_dataset}_lookup_map.json\"\n",
    "    lookUpMap = generateLuMap(test_embeddings, candidate_embeddings, test_logs, look_up_map_path)\n",
    "\n",
    "    json_data = []\n",
    "    for log in test_logs:\n",
    "        # get a prompt with five examples for each log message\n",
    "        result = generateDemonstrations(log, lookUpMap, candidate_logs, candidate_templates)\n",
    "        json_data.append(result)\n",
    "    with open(f\"DivLog\\jsondata\\{test_dataset}.json\", 'w') as f:\n",
    "        json.dump(json_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "\n",
    "def post_process(response):\n",
    "    response = response.strip().strip('\\n')\n",
    "    if \"\\n\\n\" in response:\n",
    "        response = response.split(\"\\n\\n\")[0]\n",
    "    reg = re.compile(\"`([^`]+)`\")\n",
    "    tmps = reg.findall(response)\n",
    "    tmps = [x.strip('\\n').strip() for x in tmps]\n",
    "    tmp = ''\n",
    "    if len(tmps) == 1:\n",
    "        tmp = tmps[0]\n",
    "    if len(tmps) > 1:\n",
    "        tmp = max(tmps, key=len)\n",
    "    \n",
    "    tmp = tmp.strip('\\n').strip()\n",
    "    tmp = re.sub(r'\\{\\{.*?\\}\\}', '<*>', tmp)\n",
    "    template = tmp\n",
    "    return template\n",
    "class Parser:\n",
    "    def __init__(self, api_key, model='gpt-3.5-turbo-0125', using_proxy=True, N=5):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "        self.N = N\n",
    "        self.instruction = '''You will be provided with a log message delimited by backticks. You must abstract variables with `<*>` to extract the corresponding template.\\nPrint the input log's template delimited by backticks.'''\n",
    "        self.client = OpenAI(\n",
    "                    # 3.5 https://4.0.996444.icu/v1\n",
    "                    base_url=\"https://oneapi.xty.app/v1\",  # 中转url\n",
    "                    api_key=api_key,                      # api_key\n",
    "                    http_client=httpx.Client(\n",
    "                        proxies=\"http://127.0.0.1:7890\"  # 代理地址\n",
    "                    ),\n",
    "                )\n",
    "    @retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(5))\n",
    "    def chat(self, messages):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip('\\n')\n",
    "\n",
    "    def get_responce(self, input):\n",
    "        messages = [{\"role\": \"system\", \"content\": self.instruction}]\n",
    "        for message in input[\"demonstrations\"][:2*self.N]:\n",
    "            messages.append(message)\n",
    "        messages.append({\"role\": \"user\", \"content\": input[\"log\"]})\n",
    "        output = self.chat(messages)\n",
    "        template = post_process(output)\n",
    "        print(template)\n",
    "        return template\n",
    "\n",
    "\n",
    "# main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = Parser(api_key=\"sk-zY5LaAEd3EUdBVmKA75aDe77C9684c209b128b981826C043\", N=2)\n",
    "\n",
    "    datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "    datasets = ['BGL','HDFS']\n",
    "    output_dir = 'outputs/divlog/Test/'\n",
    "    for dataset in datasets:\n",
    "        print(f\"parsing {dataset}...\")\n",
    "        with open(f\"DivLog\\jsondata\\{dataset}.json\", 'r') as f:\n",
    "            loaded_data = json.load(f)\n",
    "        df = pd.read_csv(f\"Divlog\\loghub_2k\\{dataset}\\{dataset}_2k.log_structured.csv\")\n",
    "        logs = df[\"Content\"].values.tolist()\n",
    "        inputs = []\n",
    "        for log, demontrations in zip(logs, loaded_data):\n",
    "            inputs.append({\"log\": log, \"demonstrations\": demontrations})\n",
    "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "            templates = list(\n",
    "                tqdm(executor.map(parser.get_responce, inputs), total=len(inputs)))\n",
    "        # write to file\n",
    "        df['Output'] = templates\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df[['Content', 'EventTemplate', 'Output']].to_csv(output_dir+ f'{dataset}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
