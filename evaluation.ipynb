{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import reassign_clusters, cluster, vectorize, tokenize,Cluster\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "dataset = ''\n",
    "print('-' * 50)\n",
    "print(f'Clustering {dataset} dataset...')\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "logs = df['Content'].tolist()\n",
    "templates = df['EventTemplate'].tolist()\n",
    "\n",
    "# tokenize -> vectorize -> cluster -> reassign_clusters\n",
    "tokenized_logs = [tokenize(log) for log in logs]\n",
    "labels, cluster_nums = cluster(vectorize(tokenized_logs))\n",
    "labels, cluster_nums = reassign_clusters(labels, cluster_nums, tokenized_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num of clusters:', cluster_nums)\n",
    "print('len of templates:', len(set(templates)))\n",
    "\n",
    "# store the logs in the cluster\n",
    "inputs = []\n",
    "for i in range(cluster_nums):\n",
    "    inputs.append([-1, [], [], '']) # label, logs, indexs, ground_truth\n",
    "for i, label in enumerate(labels):\n",
    "    inputs[label][0] = label\n",
    "    inputs[label][1].append(logs[i])\n",
    "    inputs[label][2].append(i)\n",
    "    if inputs[label][3] == '':\n",
    "        inputs[label][3] = df['EventTemplate'][i]\n",
    "\n",
    "clusters = []\n",
    "for input in inputs:\n",
    "    c = Cluster(*input, remove_duplicate= True)\n",
    "    clusters.append(c)\n",
    "\n",
    "num = 1\n",
    "print('cluster:', num)\n",
    "print('length:', len(clusters[num].indexs))\n",
    "print('template:', clusters[num].oracle_template)\n",
    "set_logs = set(clusters[num].static_logs)\n",
    "print('len of set:', len(set_logs))\n",
    "print('-'*20)\n",
    "for log in set_logs:\n",
    "    print(log)\n",
    "print('='*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_all_datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "theme = 'LogBatcher_0shot_32candidate_10batchsize'\n",
    "table = evaluate_all_datasets(\n",
    "    'LogBatcher_0shot_32candidate_10batchsize')\n",
    "HTML(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invocation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from together import Together\n",
    "import httpx\n",
    "import openai\n",
    "import backoff\n",
    "from utils.postprocess import post_process\n",
    "\n",
    "api_key = \"sk-proj-5EkdZfTfjJ1GJim17pgQT3BlbkFJHCMqWAOX7dTSGOcFOjrn\"\n",
    "client = OpenAI(\n",
    "    api_key=api_key,                      # api_key\n",
    "    http_client=httpx.Client(\n",
    "        proxies=\"http://127.0.0.1:7890\"  # proxies\n",
    "    ),\n",
    ")\n",
    "\n",
    "# api_key = \"3e0eca67e7a8720908ba848545b9175f556a6ab6dfc0b63ee3639f9d71e1bbeb\"\n",
    "# client = Together(\n",
    "#     api_key=api_key,                      # api_key\n",
    "# )\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, (openai.APIStatusError, openai.InternalServerError), max_tries=20)\n",
    "def get_responce(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    print(f\"model: {response.model}\")\n",
    "    print(f\"usage: {response.usage}\")\n",
    "    return response.choices[0].message.content.strip('\\n')\n",
    "\n",
    "\n",
    "output = get_responce([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out some strings' freq in the whole logs or templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.cluster import tokenize\n",
    "from utils.sample_byword import extract_variables\n",
    "from utils.postprocess import correct_single_template\n",
    "from utils.postprocess import extract_variables\n",
    "\n",
    "datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'HPC', 'Zookeeper', 'Mac', 'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "\n",
    "count = 0\n",
    "count_templates = []\n",
    "count_varaibles = {}\n",
    "for dataset in datasets:\n",
    "    # print(f\"Processing {dataset} ----------------\")\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs = df['Content'].tolist()\n",
    "    templates = df['EventTemplate'].tolist()\n",
    "    for log, template in zip(logs, templates):\n",
    "        num = template.count('<*>')\n",
    "        if count_varaibles.get(num):\n",
    "            count_varaibles[num] += 1\n",
    "        else:\n",
    "            count_varaibles[num] = 1\n",
    "print(count_varaibles)\n",
    "for key in count_varaibles:\n",
    "    count += count_varaibles[key]\n",
    "for i in range(100):\n",
    "    if count_varaibles.get(i):\n",
    "        print(f\"Number of templates with {i} variables: {count_varaibles[i]}\")\n",
    "print((4131+9825+7200+3700+2304+591) / 30000)\n",
    "print((4131+9825+7200+3700+2304+591 + 563 + 1133 + 52 + 16 + 35) / 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample based on entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sample import sample_based_on_entropy\n",
    "\n",
    "# datasets = ['BGL', 'HDFS', 'Linux', 'HealthApp', 'OpenStack', 'OpenSSH', 'Proxifier', 'HPC', 'Zookeeper', 'Mac',\n",
    "#         'Hadoop', 'Android', 'Windows', 'Apache', 'Thunderbird', 'Spark']\n",
    "# for dataset in datasets:\n",
    "#     pair = sample_based_on_entropy(dataset, 1)\n",
    "#     print(pair[0][0])\n",
    "dataset = 'HDFS'\n",
    "pairs = sample_based_on_entropy(dataset, 1)\n",
    "for pair in pairs:\n",
    "    print(f\"{pair[0][0]}\\n{pair[0][1]}\\n{'-'*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.util import count_message_tokens, count_message_variables\n",
    "\n",
    "\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "logs = {}\n",
    "counts_token = {}\n",
    "counts_message = {}\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv')\n",
    "    logs[dataset] = df['Content'].tolist()\n",
    "    counts_token[dataset] = 0\n",
    "    counts_message[dataset] = 0\n",
    "for log in logs['HealthApp']:\n",
    "    log = log.strip()\n",
    "\n",
    "# 存储解析后的日志列表\n",
    "message_list = []\n",
    "# load every message\n",
    "file = 'cost_lilac_32_3.json'\n",
    "with open('outputs/cost/LogBatcher_3shot_32candidate_10batchsize.json', 'r') as file:\n",
    "    for line in file:\n",
    "        if line.strip() == '[':\n",
    "            list_str = ''\n",
    "            start_load = True\n",
    "        if line.strip() == ']':\n",
    "            list_str += line\n",
    "            message = json.loads(list_str)\n",
    "            message_list.append(message)\n",
    "            start_load = False\n",
    "        if start_load:\n",
    "            list_str += line\n",
    "# print(len(message_list))\n",
    "for message in message_list:\n",
    "    # for LILAC\n",
    "    log = message[-1]['content'].split('\\n')[0].replace('Log message: `', '').replace('`', '')\n",
    "    # for LogBatcher\n",
    "    # log = message[-1]['content'].split('\\n')[0] \n",
    "    # print(log)\n",
    "    for dataset in datasets:\n",
    "        if log in logs[dataset]:\n",
    "            counts_token[dataset] += count_message_tokens(message, 'gpt-3.5-turbo')\n",
    "            counts_message[dataset] += 1\n",
    "            break\n",
    "        if dataset == 'Mac':\n",
    "            print(log)\n",
    "for dataset in datasets:\n",
    "    print(counts_token[dataset], counts_token[dataset] / counts_message[dataset]  )\n",
    "\n",
    "print(sum(counts_token.values()), sum(counts_token.values()) / sum(counts_message.values()))\n",
    "\n",
    "# remove the same log messages\n",
    "\n",
    "# def make_hashable(log_list):\n",
    "\n",
    "#     return tuple(tuple(sorted(d.items())) for d in log_list)\n",
    "# unique_lists = list(set(make_hashable(log_list) for log_list in message_list))\n",
    "\n",
    "# unique_big_list = [list(map(dict, log_list)) for log_list in unique_lists]\n",
    "# print(len(unique_big_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unseen logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "table_order = 'HDFS Hadoop Spark Zookeeper BGL HPC Thunderbird Windows Linux Android HealthApp Apache OpenSSH OpenStack Mac'\n",
    "datasets = table_order.split(' ')\n",
    "a = 0\n",
    "b = 0\n",
    "for dataset in datasets:\n",
    "\n",
    "    file = 'Test_10shot'\n",
    "    groundtruth_file = f'dataset/{dataset}/{dataset}_2k.log_structured_corrected.csv'\n",
    "    output_file = f'outputs/parser/{file}/{dataset}_2k.log_structured.csv'\n",
    "\n",
    "    templates_groundtruth = pd.read_csv(groundtruth_file)[\n",
    "        'EventTemplate'].tolist()\n",
    "    templates_output = pd.read_csv(output_file)['EventTemplate'].tolist()\n",
    "\n",
    "    freq = Counter(templates_groundtruth)\n",
    "    unseen_templates = [item for item, count in freq.items() if count == 1]\n",
    "\n",
    "    indexes = []\n",
    "    for index, template in enumerate(templates_groundtruth):\n",
    "        if template in unseen_templates:\n",
    "            indexes.append(index)\n",
    "\n",
    "    accuracy_exact_string_matching = accuracy_score([templates_groundtruth[index] for index in indexes], [\n",
    "                                                    templates_output[index] for index in indexes], normalize=False)\n",
    "    length = len(indexes)\n",
    "    a += length\n",
    "    b += accuracy_exact_string_matching\n",
    "    dataset = ' ' * (12 - len(dataset)) + dataset\n",
    "    print('%s: len of unseen log: %.4d, Message-Level Accuracy: %.4f' %\n",
    "          (dataset, length, accuracy_exact_string_matching/length))\n",
    "\n",
    "\n",
    "print(b/a)\n",
    "print(a/len(datasets))\n",
    "# 81.0 71.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load\n",
    "main result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '''0.998 \t0.999 \t0.999 \t0.998 \t0.999 \t0.999 \t0.998 \t0.959 \t0.997 \t0.930 \t0.961 \t0.993 \t0.930 \t0.996 \t0.999 \t1.000 \t1.000 \t1.000 \t1.000 \t1.000 \t1.000 \n",
    "0.948 \t0.613 \t0.882 \t0.869 \t0.606 \t0.901 \t0.950 \t0.158 \t0.751 \t0.694 \t0.195 \t0.708 \t0.683 \t0.744 \t0.915 \t0.991 \t0.958 \t0.986 \t0.990 \t0.886 \t0.952 \n",
    "0.920 \t0.398 \t0.963 \t0.905 \t0.398 \t0.952 \t0.998 \t0.376 \t0.950 \t0.470 \t0.296 \t0.915 \t0.738 \t0.960 \t0.983 \t0.999 \t0.983 \t0.998 \t0.998 \t0.972 \t0.989 \n",
    "0.967 \t0.799 \t0.981 \t0.965 \t0.800 \t0.981 \t0.989 \t0.779 \t0.987 \t0.956 \t0.805 \t0.970 \t0.955 \t0.979 \t0.998 \t1.000 \t0.987 \t0.999 \t0.995 \t0.988 \t0.995 \n",
    "0.963 \t0.479 \t0.885 \t0.957 \t0.474 \t0.883 \t0.996 \t0.426 \t0.891 \t0.702 \t0.282 \t0.785 \t0.736 \t0.950 \t0.990 \t0.983 \t0.972 \t0.989 \t0.987 \t0.941 \t0.989 \n",
    "0.887 \t0.662 \t0.872 \t0.904 \t0.680 \t0.880 \t0.945 \t0.660 \t0.973 \t0.978 \t0.751 \t0.870 \t0.935 \t0.980 \t0.997 \t0.970 \t0.994 \t0.999 \t0.953 \t0.943 \t0.995 \n",
    "0.957 \t0.180 \t0.941 \t0.945 \t0.180 \t0.943 \t0.971 \t0.060 \t0.932 \t0.554 \t0.097 \t0.826 \t0.234 \t0.879 \t0.978 \t0.984 \t0.913 \t0.983 \t0.914 \t0.854 \t0.953 \n",
    "0.997 \t0.466 \t0.948 \t0.691 \t0.158 \t0.840 \t0.997 \t0.463 \t0.976 \t0.694 \t0.141 \t0.915 \t0.710 \t0.715 \t0.903 \t0.696 \t0.685 \t0.897 \t1.000 \t0.609 \t0.862 \n",
    "0.422 \t0.217 \t0.750 \t0.405 \t0.205 \t0.745 \t0.358 \t0.176 \t0.770 \t0.186 \t0.125 \t0.684 \t0.484 \t0.620 \t0.935 \t0.298 \t0.422 \t0.926 \t0.998 \t0.731 \t0.969 \n",
    "0.885 \t0.750 \t0.972 \t0.773 \t0.540 \t0.876 \t0.960 \t0.253 \t0.924 \t0.795 \t0.436 \t0.822 \t0.737 \t0.677 \t0.952 \t0.953 \t0.627 \t0.923 \t0.971 \t0.787 \t0.953 \n",
    "0.901 \t0.375 \t0.749 \t0.893 \t0.368 \t0.744 \t1.000 \t0.261 \t0.871 \t0.833 \t0.677 \t0.850 \t0.876 \t0.984 \t0.997 \t0.998 \t0.988 \t0.998 \t0.920 \t0.914 \t0.961 \n",
    "1.000 \t0.978 \t0.996 \t1.000 \t0.978 \t0.996 \t1.000 \t0.984 \t0.996 \t1.000 \t0.972 \t0.995 \t0.984 \t0.985 \t0.997 \t1.000 \t1.000 \t1.000 \t1.000 \t0.978 \t0.996 \n",
    "0.789 \t0.594 \t0.919 \t0.547 \t0.729 \t0.965 \t1.000 \t0.287 \t0.948 \t0.802 \t0.928 \t0.960 \t0.749 \t0.987 \t0.999 \t0.753 \t0.805 \t0.983 \t1.000 \t0.976 \t0.989 \n",
    "0.224 \t0.105 \t0.693 \t0.249 \t0.034 \t0.718 \t0.492 \t0.112 \t0.937 \t0.315 \t0.071 \t0.724 \t0.220 \t0.437 \t0.873 \t1.000 \t0.977 \t0.991 \t1.000 \t0.982 \t0.995 \n",
    "0.814 \t0.392 \t0.896 \t0.765 \t0.284 \t0.835 \t0.949 \t0.383 \t0.902 \t0.759 \t0.359 \t0.843 \t0.712 \t0.549 \t0.898 \t0.805 \t0.562 \t0.892 \t0.831 \t0.528 \t0.879 \n",
    "0.845 \t0.534 \t0.896 \t0.791 \t0.495 \t0.884 \t0.907 \t0.422 \t0.920 \t0.711 \t0.473 \t0.857 \t0.712 \t0.829 \t0.961 \t0.895 \t0.858 \t0.971 \t0.970 \t0.873 \t0.965'''\n",
    "\n",
    "# count baseline num\n",
    "num = int(len(c.split('\\n')[0].split('\\t')) / 3)\n",
    "GA, MLA, ED = [], [], []\n",
    "for i in range(num):\n",
    "    GA.append([])\n",
    "    MLA.append([])\n",
    "    ED.append([])\n",
    "\n",
    "lines = c.split('\\n')\n",
    "for line in lines:\n",
    "    num_list = line.split('\\t')\n",
    "    for i in range(num):\n",
    "        GA[i].append(float(num_list[i*3]))\n",
    "        MLA[i].append(float(num_list[i*3 + 1]))\n",
    "        ED[i].append(float(num_list[i*3 + 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Log Parser': ['Drain', 'AEL', 'Brain', 'Logram', 'DivLog', 'LILAC', 'LogBatcher'],\n",
    "    'Group Accuracy': GA,\n",
    "    'Parsing Accuracy': MLA,\n",
    "    'Normalized Edit Distance': ED\n",
    "}\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "flierprops = dict(marker='o', color='black', markersize=3,\n",
    "                  markerfacecolor='black', markeredgecolor='black')\n",
    "# 绘制箱线图\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "label_fontsize = 18\n",
    "x_fontsize = 18\n",
    "y_fontsize = 18\n",
    "special_tick_fontsize = 16  # 特定指标的字体大小\n",
    "space_size = 0.05\n",
    "line_size = dict(linewidth=1.5)\n",
    "\n",
    "\n",
    "for ax in axs[:2]:\n",
    "    for i in [0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "        ax.axhline(y=i, color='gray', linestyle='--',\n",
    "                   linewidth=1, dashes=(5, 5))\n",
    "    ax.tick_params(axis='y', labelsize=y_fontsize)\n",
    "\n",
    "for i in [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    axs[2].axhline(y=i, color='gray', linestyle='--',\n",
    "                   linewidth=1, dashes=(5, 5))\n",
    "# Adjust y-axis label font siz\n",
    "axs[2].tick_params(axis='y', labelsize=y_fontsize)\n",
    "# Group Accuracy\n",
    "axs[0].boxplot(df['Group Accuracy'], labels=df['Log Parser'], boxprops=line_size, capprops=line_size,\n",
    "               whiskerprops=line_size, medianprops=line_size,\n",
    "               flierprops=flierprops)\n",
    "axs[0].set_ylabel('Group Accuracy', rotation=90, fontsize=label_fontsize)\n",
    "axs[0].set_ylim(0-space_size, 1+space_size)\n",
    "axs[0].set_xticklabels(df['Log Parser'], rotation=45,\n",
    "                       ha='right', fontsize=x_fontsize)\n",
    "\n",
    "\n",
    "# Parsing Accuracy\n",
    "axs[1].boxplot(df['Parsing Accuracy'], boxprops=line_size, capprops=line_size,\n",
    "               whiskerprops=line_size, medianprops=line_size,\n",
    "               labels=df['Log Parser'], flierprops=flierprops)\n",
    "axs[1].set_ylabel('Parsing Accuracy', rotation=90, fontsize=label_fontsize)\n",
    "axs[1].set_ylim(0-space_size, 1+space_size)\n",
    "axs[1].set_xticklabels(df['Log Parser'], rotation=45,\n",
    "                       ha='right', fontsize=x_fontsize)\n",
    "\n",
    "# Edit Distance\n",
    "axs[2].boxplot(df['Normalized Edit Distance'], boxprops=line_size, capprops=line_size,\n",
    "               whiskerprops=line_size, medianprops=line_size,\n",
    "               labels=df['Log Parser'], flierprops=flierprops)\n",
    "axs[2].set_ylabel('Edit Distance', rotation=90, fontsize=label_fontsize)\n",
    "axs[2].set_ylim(0.5 - space_size/2, 1 + space_size/2)\n",
    "axs[2].set_xticklabels(df['Log Parser'], rotation=45,\n",
    "                       ha='right', fontsize=x_fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存为 PDF 文件\n",
    "plt.savefig('outputs/figures/RQ1_Robust.pdf', format='pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "\n",
    "# LILAC\n",
    "data1 = '''0.8298 0.6569 0.91518\n",
    "0.8852 0.8023 0.95239\n",
    "0.8894 0.8418 0.9652\n",
    "0.8952 0.858 0.97086\n",
    "0.8947 0.8313 0.96806\n",
    "0.8953 0.8624 0.97133\n",
    "0.8965 0.8637 0.96976'''\n",
    "\n",
    "# LogBatcher\n",
    "data2 = '''0.970 \t0.873 \t0.965 \n",
    "0.973 \t0.942 \t0.985 \n",
    "0.979 \t0.967 \t0.992 \n",
    "0.975 \t0.956 \t0.989 \n",
    "0.979 \t0.939 \t0.980 \n",
    "0.979 \t0.954 \t0.986 \n",
    "0.980 \t0.928 \t0.976'''\n",
    "\n",
    "# Function to process data\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    accuracy_metric1 = []\n",
    "    accuracy_metric2 = []\n",
    "    accuracy_metric3 = []\n",
    "\n",
    "    data_list = data.replace('\\t', ' ').replace(\n",
    "        '\\n', ' ').replace('  ', ' ').split()\n",
    "    for index, data_item in enumerate(data_list):\n",
    "        if index % 3 == 0:\n",
    "            accuracy_metric1.append(float(data_item))\n",
    "        elif index % 3 == 1:\n",
    "            accuracy_metric2.append(float(data_item))\n",
    "        else:\n",
    "            accuracy_metric3.append(float(data_item))\n",
    "\n",
    "    return accuracy_metric1, accuracy_metric2, accuracy_metric3\n",
    "\n",
    "\n",
    "# Process the data\n",
    "accuracy_metric1_data1, accuracy_metric2_data1, accuracy_metric3_data1 = process_data(\n",
    "    data1)\n",
    "accuracy_metric1_data2, accuracy_metric2_data2, accuracy_metric3_data2 = process_data(\n",
    "    data2)\n",
    "\n",
    "# Data for the plot\n",
    "shots = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Create the subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "font_size = 36\n",
    "label_size = 30\n",
    "title_size = 32\n",
    "legend_size = 20\n",
    "\n",
    "# Plot data1\n",
    "ax1.plot(shots, accuracy_metric1_data1, marker='o', linestyle='--', label='GA')\n",
    "ax1.plot(shots, accuracy_metric2_data1,\n",
    "         marker='s', linestyle='--', label='MLA')\n",
    "ax1.plot(shots, accuracy_metric3_data1,\n",
    "         marker='^', linestyle='--', label='ED')\n",
    "ax1.set_xlabel('demonstration number', fontsize=font_size)\n",
    "ax1.set_xticks(shots)\n",
    "ax1.set_yticks([i/10 for i in range(5, 11)])\n",
    "ax1.set_ylim(0.6, 1)\n",
    "ax1.legend(loc='lower right', prop={'size': 20})\n",
    "ax1.grid(True)\n",
    "ax1.set_title('LILAC', fontsize=title_size)\n",
    "\n",
    "# Set bold font for tick labels on ax1\n",
    "plt.setp(ax1.get_xticklabels(), fontsize=label_size)\n",
    "plt.setp(ax1.get_yticklabels(), fontsize=label_size)\n",
    "\n",
    "# Plot data2\n",
    "ax2.plot(shots, accuracy_metric1_data2, marker='o', linestyle='--', label='GA')\n",
    "ax2.plot(shots, accuracy_metric2_data2,\n",
    "         marker='s', linestyle='--', label='MLA')\n",
    "ax2.plot(shots, accuracy_metric3_data2,\n",
    "         marker='^', linestyle='--', label='ED')\n",
    "ax2.set_xlabel('demonstration number', fontsize=font_size)\n",
    "ax2.set_xticks(shots)\n",
    "ax2.set_yticks([i/10 for i in range(5, 11)])\n",
    "ax2.set_ylim(0.6, 1)\n",
    "ax2.legend(loc='lower right', prop={'size': 20})\n",
    "ax2.grid(True)\n",
    "ax2.set_title('LogBatcher', fontsize=title_size)\n",
    "\n",
    "# Set bold font for tick labels on ax2\n",
    "plt.setp(ax2.get_xticklabels(), fontsize=label_size)\n",
    "plt.setp(ax2.get_yticklabels(), fontsize=label_size)\n",
    "\n",
    "# Create a common legend for both plots\n",
    "# handles, labels = ax1.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(\n",
    "#     0.5, 1.15), ncol=3, prop={'size': legend_size})\n",
    "\n",
    "plt.subplots_adjust(wspace=30)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/RQ3_supervised.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ablation: Bacth Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the data\n",
    "data1 = '''0.923 0.708 0.905\n",
    "0.971 0.848 0.953 \n",
    "0.970 0.873 0.965 \n",
    "0.940 0.816 0.948 \n",
    "0.928 0.804 0.942'''\n",
    "\n",
    "# Function to process data\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    accuracy_metric1 = []\n",
    "    accuracy_metric2 = []\n",
    "    accuracy_metric3 = []\n",
    "\n",
    "    data_list = data.replace('\\n', ' ').strip().split()\n",
    "    for index, data_item in enumerate(data_list):\n",
    "        if index % 3 == 0:\n",
    "            accuracy_metric1.append(float(data_item))\n",
    "        elif index % 3 == 1:\n",
    "            accuracy_metric2.append(float(data_item))\n",
    "        else:\n",
    "            accuracy_metric3.append(float(data_item))\n",
    "\n",
    "    return accuracy_metric1, accuracy_metric2, accuracy_metric3\n",
    "\n",
    "\n",
    "# Process the data\n",
    "accuracy_metric1_data1, accuracy_metric2_data1, accuracy_metric3_data1 = process_data(\n",
    "    data1)\n",
    "\n",
    "# Data for the plot\n",
    "shots = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "font_size = 36\n",
    "label_size = 24\n",
    "title_size = 32\n",
    "\n",
    "# Plot data1\n",
    "ax1.plot(shots, accuracy_metric1_data1, marker='o',\n",
    "         linestyle='--', label='GA', linewidth=2, markersize=10)\n",
    "ax1.plot(shots, accuracy_metric2_data1, marker='s',\n",
    "         linestyle='--', label='MLA', linewidth=2, markersize=10)\n",
    "ax1.plot(shots, accuracy_metric3_data1, marker='^',\n",
    "         linestyle='--', label='ED', linewidth=2, markersize=10)\n",
    "ax1.set_xlabel('batch size', fontsize=font_size)\n",
    "ax1.set_xticks(shots)\n",
    "ax1.set_yticks([i/100 for i in range(30, 101, 5)])\n",
    "ax1.set_ylim(0.65, 1)\n",
    "ax1.legend(loc='lower right', prop={'size': 20})\n",
    "ax1.grid(True)\n",
    "# ax1.set_title('LILAC', fontsize=title_size)\n",
    "\n",
    "# Set bold font for tick labels on ax1\n",
    "plt.setp(ax1.get_xticklabels(), fontsize=label_size)\n",
    "plt.setp(ax1.get_yticklabels(), fontsize=label_size)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/RQ2_batchsize.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
